404 — Nx v0.9.2
Nx
        
          v0.9.2
        
Pages
        
            Modules
          Search documentation of NxSettingsPage not foundSorry, but the page you were trying to get to, does not exist. You
may want to try searching this site using the sidebar

  or using our API Reference page

to find what you were looking for.Hex PackageHex Preview
            Search HexDocs
          
              Download ePub version
            
        Built using
        ExDoc (v0.34.2) for the

          Elixir programming languageNx.Backend — Nx v0.9.2
Nx
        
          v0.9.2
        
Pages
        
            Modules
            A tree is a plant, a cat is an animal.
          Search documentation of NxSettingsView SourceNx.Backendbehaviour(Nx v0.9.2)The behaviour for tensor backends.Each backend is module that defines a struct and implements the callbacks
defined in this module. The callbacks are mostly implementations of the
functions in the Nx module with the tensor output shape given as first
argument.Nx backends come in two flavors: opaque backends, of which you should
not access its data directly except through the functions in the Nx
module, and public ones, of which its data can be directly accessed and
traversed. The former typically have the Backend suffix.Nx ships with the following backends:Nx.BinaryBackend - an opaque backend written in pure Elixir
that stores the data in Elixir's binaries. This is the default
backend used by the Nx module. The backend itself (and its
data) is private and must not be accessed directly.Nx.TemplateBackend - an opaque backend written that works as
a template in APIs to declare the type, shape, and names of
tensors to be expected in the future.Nx.Defn.Expr - a public backend used by defn to build
expression trees that are traversed by custom compilers.This module also includes functions that are meant to be shared
across backends.SummaryTypesaxes()axis()backend_options()shape()t()tensor()Callbacksabs(out, tensor)acos(out, tensor)acosh(out, tensor)add(out, tensor, tensor)all(out, tensor, keyword)all_close(out, tensor, tensor, keyword)any(out, tensor, keyword)argmax(out, tensor, keyword)argmin(out, tensor, keyword)argsort(out, tensor, keyword)as_type(out, tensor)asin(out, tensor)asinh(out, tensor)atan2(out, tensor, tensor)atan(out, tensor)atanh(out, tensor)backend_copy(tensor, module, backend_options)backend_deallocate(tensor)backend_transfer(tensor, module, backend_options)bitcast(out, tensor)bitwise_and(out, tensor, tensor)bitwise_not(out, tensor)bitwise_or(out, tensor, tensor)bitwise_xor(out, tensor, tensor)broadcast(out, tensor, shape, axes)cbrt(out, tensor)ceil(out, tensor)cholesky(out, tensor)clip(out, tensor, min, max)concatenate(out, tensor, axis)conjugate(out, tensor)constant(out, arg2, backend_options)conv(out, tensor, kernel, keyword)cos(out, tensor)cosh(out, tensor)count_leading_zeros(out, tensor)cumulative_max(out, t, keyword)cumulative_min(out, t, keyword)cumulative_product(out, t, keyword)cumulative_sum(out, t, keyword)determinant(out, t)divide(out, tensor, tensor)dot(out, tensor, axes, axes, tensor, axes, axes)eigh({}, tensor, keyword)equal(out, tensor, tensor)erf(out, tensor)erf_inv(out, tensor)erfc(out, tensor)exp(out, tensor)expm1(out, tensor)eye(tensor, backend_options)fft2(out, tensor, keyword)fft(out, tensor, keyword)floor(out, tensor)from_binary(out, binary, backend_options)from_pointer(opaque_pointer, type, shape, backend_opts, opts)gather(out, input, indices, keyword)greater(out, tensor, tensor)greater_equal(out, tensor, tensor)ifft2(out, tensor, keyword)ifft(out, tensor, keyword)imag(out, tensor)indexed_add(out, tensor, indices, updates, keyword)indexed_put(out, tensor, indices, updates, keyword)init(keyword)inspect(tensor, t)iota(tensor, arg2, backend_options)is_infinity(out, tensor)is_nan(out, tensor)left_shift(out, tensor, tensor)less(out, tensor, tensor)less_equal(out, tensor, tensor)log1p(out, tensor)log(out, tensor)logical_and(out, tensor, tensor)logical_not(out, t)logical_or(out, tensor, tensor)logical_xor(out, tensor, tensor)lu({}, tensor, keyword)max(out, tensor, tensor)min(out, tensor, tensor)multiply(out, tensor, tensor)negate(out, tensor)not_equal(out, tensor, tensor)optional(atom, list, function)Invoked for execution of optional callbacks with a default implementation.pad(out, tensor, pad_value, padding_config)phase(out, t)population_count(out, tensor)pow(out, tensor, tensor)product(out, tensor, keyword)put_slice(out, tensor, tensor, list)qr({}, tensor, keyword)quotient(out, tensor, tensor)real(out, tensor)reduce(out, tensor, acc, keyword, function)reduce_max(out, tensor, keyword)reduce_min(out, tensor, keyword)remainder(out, tensor, tensor)reshape(out, tensor)reverse(out, tensor, axes)right_shift(out, tensor, tensor)round(out, tensor)rsqrt(out, tensor)select(out, tensor, tensor, tensor)sigmoid(out, tensor)sign(out, tensor)sin(out, tensor)sinh(out, tensor)slice(out, tensor, list, list, list)solve(out, a, b)sort(out, tensor, keyword)sqrt(out, tensor)squeeze(out, tensor, axes)stack(out, tensor, axis)subtract(out, tensor, tensor)sum(out, tensor, keyword)svd({}, tensor, keyword)take(out, input, indices, keyword)take_along_axis(out, input, indices, keyword)tan(out, tensor)tanh(out, tensor)to_batched(out, tensor, keyword)to_binary(tensor, limit)to_pointer(tensor, opts)top_k(out, tensor, keyword)transpose(out, tensor, axes)triangular_solve(out, a, b, keyword)window_max(out, tensor, shape, keyword)window_min(out, tensor, shape, keyword)window_product(out, tensor, shape, keyword)window_reduce(out, tensor, acc, shape, keyword, function)window_scatter_max(out, tensor, tensor, tensor, shape, keyword)window_scatter_min(out, tensor, tensor, tensor, shape, keyword)window_sum(out, tensor, shape, keyword)Functionscomplex_to_string(complex, precision)inspect(map, binary, inspect_opts)Inspects the given tensor given by binary.TypesLink to this typeaxes()View Source@type axes() :: Nx.Tensor.axes()Link to this typeaxis()View Source@type axis() :: Nx.Tensor.axis()Link to this typebackend_options()View Source@type backend_options() :: term()Link to this typeshape()View Source@type shape() :: Nx.Tensor.shape()Link to this typet()View Source@type t() :: %{__struct__: atom()}Link to this typetensor()View Source@type tensor() :: Nx.Tensor.t()CallbacksLink to this callbackabs(out, tensor)View Source@callback abs(out :: tensor(), tensor()) :: tensor()Link to this callbackacos(out, tensor)View Source@callback acos(out :: tensor(), tensor()) :: tensor()Link to this callbackacosh(out, tensor)View Source@callback acosh(out :: tensor(), tensor()) :: tensor()Link to this callbackadd(out, tensor, tensor)View Source@callback add(out :: tensor(), tensor(), tensor()) :: tensor()Link to this callbackall(out, tensor, keyword)View Source@callback all(out :: tensor(), tensor(), keyword()) :: tensor()Link to this callbackall_close(out, tensor, tensor, keyword)View Source(optional)@callback all_close(out :: tensor(), tensor(), tensor(), keyword()) :: tensor()Link to this callbackany(out, tensor, keyword)View Source@callback any(out :: tensor(), tensor(), keyword()) :: tensor()Link to this callbackargmax(out, tensor, keyword)View Source@callback argmax(out :: tensor(), tensor(), keyword()) :: tensor()Link to this callbackargmin(out, tensor, keyword)View Source@callback argmin(out :: tensor(), tensor(), keyword()) :: tensor()Link to this callbackargsort(out, tensor, keyword)View Source@callback argsort(out :: tensor(), tensor(), keyword()) :: tensor()Link to this callbackas_type(out, tensor)View Source@callback as_type(out :: tensor(), tensor()) :: tensor()Link to this callbackasin(out, tensor)View Source@callback asin(out :: tensor(), tensor()) :: tensor()Link to this callbackasinh(out, tensor)View Source@callback asinh(out :: tensor(), tensor()) :: tensor()Link to this callbackatan2(out, tensor, tensor)View Source@callback atan2(out :: tensor(), tensor(), tensor()) :: tensor()Link to this callbackatan(out, tensor)View Source@callback atan(out :: tensor(), tensor()) :: tensor()Link to this callbackatanh(out, tensor)View Source@callback atanh(out :: tensor(), tensor()) :: tensor()Link to this callbackbackend_copy(tensor, module, backend_options)View Source@callback backend_copy(tensor(), module(), backend_options()) :: tensor()Link to this callbackbackend_deallocate(tensor)View Source@callback backend_deallocate(tensor()) :: :ok | :already_deallocatedLink to this callbackbackend_transfer(tensor, module, backend_options)View Source@callback backend_transfer(tensor(), module(), backend_options()) :: tensor()Link to this callbackbitcast(out, tensor)View Source@callback bitcast(out :: tensor(), tensor()) :: tensor()Link to this callbackbitwise_and(out, tensor, tensor)View Source@callback bitwise_and(out :: tensor(), tensor(), tensor()) :: tensor()Link to this callbackbitwise_not(out, tensor)View Source@callback bitwise_not(out :: tensor(), tensor()) :: tensor()Link to this callbackbitwise_or(out, tensor, tensor)View Source@callback bitwise_or(out :: tensor(), tensor(), tensor()) :: tensor()Link to this callbackbitwise_xor(out, tensor, tensor)View Source@callback bitwise_xor(out :: tensor(), tensor(), tensor()) :: tensor()Link to this callbackbroadcast(out, tensor, shape, axes)View Source@callback broadcast(out :: tensor(), tensor(), shape(), axes()) :: tensor()Link to this callbackcbrt(out, tensor)View Source@callback cbrt(out :: tensor(), tensor()) :: tensor()Link to this callbackceil(out, tensor)View Source@callback ceil(out :: tensor(), tensor()) :: tensor()Link to this callbackcholesky(out, tensor)View Source(optional)@callback cholesky(out :: tensor(), tensor()) :: tensor()Link to this callbackclip(out, tensor, min, max)View Source@callback clip(out :: tensor(), tensor(), min :: tensor(), max :: tensor()) :: tensor()Link to this callbackconcatenate(out, tensor, axis)View Source@callback concatenate(out :: tensor(), tensor(), axis()) :: tensor()Link to this callbackconjugate(out, tensor)View Source@callback conjugate(out :: tensor(), tensor()) :: tensor()Link to this callbackconstant(out, arg2, backend_options)View Source@callback constant(out :: tensor(), number() | Complex.t(), backend_options()) :: tensor()Link to this callbackconv(out, tensor, kernel, keyword)View Source@callback conv(out :: tensor(), tensor(), kernel :: tensor(), keyword()) :: tensor()Link to this callbackcos(out, tensor)View Source@callback cos(out :: tensor(), tensor()) :: tensor()Link to this callbackcosh(out, tensor)View Source@callback cosh(out :: tensor(), tensor()) :: tensor()Link to this callbackcount_leading_zeros(out, tensor)View Source@callback count_leading_zeros(out :: tensor(), tensor()) :: tensor()Link to this callbackcumulative_max(out, t, keyword)View Source(optional)@callback cumulative_max(out :: tensor(), t :: tensor(), keyword()) :: tensor()Link to this callbackcumulative_min(out, t, keyword)View Source(optional)@callback cumulative_min(out :: tensor(), t :: tensor(), keyword()) :: tensor()Link to this callbackcumulative_product(out, t, keyword)View Source(optional)@callback cumulative_product(out :: tensor(), t :: tensor(), keyword()) :: tensor()Link to this callbackcumulative_sum(out, t, keyword)View Source(optional)@callback cumulative_sum(out :: tensor(), t :: tensor(), keyword()) :: tensor()Link to this callbackdeterminant(out, t)View Source(optional)@callback determinant(out :: tensor(), t :: tensor()) :: tensor()Link to this callbackdivide(out, tensor, tensor)View Source@callback divide(out :: tensor(), tensor(), tensor()) :: tensor()Link to this callbackdot(out, tensor, axes, axes, tensor, axes, axes)View Source@callback dot(out :: tensor(), tensor(), axes(), axes(), tensor(), axes(), axes()) ::
  tensor()Link to this callbackeigh({}, tensor, keyword)View Source(optional)@callback eigh({eigenvals :: tensor(), eigenvecs :: tensor()}, tensor(), keyword()) ::
  tensor()Link to this callbackequal(out, tensor, tensor)View Source@callback equal(out :: tensor(), tensor(), tensor()) :: tensor()Link to this callbackerf(out, tensor)View Source@callback erf(out :: tensor(), tensor()) :: tensor()Link to this callbackerf_inv(out, tensor)View Source@callback erf_inv(out :: tensor(), tensor()) :: tensor()Link to this callbackerfc(out, tensor)View Source@callback erfc(out :: tensor(), tensor()) :: tensor()Link to this callbackexp(out, tensor)View Source@callback exp(out :: tensor(), tensor()) :: tensor()Link to this callbackexpm1(out, tensor)View Source@callback expm1(out :: tensor(), tensor()) :: tensor()Link to this callbackeye(tensor, backend_options)View Source@callback eye(tensor(), backend_options()) :: tensor()Link to this callbackfft2(out, tensor, keyword)View Source(optional)@callback fft2(out :: tensor(), tensor(), keyword()) :: tensor()Link to this callbackfft(out, tensor, keyword)View Source@callback fft(out :: tensor(), tensor(), keyword()) :: tensor()Link to this callbackfloor(out, tensor)View Source@callback floor(out :: tensor(), tensor()) :: tensor()Link to this callbackfrom_binary(out, binary, backend_options)View Source@callback from_binary(out :: tensor(), binary(), backend_options()) :: tensor()Link to this callbackfrom_pointer(opaque_pointer, type, shape, backend_opts, opts)View Source@callback from_pointer(
  opaque_pointer :: term(),
  type :: tuple(),
  shape :: tuple(),
  backend_opts :: keyword(),
  opts :: keyword()
) :: {:ok, tensor()} | {:error, term()}Link to this callbackgather(out, input, indices, keyword)View Source@callback gather(out :: tensor(), input :: tensor(), indices :: tensor(), keyword()) ::
  tensor()Link to this callbackgreater(out, tensor, tensor)View Source@callback greater(out :: tensor(), tensor(), tensor()) :: tensor()Link to this callbackgreater_equal(out, tensor, tensor)View Source@callback greater_equal(out :: tensor(), tensor(), tensor()) :: tensor()Link to this callbackifft2(out, tensor, keyword)View Source(optional)@callback ifft2(out :: tensor(), tensor(), keyword()) :: tensor()Link to this callbackifft(out, tensor, keyword)View Source@callback ifft(out :: tensor(), tensor(), keyword()) :: tensor()Link to this callbackimag(out, tensor)View Source@callback imag(out :: tensor(), tensor()) :: tensor()Link to this callbackindexed_add(out, tensor, indices, updates, keyword)View Source@callback indexed_add(
  out :: tensor(),
  tensor(),
  indices :: tensor(),
  updates :: tensor(),
  keyword()
) ::
  tensor()Link to this callbackindexed_put(out, tensor, indices, updates, keyword)View Source@callback indexed_put(
  out :: tensor(),
  tensor(),
  indices :: tensor(),
  updates :: tensor(),
  keyword()
) ::
  tensor()Link to this callbackinit(keyword)View Source@callback init(keyword()) :: backend_options()Link to this callbackinspect(tensor, t)View Source@callback inspect(tensor(), Inspect.Opts.t()) :: tensor()Link to this callbackiota(tensor, arg2, backend_options)View Source@callback iota(tensor(), axis() | nil, backend_options()) :: tensor()Link to this callbackis_infinity(out, tensor)View Source@callback is_infinity(out :: tensor(), tensor()) :: tensor()Link to this callbackis_nan(out, tensor)View Source@callback is_nan(out :: tensor(), tensor()) :: tensor()Link to this callbackleft_shift(out, tensor, tensor)View Source@callback left_shift(out :: tensor(), tensor(), tensor()) :: tensor()Link to this callbackless(out, tensor, tensor)View Source@callback less(out :: tensor(), tensor(), tensor()) :: tensor()Link to this callbackless_equal(out, tensor, tensor)View Source@callback less_equal(out :: tensor(), tensor(), tensor()) :: tensor()Link to this callbacklog1p(out, tensor)View Source@callback log1p(out :: tensor(), tensor()) :: tensor()Link to this callbacklog(out, tensor)View Source@callback log(out :: tensor(), tensor()) :: tensor()Link to this callbacklogical_and(out, tensor, tensor)View Source@callback logical_and(out :: tensor(), tensor(), tensor()) :: tensor()Link to this callbacklogical_not(out, t)View Source(optional)@callback logical_not(out :: tensor(), t :: tensor()) :: tensor()Link to this callbacklogical_or(out, tensor, tensor)View Source@callback logical_or(out :: tensor(), tensor(), tensor()) :: tensor()Link to this callbacklogical_xor(out, tensor, tensor)View Source@callback logical_xor(out :: tensor(), tensor(), tensor()) :: tensor()Link to this callbacklu({}, tensor, keyword)View Source@callback lu({p :: tensor(), l :: tensor(), u :: tensor()}, tensor(), keyword()) ::
  tensor()Link to this callbackmax(out, tensor, tensor)View Source@callback max(out :: tensor(), tensor(), tensor()) :: tensor()Link to this callbackmin(out, tensor, tensor)View Source@callback min(out :: tensor(), tensor(), tensor()) :: tensor()Link to this callbackmultiply(out, tensor, tensor)View Source@callback multiply(out :: tensor(), tensor(), tensor()) :: tensor()Link to this callbacknegate(out, tensor)View Source@callback negate(out :: tensor(), tensor()) :: tensor()Link to this callbacknot_equal(out, tensor, tensor)View Source@callback not_equal(out :: tensor(), tensor(), tensor()) :: tensor()Link to this callbackoptional(atom, list, function)View Source(optional)@callback optional(atom(), [term()], (... -> any())) :: tensor()Invoked for execution of optional callbacks with a default implementation.First we will attempt to call the optional callback itself
(one of the many callbacks defined below), then we attempt
to call this callback (which is also optional), then we
fallback to the default iomplementation.Link to this callbackpad(out, tensor, pad_value, padding_config)View Source@callback pad(out :: tensor(), tensor(), pad_value :: tensor(), padding_config :: list()) ::
  tensor()Link to this callbackphase(out, t)View Source(optional)@callback phase(out :: tensor(), t :: tensor()) :: tensor()Link to this callbackpopulation_count(out, tensor)View Source@callback population_count(out :: tensor(), tensor()) :: tensor()Link to this callbackpow(out, tensor, tensor)View Source@callback pow(out :: tensor(), tensor(), tensor()) :: tensor()Link to this callbackproduct(out, tensor, keyword)View Source@callback product(out :: tensor(), tensor(), keyword()) :: tensor()Link to this callbackput_slice(out, tensor, tensor, list)View Source@callback put_slice(out :: tensor(), tensor(), tensor(), list()) :: tensor()Link to this callbackqr({}, tensor, keyword)View Source(optional)@callback qr({q :: tensor(), r :: tensor()}, tensor(), keyword()) :: tensor()Link to this callbackquotient(out, tensor, tensor)View Source@callback quotient(out :: tensor(), tensor(), tensor()) :: tensor()Link to this callbackreal(out, tensor)View Source@callback real(out :: tensor(), tensor()) :: tensor()Link to this callbackreduce(out, tensor, acc, keyword, function)View Source@callback reduce(out :: tensor(), tensor(), acc :: tensor(), keyword(), (... -> any())) ::
  tensor()Link to this callbackreduce_max(out, tensor, keyword)View Source@callback reduce_max(out :: tensor(), tensor(), keyword()) :: tensor()Link to this callbackreduce_min(out, tensor, keyword)View Source@callback reduce_min(out :: tensor(), tensor(), keyword()) :: tensor()Link to this callbackremainder(out, tensor, tensor)View Source@callback remainder(out :: tensor(), tensor(), tensor()) :: tensor()Link to this callbackreshape(out, tensor)View Source@callback reshape(out :: tensor(), tensor()) :: tensor()Link to this callbackreverse(out, tensor, axes)View Source@callback reverse(out :: tensor(), tensor(), axes()) :: tensor()Link to this callbackright_shift(out, tensor, tensor)View Source@callback right_shift(out :: tensor(), tensor(), tensor()) :: tensor()Link to this callbackround(out, tensor)View Source@callback round(out :: tensor(), tensor()) :: tensor()Link to this callbackrsqrt(out, tensor)View Source@callback rsqrt(out :: tensor(), tensor()) :: tensor()Link to this callbackselect(out, tensor, tensor, tensor)View Source@callback select(out :: tensor(), tensor(), tensor(), tensor()) :: tensor()Link to this callbacksigmoid(out, tensor)View Source@callback sigmoid(out :: tensor(), tensor()) :: tensor()Link to this callbacksign(out, tensor)View Source@callback sign(out :: tensor(), tensor()) :: tensor()Link to this callbacksin(out, tensor)View Source@callback sin(out :: tensor(), tensor()) :: tensor()Link to this callbacksinh(out, tensor)View Source@callback sinh(out :: tensor(), tensor()) :: tensor()Link to this callbackslice(out, tensor, list, list, list)View Source@callback slice(out :: tensor(), tensor(), list(), list(), list()) :: tensor()Link to this callbacksolve(out, a, b)View Source(optional)@callback solve(out :: tensor(), a :: tensor(), b :: tensor()) :: tensor()Link to this callbacksort(out, tensor, keyword)View Source@callback sort(out :: tensor(), tensor(), keyword()) :: tensor()Link to this callbacksqrt(out, tensor)View Source@callback sqrt(out :: tensor(), tensor()) :: tensor()Link to this callbacksqueeze(out, tensor, axes)View Source@callback squeeze(out :: tensor(), tensor(), axes()) :: tensor()Link to this callbackstack(out, tensor, axis)View Source@callback stack(out :: tensor(), tensor(), axis()) :: tensor()Link to this callbacksubtract(out, tensor, tensor)View Source@callback subtract(out :: tensor(), tensor(), tensor()) :: tensor()Link to this callbacksum(out, tensor, keyword)View Source@callback sum(out :: tensor(), tensor(), keyword()) :: tensor()Link to this callbacksvd({}, tensor, keyword)View Source(optional)@callback svd({u :: tensor(), s :: tensor(), v :: tensor()}, tensor(), keyword()) ::
  tensor()Link to this callbacktake(out, input, indices, keyword)View Source(optional)@callback take(out :: tensor(), input :: tensor(), indices :: tensor(), keyword()) ::
  tensor()Link to this callbacktake_along_axis(out, input, indices, keyword)View Source(optional)@callback take_along_axis(
  out :: tensor(),
  input :: tensor(),
  indices :: tensor(),
  keyword()
) :: tensor()Link to this callbacktan(out, tensor)View Source@callback tan(out :: tensor(), tensor()) :: tensor()Link to this callbacktanh(out, tensor)View Source@callback tanh(out :: tensor(), tensor()) :: tensor()Link to this callbackto_batched(out, tensor, keyword)View Source@callback to_batched(out :: tensor(), tensor(), keyword()) :: [tensor()]Link to this callbackto_binary(tensor, limit)View Source@callback to_binary(tensor(), limit :: non_neg_integer()) :: binary()Link to this callbackto_pointer(tensor, opts)View Source@callback to_pointer(tensor(), opts :: keyword()) :: {:ok, term()} | {:error, term()}Link to this callbacktop_k(out, tensor, keyword)View Source(optional)@callback top_k(out :: tensor(), tensor(), keyword()) :: tensor()Link to this callbacktranspose(out, tensor, axes)View Source@callback transpose(out :: tensor(), tensor(), axes()) :: tensor()Link to this callbacktriangular_solve(out, a, b, keyword)View Source@callback triangular_solve(out :: tensor(), a :: tensor(), b :: tensor(), keyword()) ::
  tensor()Link to this callbackwindow_max(out, tensor, shape, keyword)View Source@callback window_max(out :: tensor(), tensor(), shape(), keyword()) :: tensor()Link to this callbackwindow_min(out, tensor, shape, keyword)View Source@callback window_min(out :: tensor(), tensor(), shape(), keyword()) :: tensor()Link to this callbackwindow_product(out, tensor, shape, keyword)View Source@callback window_product(out :: tensor(), tensor(), shape(), keyword()) :: tensor()Link to this callbackwindow_reduce(out, tensor, acc, shape, keyword, function)View Source@callback window_reduce(
  out :: tensor(),
  tensor(),
  acc :: tensor(),
  shape(),
  keyword(),
  (... -> any())
) ::
  tensor()Link to this callbackwindow_scatter_max(out, tensor, tensor, tensor, shape, keyword)View Source@callback window_scatter_max(
  out :: tensor(),
  tensor(),
  tensor(),
  tensor(),
  shape(),
  keyword()
) :: tensor()Link to this callbackwindow_scatter_min(out, tensor, tensor, tensor, shape, keyword)View Source@callback window_scatter_min(
  out :: tensor(),
  tensor(),
  tensor(),
  tensor(),
  shape(),
  keyword()
) :: tensor()Link to this callbackwindow_sum(out, tensor, shape, keyword)View Source@callback window_sum(out :: tensor(), tensor(), shape(), keyword()) :: tensor()FunctionsLink to this functioncomplex_to_string(complex, precision)View SourceLink to this functioninspect(map, binary, inspect_opts)View SourceInspects the given tensor given by binary.Note the binary may have fewer elements than the
tensor size but, in such cases, it must strictly have
more elements than inspect_opts.limitOptionsThe following must be passed through Inspect:custom_options:nx_precision - Configures the floating-point number printing precision.
If set, will print floating-point numbers in scientific notation using the
specified number of significant digits. Otherwise, default Elixir printing
rules are applied.Hex PackageHex Preview
            Search HexDocs
          
              Download ePub version
            
        Built using
        ExDoc (v0.34.2) for the

          Elixir programming languageNx.Batch — Nx v0.9.2
Nx
        
          v0.9.2
        
Pages
        
            Modules
          Search documentation of NxSettingsView SourceNx.Batch(Nx v0.9.2)Creates a batch of tensors (and containers).A batch is lazily traversed, concatenated, and padded upon defn invocation.SummaryTypest()Functions%Nx.Batch{}A Nx.Batch struct.concatenate(batch \\ new(), entries)Concatenates the given entries to the batch.key(batch, key)Sets the batch key for the given batch.merge(list)Merges a list of batches.merge(left, right)Merges two batches.new()Returns a new empty batch.pad(batch, pad)Configures the batch with the given padding.split(batch, n)Splits a batch in two, where the first one has at most n elements.stack(batch \\ new(), entries)Stacks the given entries to the batch.TypesLink to this typet()View Source@type t() :: %Nx.Batch{
  key: term(),
  pad: non_neg_integer(),
  size: non_neg_integer(),
  stack: list(),
  template: Nx.Container.t() | Nx.Tensor.t() | nil
}FunctionsLink to this function%Nx.Batch{}View Source(struct)A Nx.Batch struct.The :size field is public.Link to this functionconcatenate(batch \\ new(), entries)View SourceConcatenates the given entries to the batch.Entries are concatenated based on their first axis.
If the first axis has multiple entries, each entry
is added to the size of the batch.You can either concatenate to an existing batch
or skip the batch argument to create a new batch.See stack/2 if you want to stack entries instead
of concatenating them.ExamplesIf no batch is given, one is automatically created:iex> batch=Nx.Batch.concatenate([Nx.tensor([1]),Nx.tensor([2]),Nx.tensor([3])])iex> Nx.Defn.jit_apply(&Function.identity/1,[batch])#Nx.Tensor<s32[3][1,2,3]>But you can also concatenate to existing batches:iex> batch=Nx.Batch.concatenate([Nx.tensor([1]),Nx.tensor([2])])iex> batch=Nx.Batch.concatenate(batch,[Nx.tensor([3]),Nx.tensor([4])])iex> Nx.Defn.jit_apply(&Function.identity/1,[batch])#Nx.Tensor<s32[4][1,2,3,4]>If the first axis has multiple entries, each entry counts
towards the size of the batch:iex> batch=Nx.Batch.concatenate([Nx.tensor([1,2]),Nx.tensor([3,4,5])])iex> batch.size5iex> Nx.Defn.jit_apply(&Function.identity/1,[batch])#Nx.Tensor<s32[5][1,2,3,4,5]>What makes batches powerful is that they can concatenate
across containers:iex> container1={Nx.tensor([11]),Nx.tensor([21])}iex> container2={Nx.tensor([12]),Nx.tensor([22])}iex> batch=Nx.Batch.concatenate([container1,container2])iex> {batched1,batched2}=Nx.Defn.jit_apply(&Function.identity/1,[batch])iex> batched1#Nx.Tensor<s32[2][11,12]>iex> batched2#Nx.Tensor<s32[2][21,22]>Link to this functionkey(batch, key)View SourceSets the batch key for the given batch.Link to this functionmerge(list)View SourceMerges a list of batches.See merge/2.Link to this functionmerge(left, right)View SourceMerges two batches.The tensors on the left will appear before the tensors on the right.The size and padding of both batches are summed. The padding still
applies only at the end of batch.It will raise if the batch templates are incompatible.Examplesiex> batch1=Nx.Batch.stack([Nx.tensor(1),Nx.tensor(2),Nx.tensor(3)])iex> batch2=Nx.Batch.concatenate([Nx.tensor([4,5]),Nx.tensor([6,7,8])])iex> batch=Nx.Batch.merge(batch1,batch2)iex> batch.size8iex> Nx.Defn.jit_apply(&Function.identity/1,[batch])#Nx.Tensor<s32[8][1,2,3,4,5,6,7,8]>Link to this functionnew()View SourceReturns a new empty batch.Link to this functionpad(batch, pad)View SourceConfigures the batch with the given padding.The batch will be padded when consumed:iex> batch=Nx.Batch.stack([Nx.tensor(1),Nx.tensor(2),Nx.tensor(3)])iex> Nx.Defn.jit_apply(&Function.identity/1,[Nx.Batch.pad(batch,2)])#Nx.Tensor<s32[5][1,2,3,0,0]>Link to this functionsplit(batch, n)View SourceSplits a batch in two, where the first one has at most n elements.If there is any padding and the batch is not full, the amount of padding
necessary will be moved to the first batch and the remaining stays in the
second batch.Examplesiex> batch=Nx.Batch.concatenate([Nx.tensor([1,2]),Nx.tensor([3,4,5])])iex> {left,right}=Nx.Defn.jit_apply(&Function.identity/1,[Nx.Batch.split(batch,3)])iex> left#Nx.Tensor<s32[3][1,2,3]>iex> right#Nx.Tensor<s32[2][4,5]>Link to this functionstack(batch \\ new(), entries)View SourceStacks the given entries to the batch.Each entry counts exactly as a single entry.
You can either stack to an existing batch
or skip the batch argument to create a new batch.See concatenate/2 if you want to concatenate entries
instead of stacking them.ExamplesIf no batch is given, one is automatically created:iex> batch=Nx.Batch.stack([Nx.tensor(1),Nx.tensor(2),Nx.tensor(3)])iex> batch.size3iex> Nx.Defn.jit_apply(&Function.identity/1,[batch])#Nx.Tensor<s32[3][1,2,3]>But you can also stack an existing batch:iex> batch=Nx.Batch.stack([Nx.tensor(1),Nx.tensor(2)])iex> batch=Nx.Batch.stack(batch,[Nx.tensor(3),Nx.tensor(4)])iex> batch.size4iex> Nx.Defn.jit_apply(&Function.identity/1,[batch])#Nx.Tensor<s32[4][1,2,3,4]>What makes batches powerful is that they can concatenate
across containers:iex> container1={Nx.tensor(11),Nx.tensor(21)}iex> container2={Nx.tensor(12),Nx.tensor(22)}iex> batch=Nx.Batch.stack([container1,container2])iex> {batched1,batched2}=Nx.Defn.jit_apply(&Function.identity/1,[batch])iex> batched1#Nx.Tensor<s32[2][11,12]>iex> batched2#Nx.Tensor<s32[2][21,22]>Hex PackageHex Preview
            Search HexDocs
          
              Download ePub version
            
        Built using
        ExDoc (v0.34.2) for the

          Elixir programming languageNx.BinaryBackend — Nx v0.9.2
Nx
        
          v0.9.2
        
Pages
        
            Modules
          Search documentation of NxSettingsView SourceNx.BinaryBackend(Nx v0.9.2)An opaque backend written in pure Elixir that stores
the data in Elixir's binaries.This is the default backend used by the Nx module.
The backend itself (and its data) is private and must
not be accessed directly.Hex PackageHex Preview
            Search HexDocs
          
              Download ePub version
            
        Built using
        ExDoc (v0.34.2) for the

          Elixir programming languageNx.Constants — Nx v0.9.2
Nx
        
          v0.9.2
        
Pages
        
            Modules
          Search documentation of NxSettingsView SourceNx.Constants(Nx v0.9.2)Common constants used in computations.This module can be used in defn.SummaryFunctionse()Returns $e$ in f32.e(type, opts \\ [])Returns a scalar tensor with the value of $e$ for the given type.epsilon(type, opts \\ [])Returns a scalar with the machine epsilon for the given type.euler_gamma()Returns $\gamma$ (Euler-Mascheroni constant) in f32.euler_gamma(type, opts \\ [])Returns a scalar tensor with the value of $\gamma$ (Euler-Mascheroni constant) for the given type.i()Returns the imaginary constant in c64.i(type, opts \\ [])Returns the imaginary constant.infinity()Returns infinity in f32.infinity(type, opts \\ [])Returns infinity.max(type, opts \\ [])Returns a scalar tensor with the maximum value for the given type.max_finite(type, opts \\ [])Returns a scalar tensor with the maximum finite value for the given type.min(type, opts \\ [])Returns a scalar tensor with the minimum value for the given type.min_finite(type, opts \\ [])Returns a scalar tensor with the minimum finite value for the given type.nan()Returns NaN in f32.nan(type, opts \\ [])Returns NaN (Not a Number).neg_infinity()Returns negative infinity in f32.neg_infinity(type, opts \\ [])Returns negative infinity.pi()Returns $\pi$ in f32.pi(type, opts \\ [])Returns a scalar tensor with the value of $\pi$ for the given type.smallest_positive_normal(type, opts \\ [])Returns a scalar tensor with the smallest positive value for the given type.FunctionsLink to this functione()View SourceReturns $e$ in f32.Link to this functione(type, opts \\ [])View SourceReturns a scalar tensor with the value of $e$ for the given type.Options:backend - a backend to allocate the tensor on.Examplesiex> Nx.Constants.e({:f,64})#Nx.Tensor<f642.718281828459045>iex> Nx.Constants.e({:f,32})#Nx.Tensor<f322.7182817459106445>iex> Nx.Constants.e({:f,16})#Nx.Tensor<f162.71875>iex> Nx.Constants.e({:bf,16})#Nx.Tensor<bf162.703125>iex> Nx.Constants.e({:f,8})#Nx.Tensor<f82.5>iex> Nx.Constants.e({:s,32})** (ArgumentError) only floating types are supported, got: {:s, 32}Link to this functionepsilon(type, opts \\ [])View SourceReturns a scalar with the machine epsilon for the given type.The values are compatible with a IEEE 754 floating point standard.Options:backend - a backend to allocate the tensor on.Examplesiex> Nx.Constants.epsilon({:f,64})#Nx.Tensor<f642.220446049250313e-16>iex> Nx.Constants.epsilon({:f,32})#Nx.Tensor<f321.1920928955078125e-7>iex> Nx.Constants.epsilon({:f,16})#Nx.Tensor<f169.765625e-4>iex> Nx.Constants.epsilon(:bf16)#Nx.Tensor<bf160.0078125>iex> Nx.Constants.epsilon(:f8)#Nx.Tensor<f80.25>iex> Nx.Constants.epsilon({:s,32})** (ArgumentError) only floating types are supported, got: {:s, 32}Link to this functioneuler_gamma()View SourceReturns $\gamma$ (Euler-Mascheroni constant) in f32.Link to this functioneuler_gamma(type, opts \\ [])View SourceReturns a scalar tensor with the value of $\gamma$ (Euler-Mascheroni constant) for the given type.Options:backend - a backend to allocate the tensor on.Examplesiex> Nx.Constants.euler_gamma({:f,64})#Nx.Tensor<f640.5772156649015329>iex> Nx.Constants.euler_gamma({:f,32})#Nx.Tensor<f320.5772156715393066>iex> Nx.Constants.euler_gamma({:f,16})#Nx.Tensor<f160.5771484375>iex> Nx.Constants.euler_gamma({:bf,16})#Nx.Tensor<bf160.57421875>iex> Nx.Constants.euler_gamma({:f,8})#Nx.Tensor<f80.5>iex> Nx.Constants.euler_gamma({:s,32})** (ArgumentError) only floating types are supported, got: {:s, 32}Link to this functioni()View SourceReturns the imaginary constant in c64.Link to this functioni(type, opts \\ [])View SourceReturns the imaginary constant.Accepts the same options as Nx.tensor/2Examplesiex> Nx.Constants.i()#Nx.Tensor<c640.0+1.0i>iex> Nx.Constants.i(:c128)#Nx.Tensor<c1280.0+1.0i>Error casesiex> Nx.Constants.i({:f,32})** (ArgumentError) invalid type for complex number. Expected {:c, 64} or {:c, 128}, got: {:f, 32}Link to this functioninfinity()View SourceReturns infinity in f32.Link to this functioninfinity(type, opts \\ [])View SourceReturns infinity.Options:backend - a backend to allocate the tensor on.Examplesiex> Nx.Constants.infinity({:f,8})#Nx.Tensor<f8Inf>iex> Nx.Constants.infinity({:bf,16})#Nx.Tensor<bf16Inf>iex> Nx.Constants.infinity({:f,16})#Nx.Tensor<f16Inf>iex> Nx.Constants.infinity({:f,32})#Nx.Tensor<f32Inf>iex> Nx.Constants.infinity({:f,64})#Nx.Tensor<f64Inf>Link to this functionmax(type, opts \\ [])View SourceReturns a scalar tensor with the maximum value for the given type.It is infinity for floating point tensors.Options:backend - a backend to allocate the tensor on.Examplesiex> Nx.Constants.max({:u,8})#Nx.Tensor<u8255>iex> Nx.Constants.max({:f,32})#Nx.Tensor<f32Inf>Link to this functionmax_finite(type, opts \\ [])View SourceReturns a scalar tensor with the maximum finite value for the given type.Options:backend - a backend to allocate the tensor on.Examplesiex> Nx.Constants.max_finite({:u,8})#Nx.Tensor<u8255>iex> Nx.Constants.max_finite({:s,16})#Nx.Tensor<s1632767>iex> Nx.Constants.max_finite({:f,32})#Nx.Tensor<f323.4028234663852886e38>Link to this functionmin(type, opts \\ [])View SourceReturns a scalar tensor with the minimum value for the given type.It is negative infinity for floating point tensors.Options:backend - a backend to allocate the tensor on.Examplesiex> Nx.Constants.min({:u,8})#Nx.Tensor<u80>iex> Nx.Constants.min({:f,32})#Nx.Tensor<f32-Inf>Link to this functionmin_finite(type, opts \\ [])View SourceReturns a scalar tensor with the minimum finite value for the given type.Options:backend - a backend to allocate the tensor on.Examplesiex> Nx.Constants.min_finite({:u,8})#Nx.Tensor<u80>iex> Nx.Constants.min_finite({:s,16})#Nx.Tensor<s16-32768>iex> Nx.Constants.min_finite({:f,32})#Nx.Tensor<f32-3.4028234663852886e38>Link to this functionnan()View SourceReturns NaN in f32.Link to this functionnan(type, opts \\ [])View SourceReturns NaN (Not a Number).Options:backend - a backend to allocate the tensor on.Examplesiex> Nx.Constants.nan({:f,8})#Nx.Tensor<f8NaN>iex> Nx.Constants.nan({:bf,16})#Nx.Tensor<bf16NaN>iex> Nx.Constants.nan({:f,16})#Nx.Tensor<f16NaN>iex> Nx.Constants.nan({:f,32})#Nx.Tensor<f32NaN>iex> Nx.Constants.nan({:f,64})#Nx.Tensor<f64NaN>Link to this functionneg_infinity()View SourceReturns negative infinity in f32.Link to this functionneg_infinity(type, opts \\ [])View SourceReturns negative infinity.Options:backend - a backend to allocate the tensor on.Examplesiex> Nx.Constants.neg_infinity({:f,8})#Nx.Tensor<f8-Inf>iex> Nx.Constants.neg_infinity({:bf,16})#Nx.Tensor<bf16-Inf>iex> Nx.Constants.neg_infinity({:f,16})#Nx.Tensor<f16-Inf>iex> Nx.Constants.neg_infinity({:f,32})#Nx.Tensor<f32-Inf>iex> Nx.Constants.neg_infinity({:f,64})#Nx.Tensor<f64-Inf>Link to this functionpi()View SourceReturns $\pi$ in f32.Link to this functionpi(type, opts \\ [])View SourceReturns a scalar tensor with the value of $\pi$ for the given type.Options:backend - a backend to allocate the tensor on.Examplesiex> Nx.Constants.pi({:f,64})#Nx.Tensor<f643.141592653589793>iex> Nx.Constants.pi({:f,32})#Nx.Tensor<f323.1415927410125732>iex> Nx.Constants.pi({:f,16})#Nx.Tensor<f163.140625>iex> Nx.Constants.pi({:bf,16})#Nx.Tensor<bf163.140625>iex> Nx.Constants.pi({:f,8})#Nx.Tensor<f83.0>iex> Nx.Constants.pi({:s,32})** (ArgumentError) only floating types are supported, got: {:s, 32}Link to this functionsmallest_positive_normal(type, opts \\ [])View SourceReturns a scalar tensor with the smallest positive value for the given type.Options:backend - a backend to allocate the tensor on.Examplesiex> Nx.Constants.smallest_positive_normal({:f,64})#Nx.Tensor<f642.2250738585072014e-308>iex> Nx.Constants.smallest_positive_normal({:f,32})#Nx.Tensor<f321.1754943508222875e-38>iex> Nx.Constants.smallest_positive_normal({:f,16})#Nx.Tensor<f166.103515625e-5>iex> Nx.Constants.smallest_positive_normal(:bf16)#Nx.Tensor<bf161.1754943508222875e-38>iex> Nx.Constants.smallest_positive_normal(:f8)#Nx.Tensor<f86.103515625e-5>iex> Nx.Constants.smallest_positive_normal({:s,32})** (ArgumentError) only floating types are supported, got: {:s, 32}Hex PackageHex Preview
            Search HexDocs
          
              Download ePub version
            
        Built using
        ExDoc (v0.34.2) for the

          Elixir programming languageNx.Container — Nx v0.9.2
Nx
        
          v0.9.2
        
Pages
        
            Modules
          Search documentation of NxSettingsView SourceNx.Containerprotocol(Nx v0.9.2)A protocol that teaches defn how to traverse data structures.When you invoke a defn, its arguments must implement
a Nx.LazyContainer and return a data structure that
implements Nx.Container. Inside defn, you can work
with any container data structure, such as:numbers/tensorstuplesmaps of any keyany struct that implements Nx.ContainerIn other words, LazyContainer is how you convert data
structures that are not meant to work inside defn into
a Nx.Container. And a Nx.Container is a data structure
that can be manipulated inside defn itself.The easiest way to implement Nx.Container is by deriving
it. For example:@derive{Nx.Container,containers:[:field_name,:other_field]}defstruct[:field_name,:other_fields,...]The :containers option is required and it must specify a
list of fields that contains tensors (or other containers).
Inside defn, the container fields will be automatically
converted to tensor expressions. All other fields will be
reset to their default value, unless you explicitly declare
them to be kept:@derive{Nx.Container,containers:[:field_name,:other_field],keep:[:another_field]}defstruct[:field_name,:other_fields,...]Note Nx.LazyContainer is automatically provided for all
data structures that implement Nx.Container.Careful!: If you keep a field, its value will be part
of the Nx.Defn compiler cache key (i.e. therefore if you
give a struct with two different values for a kept field,
Nx.Defn will have to compile and cache it twice).
You must only keep fields that you are certain to be used
inside defn during compilation time.SerializationIf you @derive {Nx.Container, ...}, it will automatically
define a serialization function with the container and keep
fields you declare. If you expect a struct to be serialized,
then you must be careful to evolve its schema over time in
a compatible way. In particular, removing fields will lead to
crashes. If you change the type of a field value, previously
serialized structs may still hold the old type. And if you add
new fields, previously serialized structs won't have such fields
and therefore be deserialized with its default value.SummaryTypest()All the types that implement this protocol.Functionsreduce(data, acc, fun)Reduces non-recursively tensors in a data structure with acc and fun.serialize(struct)Defines how this container must be serialized to disk.traverse(data, acc, fun)Traverses non-recursively tensors in a data structure with acc and fun.TypesLink to this typet()View Source@type t() :: term()All the types that implement this protocol.FunctionsLink to this functionreduce(data, acc, fun)View Source@spec reduce(t(), acc, (t(), acc -> acc)) :: acc when acc: term()Reduces non-recursively tensors in a data structure with acc and fun.fun is invoked with each tensor or tensor container in the
data structure plus an accumulator. It must return the new
accumulator.This function the final accumulator.Given fun may receive containers, it is not recursive by default.
See Nx.Defn.Composite.reduce/3 for a recursive variant.Link to this functionserialize(struct)View Source@spec serialize(t()) :: {module(), [{term(), t()}], term()}Defines how this container must be serialized to disk.It receives the container and it must return a three element tuple
of {module, list_of_container_tuples, metadata} where:the module to deserialize the containera list of tuples in the shape {key, container} with containers to be further serializedadditional metadata for serialization/deserializationOn deserialization, module.deserialize(list_of_container_tuples, metadata)
will be invoked.Link to this functiontraverse(data, acc, fun)View Source@spec traverse(t(), acc, (t(), acc -> {t(), acc})) :: {t(), acc} when acc: term()Traverses non-recursively tensors in a data structure with acc and fun.fun is invoked with each tensor or tensor container in the
data structure plus an accumulator. It must return a two element
tuple with the updated value and accumulator.This function returns the updated container and the accumulator.Given fun may receive containers, it is not recursive by default.
See Nx.Defn.Composite.traverse/3 for a recursive variant.Hex PackageHex Preview
            Search HexDocs
          
              Download ePub version
            
        Built using
        ExDoc (v0.34.2) for the

          Elixir programming languageNx.Defn.Compiler — Nx v0.9.2
Nx
        
          v0.9.2
        
Pages
        
            Modules
          Search documentation of NxSettingsView SourceNx.Defn.Compilerbehaviour(Nx v0.9.2)The specification and helper functions for custom defn compilers.SummaryCallbacks__compile__(key, vars, fun, opts)Callback for compilation.__jit__(key, vars, fun, args_list, opts)Callback for compilation.__partitions_options__(keyword)Receives a keyword list of compiler options and
returns a list of compiler options, each to run
on a separate partition/device.__stream__(key, input, acc, vars, fun, args_list, opts)Callback for streaming (on top of JIT compilation).__to_backend__(keyword)Receives a keyword list of compiler options and returns a backend
with options that corresponds to the same allocation.Functionscurrent()Returns the current compiler.defn?()Returns if we are inside defn at compilation time.CallbacksLink to this callback__compile__(key, vars, fun, opts)View Source@callback __compile__(
  key :: term(),
  vars :: vars,
  fun :: (vars -> Nx.Container.t()),
  opts :: keyword()
) :: ([[Nx.Tensor.t()]] -> [Nx.Container.t()])
when vars: [Nx.Container.t()]Callback for compilation.It receives an opaque key used for caching, the function
vars, the function fun which builds a defn expression,
and the compiler options. It must call fun with the vars
as arguments.It returns a function that receives a list of arguments and
returns a list of results.The callback uses double underscores so it can be defined
at root modules without affecting the module's main API.Link to this callback__jit__(key, vars, fun, args_list, opts)View Source@callback __jit__(
  key :: term(),
  vars,
  fun :: (vars -> Nx.Container.t()),
  args_list :: [[(-> Nx.Tensor.t())]],
  opts :: keyword()
) :: [Nx.Container.t()]
when vars: [Nx.Container.t()]Callback for compilation.It receives an opaque key used for caching, the function
vars, the function fun which builds a defn expression,
a list of argument list in args_list, and the compiler options.It must call fun with the vars as arguments. Note the key
does not include the vars in its cache. Therefore, if you want
to cache the result of fun.(vars), you likely want to include
the vars in the cache key. vars is a list of containers expressions.Once the expression is built and compiled, it must be invoked
for each list of arguments in args_list. In a nutshell, vars
are used to build the expression from fun which is then
invoked for each list of arguments in args_list. All lists
in args_list are guaranteed to be flat lists of the same length,
containing zero-arity functions that return tensors of the same type,
shape, and name.The callback uses double underscores so it can be defined
at root modules without affecting the module's main API.Link to this callback__partitions_options__(keyword)View Source@callback __partitions_options__(keyword()) :: [keyword()]Receives a keyword list of compiler options and
returns a list of compiler options, each to run
on a separate partition/device.Link to this callback__stream__(key, input, acc, vars, fun, args_list, opts)View Source@callback __stream__(
  key :: term(),
  input,
  acc,
  vars,
  fun :: (vars -> {output, acc}),
  args_list :: [[(-> Nx.t())]],
  opts :: keyword()
) :: [Nx.Stream.t()]
when input: Nx.Container.t(),
     output: Nx.Container.t(),
     acc: Nx.Container.t(),
     vars: [Nx.Container.t()]Callback for streaming (on top of JIT compilation).It receives the same arguments as __jit__/5 with the addition
of the streaming input and accumulator templates. If the input
and accumulator are containers, they are kept in their container
shapes. As in __jit__/5, both vars and args_list are flat
lists of tensors (without their container shape).It must return a struct that implements the Nx.Stream protocol.Link to this callback__to_backend__(keyword)View Source@callback __to_backend__(keyword()) :: {module(), keyword()}Receives a keyword list of compiler options and returns a backend
with options that corresponds to the same allocation.The backend is expected to match what would be returned from a
computation defined by the compiler.FunctionsLink to this functioncurrent()View SourceReturns the current compiler.Returns nil if we are not inside defn.Link to this functiondefn?()View SourceReturns if we are inside defn at compilation time.This would be invoked inside a macro that has specific defn logic.Hex PackageHex Preview
            Search HexDocs
          
              Download ePub version
            
        Built using
        ExDoc (v0.34.2) for the

          Elixir programming languageNx.Defn.Composite — Nx v0.9.2
Nx
        
          v0.9.2
        
Pages
        
            Modules
          Search documentation of NxSettingsView SourceNx.Defn.Composite(Nx v0.9.2)Functions to deal with composite data types.Composite data-types are traversed according to Nx.Container.
If a regular tensor is given, it is individually traversed. 
Numerical values, such as integers, floats, and complex numbers
are not normalized before hand. Use Nx.to_tensor/1 to do so.The functions in this module can be used both inside and outside defn.
Note that, when a value is given to defn, it is first converted to
tensors and containers via Nx.LazyContainer. Inside defn, there are
no lazy containers, only containers.SummaryFunctionscompatible?(left, right, fun)Traverses two composite types to see if they are compatible.count(tree)Counts the number of non-composite types in the composite type.flatten_list(args, tail \\ [])Flattens recursively the given list of composite types.reduce(expr, acc, fun)Reduces recursively the given composite types with acc and fun.traverse(expr, fun)Traverses recursively the given composite types with fun.traverse(expr, acc, fun)Traverses recursively the given composite types with acc and fun.FunctionsLink to this functioncompatible?(left, right, fun)View SourceTraverses two composite types to see if they are compatible.Non-tensor values are first compared using Nx.LazyContainer
and then, if not available, as Nx.Container.For non-composite types, the given fun will be called to
compare numbers/tensors pairwise.Link to this functioncount(tree)View SourceCounts the number of non-composite types in the composite type.Examplesiex> Nx.Defn.Composite.count(123)1iex> Nx.Defn.Composite.count({1,{2,3}})3iex> Nx.Defn.Composite.count({Complex.new(1),{Nx.tensor(2),3}})3Link to this functionflatten_list(args, tail \\ [])View SourceFlattens recursively the given list of composite types.Elements that are not tensors (i.e. numbers and Complex numbers) are kept as is
unless a custom function is given.Examplesiex> Nx.Defn.Composite.flatten_list([1,{2,3}])[1,2,3]iex> Nx.Defn.Composite.flatten_list([1,{2,3}],[Nx.tensor(4)])[1,2,3,Nx.tensor(4)]Link to this functionreduce(expr, acc, fun)View SourceReduces recursively the given composite types with acc and fun.If composite tensor expressions are given, such as a tuple,
the composite type is recursively traversed and returned.If a non-composite tensor expression is given, the function
is invoked for it but not for its arguments.Link to this functiontraverse(expr, fun)View SourceTraverses recursively the given composite types with fun.If a composite tensor is given, such as a tuple, the composite
type is recursively traversed and returned.Otherwise the function is invoked with the tensor (be it a
number, complex, or actual tensor).Link to this functiontraverse(expr, acc, fun)View SourceTraverses recursively the given composite types with acc and fun.If a composite tensor is given, such as a tuple, the composite
type is recursively traversed and returned.Otherwise the function is invoked with the tensor (be it a
number, complex, or actual tensor).Hex PackageHex Preview
            Search HexDocs
          
              Download ePub version
            
        Built using
        ExDoc (v0.34.2) for the

          Elixir programming languageNx.Defn.Evaluator — Nx v0.9.2
Nx
        
          v0.9.2
        
Pages
        
            Modules
          Search documentation of NxSettingsView SourceNx.Defn.Evaluator(Nx v0.9.2)The default implementation of a Nx.Defn.Compiler
that evaluates the expression tree against the
tensor backend.OptionsThe following options are specific to this compiler::garbage_collect - when true, garbage collects
after evaluating each node:max_concurrency - the number of partitions to
start when running a Nx.Serving with this compilerHex PackageHex Preview
            Search HexDocs
          
              Download ePub version
            
        Built using
        ExDoc (v0.34.2) for the

          Elixir programming languageNx.Defn.Expr — Nx v0.9.2
Nx
        
          v0.9.2
        
Pages
        
            Modules
          Search documentation of NxSettingsView SourceNx.Defn.Expr(Nx v0.9.2)SummaryFunctions%Nx.Defn.Expr{}The expression used by Nx.Defn.Compiler.cond(clauses, last)Creates a cond tensor expression.metadata(expr, metadata)Creates a tensor expression metadata node wrapping the
given tensor expression.parameter(tensor, pos)Creates a tensor expression parameter at pos based on the given tensor expression.parameter(tensor, context, pos)Creates a tensor expression parameter at pos based on the given tensor and context.parameter(context, type, shape, pos)Creates a tensor expression parameter at pos with the given context, type,
shape, and pos.tensor(tensor)Builds an tensor expression from the given tensor.tuple(expr, list)Creates a tuple with elements in list that points to tuple
expression expr.while(initial, context, arg, condition, body)Creates a while tensor expression.FunctionsLink to this function%Nx.Defn.Expr{}View Source(struct)The expression used by Nx.Defn.Compiler.Nx.Defn.Compiler changes Nx default backend from Nx.BinaryBackend
to Nx.Defn.Expr. It is a struct with the following fields::id - a unique identifier:op - the operation name:args - the operation arguments:context - the context of the expression.
The default context is :root.Convenience functions for traversing expressions and composite types
can be found in Nx.Defn.Composite and Nx.Defn.Tree.Syntax nodesMost nodes are created directly via the Nx module and
therefore map directly to Nx.Tensor callbacks. However
the following syntax nodes exist:parameter(integer)constant(number)tensor(tensor)metadata(expr, metadata)elem(tuple, pos) - created automatically from
expression that return tuples. Note it may return
tuples too, which means we have nested tuplesfun(parameters, t, mfa) - the mfa is used only for
introspection purposescond(clauses, otherwise)while(initial, condition, body)attach_token(token(%Nx.Defn.Token{}), expr)defn compilers must handle said nodes accordingly.Link to this functioncond(clauses, last)View SourceCreates a cond tensor expression.Link to this functionmetadata(expr, metadata)View SourceCreates a tensor expression metadata node wrapping the
given tensor expression.The metadata is map. If the inspect key is present,
it will be used to annotate the metadata when inspected.
Otherwise the metadata node does not appear during
inspection.Link to this functionparameter(tensor, pos)View SourceCreates a tensor expression parameter at pos based on the given tensor expression.Link to this functionparameter(tensor, context, pos)View SourceCreates a tensor expression parameter at pos based on the given tensor and context.Link to this functionparameter(context, type, shape, pos)View SourceCreates a tensor expression parameter at pos with the given context, type,
shape, and pos.Link to this functiontensor(tensor)View SourceBuilds an tensor expression from the given tensor.Link to this functiontuple(expr, list)View SourceCreates a tuple with elements in list that points to tuple
expression expr.list must be a list of tensor expressions of the same size
as the tuple expression.Link to this functionwhile(initial, context, arg, condition, body)View SourceCreates a while tensor expression.Hex PackageHex Preview
            Search HexDocs
          
              Download ePub version
            
        Built using
        ExDoc (v0.34.2) for the

          Elixir programming languageNx.Defn.Kernel — Nx v0.9.2
Nx
        
          v0.9.2
        
Pages
        
            Modules
          Search documentation of NxSettingsView SourceNx.Defn.Kernel(Nx v0.9.2)All imported functionality available inside defn blocks.This module can be used in defn.SummaryFunctionsleft &&& rightElement-wise bitwise AND operation.left ** rightElement-wise power operator.left * rightElement-wise multiplication operator.+tensorElement-wise unary plus operator.left + rightElement-wise addition operator.-tensorElement-wise unary plus operator.left - rightElement-wise subtraction operator...Creates the full-slice range 0..-1//1.first..lastBuilds a range.first..last//stepBuilds a range with step.left / rightElement-wise division operator.left != rightElement-wise inequality operation.left < rightElement-wise less than operation.left <<< rightElement-wise left shift operation.left <= rightElement-wise less-equal operation.left <> rightConcatenates two strings.left == rightElement-wise equality operation.left > rightElement-wise greater than operation.left >= rightElement-wise greater-equal operation.left >>> rightElement-wise right shift operation.@exprReads a module attribute at compilation time.alias(module, opts \\ [])Defines an alias, as in Kernel.SpecialForms.alias/2.left and rightElement-wise logical AND operation.assert_keys(keyword, keys)Asserts the keyword list has the given keys.attach_token(token, expr)Attaches a token to an expression. See hook/3.case(expr, list)Pattern matches the result of expr against the given clauses.cond(opts)Evaluates the expression corresponding to the first
clause that evaluates to a truthy value.create_token()Creates a token for hooks. See hook/3.custom_grad(expr, inputs, fun)Defines a custom gradient for the given expression.div(left, right)Element-wise quotient operator.elem(tuple, index)Gets the element at the zero-based index in tuple.hook(expr, name_or_function)Shortcut for hook/3.hook(expr, name, function)Defines a hook.hook_token(token, expr, name_or_function)Shortcut for hook_token/4.hook_token(token, expr, name, function)Defines a hook with an existing token. See hook/3.if(pred, do_else)Provides if/else expressions.import(module, opts \\ [])Imports functions and macros into the current scope,
as in Kernel.SpecialForms.import/2.inspect(expr, opts \\ [])Converts the given expression into a string.keyword!(keyword, values)Ensures the first argument is a keyword with the given
keys and default values.max(left, right)Element-wise maximum operation.min(left, right)Element-wise minimum operation.not tensorElement-wise logical NOT operation.left or rightElement-wise logical OR operation.print_expr(expr, opts \\ [])Prints the given expression to the terminal.print_value(expr, opts_or_fun \\ [])Shortcut for print_value/3.print_value(expr, fun, opts)Prints the value at runtime to the terminal.raise(message)Raises a runtime exception with the given message.raise(exception, arguments)Raises an exception with the given arguments.rem(left, right)Element-wise remainder operation.require(module, opts \\ [])Requires a module in order to use its macros, as in Kernel.SpecialForms.require/2.stop_grad(expr)Stops computing the gradient for the given expression.tap(value, fun)Pipes value to the given fun and returns the value itself.then(value, fun)Pipes value into the given fun.while(initial, condition_or_generator, opts \\ [], do_block)Defines a while loop.left |> rightPipes the argument on the left to the function call on the right.left ||| rightElement-wise bitwise OR operation.~~~tensorElement-wise bitwise not operation.FunctionsLink to this functionleft &&& rightView SourceElement-wise bitwise AND operation.Only integer tensors are supported.
It delegates to Nx.bitwise_and/2 (supports broadcasting).Examplesdefnand_or(a,b)do{a&&&b,a|||b}endLink to this functionleft ** rightView SourceElement-wise power operator.It delegates to Nx.pow/2 (supports broadcasting).Examplesdefnpow(a,b)doa**bendLink to this functionleft * rightView SourceElement-wise multiplication operator.It delegates to Nx.multiply/2 (supports broadcasting).Examplesdefnmultiply(a,b)doa*bendLink to this macro+tensorView Source(macro)Element-wise unary plus operator.Simply returns the given argument.Examplesdefnplus_and_minus(a)do{+a,-a}endLink to this functionleft + rightView SourceElement-wise addition operator.It delegates to Nx.add/2 (supports broadcasting).Examplesdefnadd(a,b)doa+bendLink to this macro-tensorView Source(macro)Element-wise unary plus operator.It delegates to Nx.negate/1.Examplesdefnplus_and_minus(a)do{+a,-a}endLink to this functionleft - rightView SourceElement-wise subtraction operator.It delegates to Nx.subtract/2 (supports broadcasting).Examplesdefnsubtract(a,b)doa-bendLink to this function..View SourceCreates the full-slice range 0..-1//1.This function returns a range with the following properties:When enumerated, it is emptyWhen used as a slice, it returns the sliced element as isExamplesiex> t=Nx.tensor([1,2,3])iex> t[..]#Nx.Tensor<s32[3][1,2,3]>Link to this functionfirst..lastView SourceBuilds a range.Ranges are inclusive and both sides must be integers.The step of the range is computed based on the first
and last values of the range.Examplesiex> t=Nx.tensor([1,2,3])iex> t[1..2]#Nx.Tensor<s32[2][2,3]>Link to this functionfirst..last//stepView SourceBuilds a range with step.Ranges are inclusive and both sides must be integers.Examplesiex> t=Nx.tensor([1,2,3])iex> t[1..2//1]#Nx.Tensor<s32[2][2,3]>Link to this functionleft / rightView SourceElement-wise division operator.It delegates to Nx.divide/2 (supports broadcasting).Examplesdefndivide(a,b)doa/bendLink to this macroleft != rightView Source(macro)Element-wise inequality operation.It delegates to Nx.not_equal/2.Examplesdefncheck_inequality(a,b)doa!=bendLink to this macroleft < rightView Source(macro)Element-wise less than operation.It delegates to Nx.less/2.Examplesdefncheck_less_than(a,b)doa<bendLink to this functionleft <<< rightView SourceElement-wise left shift operation.Only integer tensors are supported.
It delegates to Nx.left_shift/2 (supports broadcasting).Examplesdefnshift_left_and_right(a,b)do{a<<<b,a>>>b}endLink to this macroleft <= rightView Source(macro)Element-wise less-equal operation.It delegates to Nx.less_equal/2.Examplesdefncheck_less_equal(a,b)doa<=bendLink to this macroleft <> rightView Source(macro)Concatenates two strings.Equivalent to Kernel.<>/2.Link to this macroleft == rightView Source(macro)Element-wise equality operation.It delegates to Nx.equal/2.Examplesdefncheck_equality(a,b)doa==bendLink to this macroleft > rightView Source(macro)Element-wise greater than operation.It delegates to Nx.greater/2.Examplesdefncheck_greater_than(a,b)doa>bendLink to this macroleft >= rightView Source(macro)Element-wise greater-equal operation.It delegates to Nx.greater_equal/2.Examplesdefncheck_greater_equal(a,b)doa>=bendLink to this functionleft >>> rightView SourceElement-wise right shift operation.Only integer tensors are supported.
It delegates to Nx.right_shift/2 (supports broadcasting).Examplesdefnshift_left_and_right(a,b)do{a<<<b,a>>>b}endLink to this macro@exprView Source(macro)Reads a module attribute at compilation time.It is useful to inject code constants into defn.
It delegates to Kernel.@/1.Examples@two_per_twoNx.tensor([[1,2],[3,4]])defnadd_2x2_attribute(t),do:t+@two_per_twoLink to this macroalias(module, opts \\ [])View Source(macro)Defines an alias, as in Kernel.SpecialForms.alias/2.An alias allows you to refer to a module using its aliased
name. For example:defnsome_fun(t)doaliasMath.Helpers,as:MHMH.fft(t)endIf the :as option is not given, the alias defaults to
the last part of the given alias. For example,aliasMath.Helpersis equivalent to:aliasMath.Helpers,as:HelpersFinally, note that aliases define outside of a function also
apply to the function, as they have lexical scope:aliasMath.Helpers,as:MHdefnsome_fun(t)doMH.fft(t)endLink to this macroleft and rightView Source(macro)Element-wise logical AND operation.Zero is considered false, all other numbers
are considered true.It delegates to Nx.logical_and/2 (supports broadcasting).It does not support short-circuiting.Examplesdefnand_or(a,b)do{aandb,aorb}endLink to this functionassert_keys(keyword, keys)View SourceAsserts the keyword list has the given keys.If it succeeds, it returns the given keyword list. Raises
an error otherwise.ExamplesTo assert the tensor is a scalar, you can pass the empty tuple shape:iex> assert_keys([one:1,two:2],[:one,:two])[one:1,two:2]If the keys are not available, an error is raised:iex> assert_keys([one:1,two:2],[:three])** (ArgumentError) expected key :three in keyword list, got: [one: 1, two: 2]Link to this functionattach_token(token, expr)View SourceAttaches a token to an expression. See hook/3.Link to this macrocase(expr, list)View Source(macro)Pattern matches the result of expr against the given clauses.For example:caseNx.shape(tensor)do{_}->implementation_for_rank_one(tensor){_,_}->implementation_for_rank_two(tensor)_->implementation_for_rank_n(tensor)endOpposite to cond/2 and if/2, which can execute the branching
in the device, cases are always expanded when building the
expression, and never on the device. This allows case/2 to work
very similarly to Elixir's own Kernel.SpecialForms.case/2,
with only the following restrictions in place:case inside defn only accepts structs, atoms, integers, and tuples as argumentscase can match on struct names but not on its fieldsguards in case inside defn can only access variables defined within the patternHere is an example of case with guards:caseNx.shape(tensor)do{x,y}whenx>y->implementation_for_tall(tensor){x,y}whenx<y->implementation_for_wide(tensor){x,x}->implementation_for_square(tensor)endLink to this macrocond(opts)View Source(macro)Evaluates the expression corresponding to the first
clause that evaluates to a truthy value.It has the format of:conddocondition1->expr1condition2->expr2true->expr3endThe conditions must be a scalar. Zero is considered false,
any other number is considered true. The booleans false and
true are supported, but any other value will raise.All clauses are normalized to the same type and are broadcast
to the same shape. The last condition must always evaluate to
true. All clauses are executed in the device, unless they can
be determined to always be true/false while building the numerical
expression.ExamplesconddoNx.all(Nx.greater(a,0))->b*cNx.all(Nx.less(a,0))->b+ctrue->b-cendWhen a defn is invoked, all cond clauses are traversed
and expanded in order to build their expressions. This means that,
if you attempt to raise in any clause, then it will always raise.
You can only raise in limited situations inside defn, see
raise/2 for more information.Link to this functioncreate_token()View SourceCreates a token for hooks. See hook/3.Link to this functioncustom_grad(expr, inputs, fun)View SourceDefines a custom gradient for the given expression.It also expects a list of inputs of the gradient and a fun
to compute the gradient. The function will be called with the
current gradient. It must return a list of arguments and their
updated gradient to continue applying grad on.ExamplesFor example, if the gradient of cos(t) were to be
implemented by hand:defcos(t)docustom_grad(Nx.cos(t),[t],fng->[-g*Nx.sin(t)]end)endLink to this functiondiv(left, right)View SourceElement-wise quotient operator.It delegates to Nx.quotient/2 (supports broadcasting).Examplesdefnquotient(a,b)dodiv(a,b)endLink to this functionelem(tuple, index)View SourceGets the element at the zero-based index in tuple.It raises ArgumentError when index is negative or it
is out of range of the tuple elements.Examplesiex> tuple={1,2,3}iex> elem(tuple,0)1Link to this functionhook(expr, name_or_function)View SourceShortcut for hook/3.Link to this functionhook(expr, name, function)View SourceDefines a hook.Hooks are a mechanism to execute an anonymous function for
side-effects with runtime tensor values.Let's see an example:defmoduleHooksdoimportNx.Defndefnadd_and_mult(a,b)doadd=hook(a+b,fntensor->IO.inspect({:add,tensor})end)mult=hook(a*b,fntensor->IO.inspect({:mult,tensor})end){add,mult}endendNote a hook can only access the variables passed as arguments
to the hook. It cannot access any other variable defined in
defn outside of the hook.The defn above defines two hooks, one is called with the
value of a + b and another with a * b. Once you invoke
the function above, you should see this printed:Hooks.add_and_mult(2,3){:add,#Nx.Tensor<s325>}{:mult,#Nx.Tensor<s326>}In other words, the hook function accepts a tensor
expression as argument and it will invoke a custom
function with a tensor value at runtime. hook returns
the result of the given expression. The expression can
be any tensor or a Nx.Container.Note you must return the result of the hook call.
For example, the code below won't inspect the :add
tuple, because the hook is not returned from defn:defnadd_and_mult(a,b)do_add=hook(a+b,fntensor->IO.inspect({:add,tensor})end)mult=hook(a*b,fntensor->IO.inspect({:mult,tensor})end)multendWe will learn how to hook into a value that is not part
of the result in the "Hooks and tokens" section.Named hooksIt is possible to give names to the hooks. This allows them
to be defined or overridden by calling Nx.Defn.jit/2 or
Nx.Defn.stream/2. Let's see an example:defmoduleHooksdoimportNx.Defndefnadd_and_mult(a,b)doadd=hook(a+b,:hooks_add)mult=hook(a*b,:hooks_mult){add,mult}endendNow you can pass the hook as argument as follows:hooks=%{hooks_add:fntensor->IO.inspect{:add,tensor}end}fun=Nx.Defn.jit(&Hooks.add_and_mult/2,hooks:hooks)fun.(Nx.tensor(2),Nx.tensor(3))Important! We recommend to prefix your hook names
by the name of your project to avoid conflicts.If a named hook is not given, compilers can optimize
that away and not transfer the tensor from the device
in the first place.You can also mix named hooks with callbacks:defnadd_and_mult(a,b)doadd=hook(a+b,:hooks_add,fntensor->IO.inspect({:add,tensor})end)mult=hook(a*b,:hooks_mult,fntensor->IO.inspect({:mult,tensor})end){add,mult}endIf a hook with the same name is given to Nx.Defn.jit/2
or Nx.Defn.stream/2, then it will override the default
callback.Hooks and tokensSo far, we have always returned the result of the hook
call. However, what happens if the values we want to
hook are not part of the return value, such as below?defnadd_and_mult(a,b)do_add=hook(a+b,:hooks_add,&IO.inspect({:add,&1}))mult=hook(a*b,:hooks_mult,&IO.inspect({:mult,&1}))multendIn such cases, you must use tokens. Tokens are used to
create an ordering over hooks, ensuring hooks execute
in a certain sequence:defnadd_and_mult(a,b)dotoken=create_token(){token,_add}=hook_token(token,a+b,:hooks_add,&IO.inspect({:add,&1})){token,mult}=hook_token(token,a*b,:hooks_mult,&IO.inspect({:mult,&1}))attach_token(token,mult)endThe example above creates a token and uses hook_token/4
to create hooks attached to their respective tokens. By using a token,
we guarantee that those hooks will be invoked in the order
in which they were defined. Then, at the end of the function,
we attach the token (and its associated hooks) to the result mult.In fact, the hook/3 function is implemented roughly like this:defhook(tensor_expr,name,function)do{token,result}=hook_token(create_token(),tensor_expr,name,function)attach_token(token,result)endNote you must attach the token at the end, otherwise the hooks
will be "lost", as if they were not defined. This also applies
to conditionals and loops. The token must be attached within
the branch they are used. For example, this won't work:token=create_token(){token,result}=ifNx.any(value)dohook_token(token,some_value)elsehook_token(token,another_value)endattach_token(token,result)Instead, you must write:token=create_token()ifNx.any(value)do{token,result}=hook_token(token,some_value)attach_token(token,result)else{token,result}=hook_token(token,another_value)attach_token(token,result)endLink to this functionhook_token(token, expr, name_or_function)View SourceShortcut for hook_token/4.Link to this functionhook_token(token, expr, name, function)View SourceDefines a hook with an existing token. See hook/3.Link to this macroif(pred, do_else)View Source(macro)Provides if/else expressions.The first argument must be a scalar. Zero is considered false,
any other number is considered true. The booleans false and
true are supported, but any other value will raise.The second argument is a keyword list with do and else
blocks. The sides are broadcast to return the same shape
and normalized to return the same type.ExamplesifNx.any(Nx.equal(t,0))do0.0else1/tendIn case else is not given, it is assumed to be 0 with the
same as the do clause. If you want to nest multiple conditionals,
see cond/1 instead.When a defn is invoked, both do/else clauses are traversed
and expanded in order to build their expressions. This means that,
if you attempt to raise in any clause, then it will always raise.
You can only raise in limited situations inside defn, see
raise/2 for more information.Link to this macroimport(module, opts \\ [])View Source(macro)Imports functions and macros into the current scope,
as in Kernel.SpecialForms.import/2.Imports are typically discouraged in favor of alias/2.Examplesdefnsome_fun(t)doimportMath.Helpersfft(t)endLink to this functioninspect(expr, opts \\ [])View SourceConverts the given expression into a string.inspect/2 is used to convert expressions into strings, typically
to be used as part of error messages. If you want to inspect for
debugging, consider using print_expr/2, to print the underlying
expression, or print_value/2 to print the value during execution.defnsquare_shape(tensor)docaseNx.shape(tensor)do{n,n}->nshape->raiseArgumentError,"expected a square tensor: #{inspect(shape)}"endendLink to this functionkeyword!(keyword, values)View SourceEnsures the first argument is a keyword with the given
keys and default values.The second argument must be a list of atoms, specifying
a given key, or tuples specifying a key and a default value.
If any of the keys in the keyword is not defined in
values, it raises an error.This does not validate required keys. For such, use assert_keys/2
instead.This is equivalent to Elixir's Keyword.validate!/2.Examplesiex> keyword!([],[one:1,two:2])|>Enum.sort()[one:1,two:2]iex> keyword!([two:3],[one:1,two:2])|>Enum.sort()[one:1,two:3]If atoms are given, they are supported as keys but do not
provide a default value:iex> keyword!([],[:one,two:2])|>Enum.sort()[two:2]iex> keyword!([one:1],[:one,two:2])|>Enum.sort()[one:1,two:2]Passing an unknown key raises:iex> keyword!([three:3],[one:1,two:2])** (ArgumentError) unknown key :three in [three: 3], expected one of [:one, :two]Link to this functionmax(left, right)View SourceElement-wise maximum operation.It delegates to Nx.max/2 (supports broadcasting).Examplesdefnmin_max(a,b)do{min(a,b),max(a,b)}endLink to this functionmin(left, right)View SourceElement-wise minimum operation.It delegates to Nx.min/2 (supports broadcasting).Examplesdefnmin_max(a,b)do{min(a,b),max(a,b)}endLink to this macronot tensorView Source(macro)Element-wise logical NOT operation.Zero is considered false, all other numbers
are considered true.It delegates to Nx.logical_not/1.Examplesdefnlogical_not(a),do:notaLink to this macroleft or rightView Source(macro)Element-wise logical OR operation.Zero is considered false, all other numbers
are considered true.It delegates to Nx.logical_or/2 (supports broadcasting).It does not support short-circuiting.Examplesdefnand_or(a,b)do{aandb,aorb}endLink to this functionprint_expr(expr, opts \\ [])View SourcePrints the given expression to the terminal.It returns the given expressions.Examplesdefntanh_grad(t)dograd(t,&Nx.tanh/1)|>print_expr()endWhen invoked, it will print the expression being built by defn:#Nx.Tensor<Nx.Defn.Exprparameteras32parametercs32b=tanh[a]f64d=pow[c,2]s32e=add[b,d]f64>Link to this functionprint_value(expr, opts_or_fun \\ [])View SourceShortcut for print_value/3.Link to this functionprint_value(expr, fun, opts)View SourcePrints the value at runtime to the terminal.The given expression is transformed with fun before printing.This function is implemented on top of hook/3 and therefore
has the following restrictions:It can only inspect tensors and Nx.ContainerThe return value of this function must be part of the outputAll options are passed to IO.inspect/2.Examplesdefntanh_grad(t)dograd(t,fnt->t|>Nx.tanh()|>print_value()end)enddefntanh_grad(t)dograd(t,fnt->t|>Nx.tanh()|>print_value(label:"tanh")end)enddefntanh_grad(t)dograd(t,fnt->t|>Nx.tanh()|>print_value(fnt->Nx.sum(t)end)end)endLink to this macroraise(message)View Source(macro)Raises a runtime exception with the given message.See raise/2 for more information on exceptions inside defn.Link to this macroraise(exception, arguments)View Source(macro)Raises an exception with the given arguments.raise/2 is invoked while building the numerical expression,
not inside the device. This means that raise may be invoked
on unexpected situations, as we build the numerical expression.
To better understand those cases, let's see some examples.First, let's start with a valid use case for raise/2: raise
on mismatched shapes. Inside defn, we know the tensor shapes
and types, but not their values, so we can assert on the shape
while building the numerical expression:defnsquare_shape(tensor)docaseNx.shape(tensor)do{n,n}->nshape->raiseArgumentError,"expected a square tensor: #{inspect(shape)}"endendIn the example above, only the matching branch of the case is executed,
so if you give it a 2x2 tensor, it will return 2. However, if you give
it a non-square tensor, it will raise.Now consider this code:defnsome_check(a,b)doifa!=bdoa*belseraise"expected different tensors, got: #{inspect(a)} and #{inspect(b)}"endendIn this case, both a and b are tensors and we are comparing their values.
However, their values are unknown, which means we need to convert the whole
if to a numerical expression and run it on the device. Therefore, once we
convert the else branch, it will execute raise/2, making it so the code
above always raises!In such cases, there are no alternatives. We can't execute exceptions in the
CPU/GPU, so you need to approach the problem under a different perspective.Link to this functionrem(left, right)View SourceElement-wise remainder operation.It delegates to Nx.remainder/2 (supports broadcasting).Examplesdefndivides_by_5?(a)dorem(a,5)|>Nx.any()|>Nx.equal(Nx.tensor(1))endLink to this macrorequire(module, opts \\ [])View Source(macro)Requires a module in order to use its macros, as in Kernel.SpecialForms.require/2.Examplesdefnsome_fun(t)dorequireNumericalMacrosNumericalMacros.some_macrotdo...endendLink to this functionstop_grad(expr)View SourceStops computing the gradient for the given expression.It effectively annotates the gradient for the given
expression is 1.0.Examplesexpr=stop_grad(expr)Link to this macrotap(value, fun)View Source(macro)Pipes value to the given fun and returns the value itself.Useful for running synchronous side effects in a pipeline.ExamplesLet's suppose you want to inspect an expression in the middle of
a pipeline. You could write:a|>Nx.add(b)|>tap(&print_expr/1)|>Nx.multiply(c)Link to this macrothen(value, fun)View Source(macro)Pipes value into the given fun.In other words, it invokes fun with value as argument.
This is most commonly used in pipelines, allowing you
to pipe a value to a function outside of its first argument.Examplesa|>Nx.add(b)|>then(&Nx.subtract(c,&1))Link to this macrowhile(initial, condition_or_generator, opts \\ [], do_block)View Source(macro)Defines a while loop.It expects the initial arguments, a condition expression, and
a block:whileinitial,conditiondoblockendcondition must return a scalar tensor where 0 is false and any
other number is true. The given block will be executed while
condition is true. Each invocation of block must return a
value in the same shape as initial arguments.while will return the value of the last execution of block.
If block is never executed because the initial condition is
false, it returns initial.Note: you must prefer to use the operations in the Nx module,
whenever available, instead of writing your own loops.ExamplesA simple loop that increments x until it is 10 can be written as:whilex=0,Nx.less(x,10)dox+1endHowever, it is important to note that all variables you intend
to use inside the "while" must be explicitly given as argument
to "while". For example, imagine the amount we want to increment
by in the example above is given by a variable y. The following
example is invalid:whilex=0,Nx.less(x,10)dox+yendInstead, both x and y must be passed as variables to while:while{x=0,y},Nx.less(x,10)do{x+y,y}endSimilarly, to compute the factorial of x using while:defnfactorial(x)do{factorial,_}=while{factorial=1,x},Nx.greater(x,1)do{factorial*x,x-1}endfactorialendGeneratorsInspired by Elixir's for-comprehensions,
while in defn supports generators. Generators may be tensors or ranges.Tensor generatorsWhen the generator is a tensor, Nx will traverse its highest dimension.
For example, you could sum a one dimensional tensor as follows:whileacc=0,i<-tensordoacc+iendNote: implementing sum using while, as above, is done as an example.
In practice, you must prefer to use the operations in the Nx module,
whenever available, instead of writing your own loops.One advantage of using generators is that you can also unroll the loop
for performance:whileacc=0,i<-tensor,unroll:truedoacc+iendOr unroll it in batches:whileacc=0,i<-tensor,unroll:4doacc+iendUnrolling means that the the while body is automatically duplicated
a certain amount of times, as if you wrote all iterations by hand. This
makes the final expression larger, which causes a longer compilation
time, however it enables additional compile-time optimizations (such as
fusion), improving the runtime efficiency.In case the tensor for generator is vectorized, :unroll will only
affect the non-vectorized part. For instance, if a tensor has shape {4}
and vectorized axes [x: 2][y: 3], unroll: true will only unroll
the 4 inner iterations.Range generatorsA range can also be given as a generator. The range may be increasing or
decreasing. Also remember that ranges in Elixir are inclusive on both
begin and end. The sum example from the previous section could also be
written with ranges:while{tensor,acc=0},i<-0..Nx.axis_size(tensor,0)-1doacc+tensor[i]endLink to this macroleft |> rightView Source(macro)Pipes the argument on the left to the function call on the right.It delegates to Kernel.|>/2.Examplesdefnexp_sum(t)dot|>Nx.exp()|>Nx.sum()endLink to this functionleft ||| rightView SourceElement-wise bitwise OR operation.Only integer tensors are supported.
It delegates to Nx.bitwise_or/2 (supports broadcasting).Examplesdefnand_or(a,b)do{a&&&b,a|||b}endLink to this function~~~tensorView SourceElement-wise bitwise not operation.Only integer tensors are supported.
It delegates to Nx.bitwise_not/1.Examplesdefnbnot(a),do:~~~aHex PackageHex Preview
            Search HexDocs
          
              Download ePub version
            
        Built using
        ExDoc (v0.34.2) for the

          Elixir programming languageNx.Defn.Token — Nx v0.9.2
Nx
        
          v0.9.2
        
Pages
        
            Modules
          Search documentation of NxSettingsView SourceNx.Defn.Token(Nx v0.9.2)A defn token used by hooks.Documentation for compilersThe token has a hooks field as a list of maps of the shape:%{expr:Nx.Tensor.t|Nx.Container.t,name:atom(),callback:(Nx.Tensor.t|Nx.Container.t->term())|nil}The hooks field must only be accessed by defn compilers.Hex PackageHex Preview
            Search HexDocs
          
              Download ePub version
            
        Built using
        ExDoc (v0.34.2) for the

          Elixir programming languageNx.Defn.Tree — Nx v0.9.2
Nx
        
          v0.9.2
        
Pages
        
            Modules
          Search documentation of NxSettingsView SourceNx.Defn.Tree(Nx v0.9.2)Helper functions to traverse defn expressions,
either as single nodes or in-depth.SummaryFunctionsapply_args(expr, type \\ :all, acc, fun)Applies the given function to the arguments of the node,
with the given accumulator as a starting value.has_hooks?(tree, hooks)Check if the given tree has any of the given hooks in it.put_args(t, args)Puts new args in the given tensor expression and gives it a new id.scope_ids(expr, ids \\ %{})Gets all IDs of all elements in the same scope.FunctionsLink to this functionapply_args(expr, type \\ :all, acc, fun)View SourceApplies the given function to the arguments of the node,
with the given accumulator as a starting value.By default, type is :all, which means all arguments
are traversed. If type is :scope, only expressions
that are in the same scope are traversed. Therefore,
expressions such as while's condition and body,
optional's default implementation, functions, and so forth
are not traversed. Note conds are always traversed because,
while they introduce a new scope, they can also access its
parents directly, so you must take conds into account
accordingly.Warning: be very careful when using this function to traverse
the expression recursively. If you plan to do so, you should
consider also storing the visited nodes to avoid multiple
traversals by using tensor.data.expr.id as cache key.Link to this functionhas_hooks?(tree, hooks)View SourceCheck if the given tree has any of the given hooks in it.Link to this functionput_args(t, args)View SourcePuts new args in the given tensor expression and gives it a new id.Link to this functionscope_ids(expr, ids \\ %{})View SourceGets all IDs of all elements in the same scope.while's condition and body, fun's body and similar are
considered different scopes. When it comes to cond, an ID will
only be considered if it is used outside of the cond or used
in several distinct conds. Constants are also ignored, as they
have global IDs based on the constants themselves.An existing map of ids can be given to accumulate on top of it.Hex PackageHex Preview
            Search HexDocs
          
              Download ePub version
            
        Built using
        ExDoc (v0.34.2) for the

          Elixir programming languageNx.Defn — Nx v0.9.2
Nx
        
          v0.9.2
        
Pages
        
            Modules
          Search documentation of NxSettingsView SourceNx.Defn(Nx v0.9.2)Numerical functions.A numerical function is a subset of Elixir tailored for
numerical computations. For example, the following function:defmoduleMyModuledoimportNx.Defndefnsoftmax(t)doNx.exp(t)/Nx.sum(Nx.exp(t))endendwill work with scalars, vector, matrices, and n-dimensional
tensors. Depending on your compiler of choice, the code can even
be JIT-compiled and run either on the CPU or GPU.To support these features, defn is a subset of Elixir. It
replaces Elixir's Kernel by Nx.Defn.Kernel. Nx.Defn.Kernel
provides tensor-aware operators, such as +, -, etc, while
also preserving many high-level constructs known to Elixir
developers, such as pipe operator, aliases, conditionals,
pattern-matching, the access syntax, and more:For example, the code above can also be written as:defmoduleMyModuledoimportNx.Defndefnsoftmax(t)dot|>Nx.exp()|>then(&&1/Nx.sum(&1))endendPlease consult Nx.Defn.Kernel for a complete reference.Some of the functions in this module may also be used within
defn.Operatorsdefn attempts to keep as close to the Elixir semantics as
possible but that's not achievable. For example, mathematical
and bitwise operators (+, -, &&&, <<<, etc.) in Elixir
work on numbers, which means mapping them to tensors is
straight-forward and they largely preserve the same semantics,
except they are now multi-dimensional.On the other hand, the logical operators and, or, and not
work with booleans in Elixir (true and false), which map
to 0 and 1 in defn.Therefore, when working with logical operators inside defn,
0 is considered false and all other numbers are considered
true, which is represented as the number 1. For example, in
defn, 0 and 1 as well as 0 and 2 return 0, while
1 and 1 or 1 and -1 will return 1.The same semantics apply to conditional expressions inside defn,
such as if, while, etc.JIT compilersThe power of Nx.Defn is given by its compilers. The default
compiler is Nx.Defn.Evaluator, which evalutes the code.
You can use jit/3 to compile a function on the fly using a
different compiler, such as EXLA:fun=Nx.Defn.jit(&MyModule.softmax/1,compiler:EXLA)fun.(my_tensor)The above will return an anonymous function that optimizes,
compiles, and run softmax on the fly on the CPU (or the GPU)
if available. EXLA, in particular, also exports a EXLA.jit/2
function for convenience.defn functions are compiled when they are invoked, based on
the type and shapes of the tensors given as arguments.
Therefore compilation may be quite time consuming on the first
invocation. The compilation is then cached based on the tensors
shapes and types. Calling the same function with a tensor of
different values but same shape and type means no recompilation
is performed.For those interested in writing custom compilers, see Nx.Defn.Compiler.Invoking custom Elixir codeInside defn you can only call other defn functions and
the functions in the Nx module. However, it is possible
to use transforms, defined with either deftransform or
deftransformp to invoke any Elixir code.You can call code which was defined with deftransform from another module:defmoduleMyRemoteModuledoimportNx.Defndeftransformremote_elixir_code(value)doIO.inspect(value)endenddefnadd_and_mult(a,b,c)dores=a*b+cMyRemoteModule.remote_elixir_code(res)endYou can also define and call a private transform defined through deftransformp:defnadd_and_mult(a,b,c)dores=a*b+ccustom_elixir_code(res)enddeftransformpcustom_elixir_code(value),do:IO.inspect(value)The only difference between using deftransform and deftransformp
is whether you want to expose and share the code with other modules,
just like def and defp.Transforms are useful to manipulate tensor expressions or
Elixir data structures without the constraints of defn.Inputs and outputs typesNx and defn expect the arguments to be numbers, tensors,
or one composite data type that implements Nx.LazyContainer.
Tuples and maps implement Nx.LazyContainer by default.
As previously described, defn are cached based on the shape,
type, and names of the input tensors, but not their values.defn also accepts two special arguments: functions (or tuples
of functions) and lists (most commonly as keyword lists). Those
values are passed as is to numerical definitions and cached as
a whole. For this reason, you must never capture tensors in
functions or pass tensors in keyword lists.When numbers are given as arguments, they are always immediately
converted to tensors on invocation. If you want to keep numbers
as is or if you want to pass any other value to numerical definitions,
they must be given as keyword lists.Default argumentsdefn functions support default arguments. They are typically used
as options. For example, imagine you want to create a function named
zeros, which returns a tensor of zeroes with a given type and shape.
It could be implemented like this:defnzeros(opts\\[])doopts=keyword!(opts,type:{:f,32},shape:{})Nx.broadcast(Nx.tensor(0,type:opts[:type]),opts[:shape])endThe function above accepts opts which are then validated and given
default values via the keyword!/2 function. Note that while it is
possible to access options via the Access syntax, such as opts[:shape],
it is not possible to directly call functions in the Keyword module
inside defn. To freely manipulate any Elixir value inside defn,
you have to use transforms, as described in the "Invoking custom Elixir
code" section.Important! When it comes to JIT compilation, each different set of
options (as well as anonymous functions) will lead to a different
compilation of the numerical function.Furthermore, if tensors are given through keyword lists, they won't
be cached effectively. Tensors in defn are cached based on their shape
and type, not their value, but this is not true if the tensor is given
via a default argument or captured by an anonymous function. For this
reason, it is extremely discouraged to pass tensors through anonymous
functions and default arguments.Working with maps and structsWhile Nx supports maps in defn, you must be careful if your numerical
definitions are receiving maps and returning maps. For example, imagine
this code:defnupdate_a(map)do%{map|a:Nx.add(map.a,1)}endThe following code increments the value under the key :a
by 1. However, because the function receives the whole map and
returns the whole map, it means if the map has 120 keys, the
whole map will be copied to the CPU/GPU, and then brought back.However, if you do this instead:defnupdate_a(map)doNx.add(map.a,1)endAnd then update the map on Elixir, outside of defn:%{map|a:update_a(map)}Nx will only send the parts of the map that matters.Recursion and loopsGiven numerical definition first build a representation of
your code, it is not possible to write recursive (nor tail
recursive) code inside defn. Instead, one must use
Nx.Defn.Kernel.while/4.SummaryFunctionscompile(fun, template_args, opts \\ [])Compiles the given anonymous function with the given tensor shapes.debug_expr(fun, opts \\ [])Wraps an anonymous function to return its underlying defn expression.debug_expr_apply(fun, args, opts \\ [])Invokes the anonymous function to return its underlying defn expression.default_options()Gets the default options for the current process.default_options(options)Sets the default options for defn in the current process.defn(call, list)Defines a public numerical function.defnp(call, list)Defines a private numerical function.deftransform(call)Can be used to define bodiless clauses for multi-clause transforms.deftransform(call, list)Defines a transform that executes the given fun with arg
when building defn expressions.deftransformp(call)Private function version for deftransform/1deftransformp(call, list)Private function version for deftransform/2global_default_options(options)Sets the default options globally.grad(fun)Receives an anonymous function and returns a new anonymous function
that returns the gradient of the input function when invoked.grad(var_or_vars, fun)Computes the gradient of the given var on fun.jit(fun, opts \\ [])Wraps an anonymous function with just-in-time compilation.jit_apply(fun, args, opts \\ [])Invokes the anonymous function with just-in-time compilation.stream(fun, args, opts \\ [])deprecatedStarts streaming the given anonymous function with just-in-time
compilation.to_backend(opts)Returns a backend corresponding to the compiler options.value_and_grad(fun)Receives an anonymous function and returns a new anonymous function
that returns the value and gradient of the input function when invoked.value_and_grad(var_or_vars, fun, transform \\ & &1)Computes the value and gradient of the given var on fun
with an optional data transformation.FunctionsLink to this functioncompile(fun, template_args, opts \\ [])View SourceCompiles the given anonymous function with the given tensor shapes.While jit/2 compiles a function just-in time based on the
input shapes, this function precompiles the given anonymous
function based on the input shapes. This can be beneficial for
large numerical definitions, where the cache mechanism in jit/2
may take milliseconds.For example, take the following definition:defnsoftmax(t),do:Nx.exp(t)/Nx.sum(Nx.exp(t))You can jit and then apply it as:fun=Nx.Defn.compile(&softmax/1,[Nx.template({3},{:s,32})],compiler:EXLA)fun.(Nx.tensor([1,2,3]))You can also pass a mixture of templates and options when
compiling a function. In such cases, you must only pass
the inputs when invoking the compiled function, as the options
will already be embedded in its compiled value:fun=Nx.Defn.compile(&Nx.sum/2,[Nx.template({2,2},{:s,32}),[axes:[1]]])fun.(Nx.iota({2,2}))If the input tensors do not match the shape of the tensors
given on compilation, it will raise.Options:compiler - the compiler for the JIT compilation:hooks - a map of hooks to execute. See Nx.Defn.Kernel.hook/3Link to this functiondebug_expr(fun, opts \\ [])View SourceWraps an anonymous function to return its underlying defn expression.WarningThis function must be invoked for debugging purposes only.Options:hooks - a map of hooks to execute. See Nx.Defn.Kernel.hook/3Link to this functiondebug_expr_apply(fun, args, opts \\ [])View SourceInvokes the anonymous function to return its underlying defn expression.WarningThis function must be invoked for debugging purposes only.It accepts the same options as debug_expr/2.Link to this functiondefault_options()View SourceGets the default options for the current process.Link to this functiondefault_options(options)View SourceSets the default options for defn in the current process.The options defined here apply to all future invocations of
defn done by the current process. It also applies to calls
to the jit/3 and stream/3 functions in this module.The default options are stored only in the process dictionary
and override any global options. This means if you start a
separate process, such as Task, the default options must be
set on the new process too.The function returns the values that were previously set as default
options.This function must be used only for scripting and testing.Examplesiex> Nx.Defn.default_options(compiler:EXLA,client::cuda)iex> Nx.Defn.default_options()[compiler:EXLA,client::cuda]Link to this macrodefn(call, list)View Source(macro)Defines a public numerical function.Link to this macrodefnp(call, list)View Source(macro)Defines a private numerical function.Private numerical functions are always inlined by
their callers at compilation time. This happens to
all local function calls within defn.Link to this macrodeftransform(call)View Source(macro)Can be used to define bodiless clauses for multi-clause transforms.See also: deftransform/2Examplesdeftransformfoo(bar,baz\1)deftransformfoo(bar,1),do:bardeftransformfoo(bar,baz),do:bar+bazLink to this macrodeftransform(call, list)View Source(macro)Defines a transform that executes the given fun with arg
when building defn expressions.ExampleTake the following defn expression:defntanh_power(a,b)doNx.tanh(a)+Nx.pow(b,2)endLet's see a trivial example, which is to use IO.inspect/1 to
print a tensor expression at definition time:defntanh_power(a,b)doNx.tanh(a)+Nx.pow(b,2)|>my_inspect()enddeftransformpmy_inspect(expr),do:IO.inspect(expr)Or:defntanh_power(a,b)dores=Nx.tanh(a)+Nx.pow(b,2)my_inspect(res)resendWhen invoked in both cases, it will print the expression being built
by defn:#Nx.Defn.Expr<parameteraparametercb=tanh[a]()d=pow[c,2]()e=add[b,d]()>Although, for convenience, you might use print_expr/2 instead.Link to this macrodeftransformp(call)View Source(macro)Private function version for deftransform/1Link to this macrodeftransformp(call, list)View Source(macro)Private function version for deftransform/2Link to this functionglobal_default_options(options)View SourceSets the default options globally.The options defined here apply to all future invocations of
defn. It also applies to calls to the jit/3 and stream/3
functions in this module.You must avoid calling this function at runtime and mostly for
testing purposes. You may also set in your test environment using
configuration:config:nx,:default_defn_options,[compiler:EXLA,client::cuda]The function returns the values that were previously set as global
default options.Link to this functiongrad(fun)View SourceReceives an anonymous function and returns a new anonymous function
that returns the gradient of the input function when invoked.Examplesiex> fun=Nx.Defn.grad(fnx->Nx.sin(x)end)iex> fun.(Nx.tensor(0))#Nx.Tensor<f321.0>Link to this functiongrad(var_or_vars, fun)View SourceComputes the gradient of the given var on fun.The result of the grad function must be a scalar tensor.
If a non-scalar tensor is given, it is assumed the additional
dimensions are batch dimensions.Examplesdefntanh_grad(t)dograd(t,&Nx.tanh/1)endTo differentiate on multiple vars, pass a tuple as first argument:defntanh_power_grad(a,b)dograd({a,b},fn{a,b}->Nx.tanh(a)+Nx.pow(b,2)end)endvar_or_vars can be any Nx.Container with one or multiple
tensors.Link to this functionjit(fun, opts \\ [])View SourceWraps an anonymous function with just-in-time compilation.Once invoked, the wrapped anonymous function will perform just
in time compilation with the configured compiler. For example,
take the following definition:defnsoftmax(t),do:Nx.exp(t)/Nx.sum(Nx.exp(t))You can jit and then apply it as:fun=Nx.Defn.jit(&softmax/1,compiler:EXLA)fun.(Nx.tensor([1,2,3]))Options:compiler - the compiler for the JIT compilation:hooks - a map of hooks to execute. See Nx.Defn.Kernel.hook/3:on_conflict - what to do if a JIT compilation is already in place.
It may be :raise (the default), :force (forces a new JIT compilation),
or :reuse (reuses the exiting JIT compilation). It is not recommended
to set the :compiler option when reusing.Link to this functionjit_apply(fun, args, opts \\ [])View SourceInvokes the anonymous function with just-in-time compilation.This function is equivalent to calling jit/2 and then applying
the given arguments to the anonymous function.For example, take the following definition:defnsoftmax(t),do:Nx.exp(t)/Nx.sum(Nx.exp(t))You can jit_apply/3 it as:Nx.Defn.jit_apply(&softmax/1,[Nx.tensor([1,2,3])],compiler:EXLA)It accepts the same options as jit/2.Link to this functionstream(fun, args, opts \\ [])View Source
      This function is deprecated. Move the streaming loop to Elixir instead.
    Starts streaming the given anonymous function with just-in-time
compilation.At least two arguments are expected:The first argument is a tensor template of the data to
be streamed inThe second argument is a tensor with the stream initial stateThe streaming function must return a two element tuple, the
first element is the data to be sent and the second is the
accumulator.For each streamed chunk, you must call Nx.Stream.send/2 and
Nx.Stream.recv/1. You don't need to call recv immediately
after send, but doing so can be a useful mechanism to provide
backpressure. Once all chunks are sent, you must use Nx.Stream.done/1
to receive the accumulated result. Let's see an example:defmoduleStreameddoimportNx.Defndefnsum(tensor,acc)do{acc,tensor+acc}endendNow let's invoke it:stream=Nx.Defn.stream(&Streamed.sum/2,[Nx.template({},{:s,32}),0])fori<-1..5doNx.Stream.send(stream,i)IO.inspect{:chunk,Nx.Stream.recv(stream)}endIO.inspect{:result,Nx.Stream.done(stream)}It will print:{:chunk,0}{:chunk,1}{:chunk,2}{:chunk,3}{:chunk,4}{:result,5}Options:hooks - a map of hooks to execute. See Nx.Defn.Kernel.hook/3Beware: deadlocksSome backends (such as XLA) place locks around devices. For example,
if you start streaming on the GPU, you cannot perform any other
operation on the GPU until streaming is over.This means if we modify the loop above to the following:fori<-1..5doNx.Stream.send(stream,Nx.tensor(i)|>Nx.multiply(2))IO.inspect{:chunk,Nx.Stream.recv(stream)}endThe loop may deadlock at the time it performs the multiplication.
In practice, this means you should perform the streaming on the GPU
and the remaining operations on the CPU. If you only have a single
device (i.e. only a CPU), then it may not be possible to perform the
above and you will have to restructure your code to manipulate the
input before streaming starts.Link to this functionto_backend(opts)View SourceReturns a backend corresponding to the compiler options.The backend matches the backend used for outputs from computations
defined by the given compiler.Link to this functionvalue_and_grad(fun)View SourceReceives an anonymous function and returns a new anonymous function
that returns the value and gradient of the input function when invoked.Examplesiex> fun=Nx.Defn.value_and_grad(fnx->Nx.sin(x)end)iex> {value,grad}=fun.(Nx.tensor(0))iex> value#Nx.Tensor<f320.0>iex> grad#Nx.Tensor<f321.0>Link to this functionvalue_and_grad(var_or_vars, fun, transform \\ & &1)View SourceComputes the value and gradient of the given var on fun
with an optional data transformation.It returns a tuple with the value and the gradient.Examplesdefntanh_grad(t)dovalue_and_grad(t,&Nx.tanh/1)endTo differentiate on multiple vars, pass a tuple as first argument:defntanh_power_grad(a,b)dovalue_and_grad({a,b},fn{a,b}->Nx.tanh(a)+Nx.pow(b,2)end)endvar_or_vars can be any Nx.Container with one or multiple
tensors.transform allows you to transform the expression before the gradient is
calculated. This enables optimizations that reuse parts of expressions. As
an example, consider the following objective function:defnobjective(predict_fn,loss_fn,params,inputs,targets)dopreds=predict_fn.(params,inputs)loss=loss_fn.(preds,targets){preds,loss}endYou can compute the gradient with respect to just the loss function by applying
a transform:{{preds,loss},gradient}=value_and_grad(params,&objective(predict_fn,loss_fn,&1,inputs,targets),&elem(&1,1))preds can be re-used to compute other metrics such as accuracy, absolute error,
etc. without having to do another forward pass.Hex PackageHex Preview
            Search HexDocs
          
              Download ePub version
            
        Built using
        ExDoc (v0.34.2) for the

          Elixir programming languageNx.Heatmap — Nx v0.9.2
Nx
        
          v0.9.2
        
Pages
        
            Modules
          Search documentation of NxSettingsView SourceNx.Heatmap(Nx v0.9.2)Provides a heatmap that is printed using ANSI colors
in the terminal.Hex PackageHex Preview
            Search HexDocs
          
              Download ePub version
            
        Built using
        ExDoc (v0.34.2) for the

          Elixir programming languageNx.LazyContainer — Nx v0.9.2
Nx
        
          v0.9.2
        
Pages
        
            Modules
          Search documentation of NxSettingsView SourceNx.LazyContainerprotocol(Nx v0.9.2)Converts a data structure to a container lazily.When you invoke a defn, its arguments must implement
the Nx.LazyContainer protocol and return a data structure
that implements Nx.Container. In other words, this
protocol teaches Nx to work with additional data types
beyond numbers and tensors.This module provides a single traverse implementation
that emits the tensor template and a function that computes
the tensor as two distinct values. Then a tensor is only
allocated if necessary.Nx.LazyContainer is automatically provided for data
structures that implement Nx.Container.SummaryTypest()All the types that implement this protocol.Functionstraverse(data, acc, fun)Traverses recursively tensors in a data structure with acc and fun.TypesLink to this typet()View Source@type t() :: term()All the types that implement this protocol.FunctionsLink to this functiontraverse(data, acc, fun)View Source@spec traverse(t(), acc, (Nx.template(), (-> Nx.Tensor.t()), acc -> {term(), acc})) ::
  {Nx.Container.t(), acc}
when acc: term()Traverses recursively tensors in a data structure with acc and fun.For each tensor in the container, fun receives a tensor
template, an anonymous function to build the actual tensor,
and the accumulator . It returns a two element tuple with
a non-lazy Nx.Container and the accumulator.This function returns the updated container and the accumulator.Note this function is recursive by default. Therefore if you
are implementing this function and one of your arguments may
be containers, you must call Nx.LazyContainer.traverse/3
on said arguments so they are recursively traversed.Hex PackageHex Preview
            Search HexDocs
          
              Download ePub version
            
        Built using
        ExDoc (v0.34.2) for the

          Elixir programming languageNx.LinAlg.Cholesky — Nx v0.9.2
Nx
        
          v0.9.2
        
Pages
        
            Modules
          Search documentation of NxSettingsView SourceNx.LinAlg.Cholesky(Nx v0.9.2)SummaryFunctionscholesky(a, opts \\ [])cholesky_grad(l, arg2, g)cholesky_matrix(a, opts \\ [])FunctionsLink to this functioncholesky(a, opts \\ [])View SourceLink to this functioncholesky_grad(l, arg2, g)View SourceLink to this functioncholesky_matrix(a, opts \\ [])View SourceHex PackageHex Preview
            Search HexDocs
          
              Download ePub version
            
        Built using
        ExDoc (v0.34.2) for the

          Elixir programming languageNx.LinAlg.Eigh — Nx v0.9.2
Nx
        
          v0.9.2
        
Pages
        
            Modules
          Search documentation of NxSettingsView SourceNx.LinAlg.Eigh(Nx v0.9.2)SummaryFunctionseigh(a, opts \\ [])FunctionsLink to this functioneigh(a, opts \\ [])View SourceHex PackageHex Preview
            Search HexDocs
          
              Download ePub version
            
        Built using
        ExDoc (v0.34.2) for the

          Elixir programming languageNx.LinAlg.QR — Nx v0.9.2
Nx
        
          v0.9.2
        
Pages
        
            Modules
          Search documentation of NxSettingsView SourceNx.LinAlg.QR(Nx v0.9.2)SummaryFunctionshouseholder_reflector(x, i, eps)qr(a, opts \\ [])qr_grad(arg1, arg2, arg3)FunctionsLink to this functionhouseholder_reflector(x, i, eps)View SourceLink to this functionqr(a, opts \\ [])View SourceLink to this functionqr_grad(arg1, arg2, arg3)View SourceHex PackageHex Preview
            Search HexDocs
          
              Download ePub version
            
        Built using
        ExDoc (v0.34.2) for the

          Elixir programming languageNx.LinAlg — Nx v0.9.2
Nx
        
          v0.9.2
        
Pages
        
            Modules
          Search documentation of NxSettingsView SourceNx.LinAlg(Nx v0.9.2)Nx conveniences for linear algebra.This module can be used in defn.SummaryFunctionsadjoint(t)Returns the adjoint of a given tensor.cholesky(tensor)Performs a Cholesky decomposition of a batch of square matrices.determinant(tensor)Calculates the determinant of batched square matrices.eigh(tensor, opts \\ [])Calculates the Eigenvalues and Eigenvectors of batched Hermitian 2-D matrices.invert(tensor)Inverts a batch of square matrices.least_squares(a, b, opts \\ [])Return the least-squares solution to a linear matrix equation Ax = b.lu(tensor, opts \\ [])Calculates the A = PLU decomposition of batched square 2-D matrices A.matrix_power(tensor, power)Produces the tensor taken to the given power by matrix dot-product.matrix_rank(a, opts \\ [])Return matrix rank of input M × N matrix using Singular Value Decomposition method.norm(tensor, opts \\ [])Calculates the p-norm of a tensor.pinv(tensor, opts \\ [])Calculates the Moore-Penrose inverse, or the pseudoinverse, of a matrix.qr(tensor, opts \\ [])Calculates the QR decomposition of a tensor with shape {..., M, N}.solve(a, b)Solves the system AX = B.svd(tensor, opts \\ [])Calculates the Singular Value Decomposition of batched 2-D matrices.triangular_solve(a, b, opts \\ [])Solve the equation a x = b for x, assuming a is a batch of triangular matrices.
Can also solve x a = b for x. See the :left_side option below.FunctionsLink to this functionadjoint(t)View SourceReturns the adjoint of a given tensor.If the input tensor is real it transposes it's two inner-most axes.
If the input tensor is complex, it additionally applies Nx.conjugate/1 to it.Examplesiex> Nx.LinAlg.adjoint(Nx.tensor([[1,2],[3,4]]))#Nx.Tensor<s32[2][2][[1,3],[2,4]]>iex> Nx.LinAlg.adjoint(Nx.tensor([[1,Complex.new(0,2)],[3,Complex.new(0,-4)]]))#Nx.Tensor<c64[2][2][[1.0-0.0i,3.0-0.0i],[0.0-2.0i,0.0+4.0i]]>Link to this functioncholesky(tensor)View SourcePerforms a Cholesky decomposition of a batch of square matrices.The matrices must be positive-definite and either Hermitian
if complex or symmetric if real. An error is raised by the
default backend if those conditions are not met. Other
backends may emit undefined behaviour.Examplesiex> Nx.LinAlg.cholesky(Nx.tensor([[20.0,17.6],[17.6,16.0]]))#Nx.Tensor<f32[2][2][[4.4721360206604,0.0],[3.9354796409606934,0.7155418395996094]]>iex> Nx.LinAlg.cholesky(Nx.tensor([[[2.0,3.0],[3.0,5.0]],[[1.0,0.0],[0.0,1.0]]]))#Nx.Tensor<f32[2][2][2][[[1.4142135381698608,0.0],[2.1213204860687256,0.7071064710617065]],[[1.0,0.0],[0.0,1.0]]]>iex> t=Nx.tensor([...> [6.0,3.0,4.0,8.0],...> [3.0,6.0,5.0,1.0],...> [4.0,5.0,10.0,7.0],...> [8.0,1.0,7.0,25.0]...> ])iex> Nx.LinAlg.cholesky(t)#Nx.Tensor<f32[4][4][[2.4494898319244385,0.0,0.0,0.0],[1.2247447967529297,2.1213202476501465,0.0,0.0],[1.6329931020736694,1.41421377658844,2.309401035308838,0.0],[3.265986204147339,-1.4142134189605713,1.5877134799957275,3.132491111755371]]>iex> Nx.LinAlg.cholesky(Nx.tensor([[1.0,Complex.new(0,-2)],[Complex.new(0,2),5.0]]))#Nx.Tensor<c64[2][2][[1.0+0.0i,0.0+0.0i],[0.0+2.0i,1.0+0.0i]]>iex> t=Nx.tensor([[[2.0,3.0],[3.0,5.0]],[[1.0,0.0],[0.0,1.0]]])|>Nx.vectorize(x:2)iex> Nx.LinAlg.cholesky(t)#Nx.Tensor<vectorized[x:2]f32[2][2][[[1.4142135381698608,0.0],[2.1213204860687256,0.7071064710617065]],[[1.0,0.0],[0.0,1.0]]]>Link to this functiondeterminant(tensor)View SourceCalculates the determinant of batched square matrices.ExamplesFor 2x2 and 3x3, the results are given by the closed formulas:iex> Nx.LinAlg.determinant(Nx.tensor([[1,2],[3,4]]))#Nx.Tensor<f32-2.0>iex> Nx.LinAlg.determinant(Nx.tensor([[1.0,2.0,3.0],[1.0,-2.0,3.0],[7.0,8.0,9.0]]))#Nx.Tensor<f3248.0>When there are linearly dependent rows or columns, the determinant is 0:iex> Nx.LinAlg.determinant(Nx.tensor([[1.0,0.0],[3.0,0.0]]))#Nx.Tensor<f320.0>iex> Nx.LinAlg.determinant(Nx.tensor([[1.0,2.0,3.0],[-1.0,-2.0,-3.0],[4.0,5.0,6.0]]))#Nx.Tensor<f320.0>The determinant can also be calculated when the axes are bigger than 3:iex> Nx.LinAlg.determinant(Nx.tensor([...> [1,0,0,0],...> [0,1,2,3],...> [0,1,-2,3],...> [0,7,8,9.0]...> ]))#Nx.Tensor<f3248.0>iex> Nx.LinAlg.determinant(Nx.tensor([...> [0,0,0,0,-1],...> [0,1,2,3,0],...> [0,1,-2,3,0],...> [0,7,8,9,0],...> [1,0,0,0,0]...> ]))#Nx.Tensor<f3248.0>iex> Nx.LinAlg.determinant(Nx.tensor([...> [[2,4,6,7],[5,1,8,8],[1,7,3,1],[3,9,2,4]],...> [[2,5,1,3],[4,1,7,9],[6,8,3,2],[7,8,1,4]]...> ]))#Nx.Tensor<f32[2][630.0,630.0]>iex> t=Nx.tensor([[[1,0],[0,2]],[[3,0],[0,4]]])|>Nx.vectorize(x:2)iex> Nx.LinAlg.determinant(t)#Nx.Tensor<vectorized[x:2]f32[2.0,12.0]>If the axes are named, their names are not preserved in the output:iex> two_by_two=Nx.tensor([[1,2],[3,4]],names:[:x,:y])iex> Nx.LinAlg.determinant(two_by_two)#Nx.Tensor<f32-2.0>iex> three_by_three=Nx.tensor([[1.0,2.0,3.0],[1.0,-2.0,3.0],[7.0,8.0,9.0]],names:[:x,:y])iex> Nx.LinAlg.determinant(three_by_three)#Nx.Tensor<f3248.0>Also supports complex inputs:iex> t=Nx.tensor([[1,0,0],[0,Complex.new(0,2),0],[0,0,3]])iex> Nx.LinAlg.determinant(t)#Nx.Tensor<c640.0+6.0i>iex> t=Nx.tensor([[0,0,0,1],[0,Complex.new(0,2),0,0],[0,0,3,0],[1,0,0,0]])iex> Nx.LinAlg.determinant(t)#Nx.Tensor<c64-0.0-6.0i>Link to this functioneigh(tensor, opts \\ [])View SourceCalculates the Eigenvalues and Eigenvectors of batched Hermitian 2-D matrices.It returns {eigenvals, eigenvecs}.Options:max_iter - integer. Defaults to 1_000
Number of maximum iterations before stopping the decomposition:eps - float. Defaults to 1.0e-4
Tolerance applied during the decompositionNote not all options apply to all backends, as backends may have
specific optimizations that render these mechanisms unnecessary.Examplesiex> {eigenvals,eigenvecs}=Nx.LinAlg.eigh(Nx.tensor([[1,0],[0,2]]))iex> Nx.round(eigenvals)#Nx.Tensor<f32[2][1.0,2.0]>iex> eigenvecs#Nx.Tensor<f32[2][2][[1.0,0.0],[0.0,1.0]]>iex> {eigenvals,eigenvecs}=Nx.LinAlg.eigh(Nx.tensor([[0,1,2],[1,0,2],[2,2,3]]))iex> Nx.round(eigenvals)#Nx.Tensor<f32[3][5.0,-1.0,-1.0]>iex> eigenvecs#Nx.Tensor<f32[3][3][[0.4075949788093567,0.9131628274917603,0.0],[0.40837883949279785,-0.18228201568126678,0.8944271802902222],[0.8167576789855957,-0.36456403136253357,-0.4472135901451111]]>iex> {eigenvals,eigenvecs}=Nx.LinAlg.eigh(Nx.tensor([[[2,5],[5,6]],[[1,0],[0,4]]]))iex> Nx.round(eigenvals)#Nx.Tensor<f32[2][2][[9.0,-1.0],[1.0,4.0]]>iex> eigenvecs#Nx.Tensor<f32[2][2][2][[[0.5612090229988098,-0.8276740908622742],[0.8276740908622742,0.5612090229988098]],[[1.0,0.0],[0.0,1.0]]]>iex> t=Nx.tensor([[[2,5],[5,6]],[[1,0],[0,4]]])|>Nx.vectorize(x:2)iex> {eigenvals,eigenvecs}=Nx.LinAlg.eigh(t)iex> Nx.round(eigenvals)#Nx.Tensor<vectorized[x:2]f32[2][[9.0,-1.0],[1.0,4.0]]>iex> eigenvecs#Nx.Tensor<vectorized[x:2]f32[2][2][[[0.5612090229988098,-0.8276740908622742],[0.8276740908622742,0.5612090229988098]],[[1.0,0.0],[0.0,1.0]]]>Error casesiex> Nx.LinAlg.eigh(Nx.tensor([[1,2,3],[4,5,6]]))** (ArgumentError) tensor must be a square matrix or a batch of square matrices, got shape: {2, 3}Link to this functioninvert(tensor)View SourceInverts a batch of square matrices.For non-square matrices, use pinv/2 for pseudo-inverse calculations.Examplesiex> a=Nx.tensor([[1,2,1,1],[0,1,0,1],[0,0,1,1],[0,0,0,1]])iex> a_inv=Nx.LinAlg.invert(a)#Nx.Tensor<f32[4][4][[1.0,-2.0,-1.0,2.0],[0.0,1.0,0.0,-1.0],[0.0,0.0,1.0,-1.0],[0.0,0.0,0.0,1.0]]>iex> Nx.dot(a,a_inv)#Nx.Tensor<f32[4][4][[1.0,0.0,0.0,0.0],[0.0,1.0,0.0,0.0],[0.0,0.0,1.0,0.0],[0.0,0.0,0.0,1.0]]>iex> Nx.dot(a_inv,a)#Nx.Tensor<f32[4][4][[1.0,0.0,0.0,0.0],[0.0,1.0,0.0,0.0],[0.0,0.0,1.0,0.0],[0.0,0.0,0.0,1.0]]>iex> a=Nx.tensor([[[1,2],[0,1]],[[1,1],[0,1]]])iex> a_inv=Nx.LinAlg.invert(a)#Nx.Tensor<f32[2][2][2][[[1.0,-2.0],[0.0,1.0]],[[1.0,-1.0],[0.0,1.0]]]>iex> Nx.dot(a,[2],[0],a_inv,[1],[0])#Nx.Tensor<f32[2][2][2][[[1.0,0.0],[0.0,1.0]],[[1.0,0.0],[0.0,1.0]]]>iex> Nx.dot(a_inv,[2],[0],a,[1],[0])#Nx.Tensor<f32[2][2][2][[[1.0,0.0],[0.0,1.0]],[[1.0,0.0],[0.0,1.0]]]>If a singular matrix is passed, the result will silently fail.iex> Nx.LinAlg.invert(Nx.tensor([[0,0,0,0],[0,0,0,0],[0,0,0,0],[1,1,1,1]]))#Nx.Tensor<f32[4][4][[NaN,NaN,NaN,NaN],[NaN,NaN,NaN,NaN],[NaN,NaN,NaN,NaN],[NaN,NaN,NaN,NaN]]>Error casesiex> Nx.LinAlg.invert(Nx.tensor([[3,0,0,0],[2,1,0,0]]))** (ArgumentError) invert/1 expects a square matrix or a batch of square matrices, got tensor with shape: {2, 4}Link to this functionleast_squares(a, b, opts \\ [])View SourceReturn the least-squares solution to a linear matrix equation Ax = b.Options:eps - Rounding error threshold used to assume values as 0. Defaults to 1.0e-15Examplesiex> Nx.LinAlg.least_squares(Nx.tensor([[1,2],[2,3]]),Nx.tensor([1,2]))#Nx.Tensor<f32[2][0.9977624416351318,0.0011188983917236328]>iex> Nx.LinAlg.least_squares(Nx.tensor([[0,1],[1,1],[2,1],[3,1]]),Nx.tensor([-1,0.2,0.9,2.1]))#Nx.Tensor<f32[2][0.9966151118278503,-0.947966456413269]>iex> Nx.LinAlg.least_squares(Nx.tensor([[1,2,3],[4,5,6]]),Nx.tensor([1,2]))#Nx.Tensor<f32[3][-0.05534052848815918,0.1111316829919815,0.27760395407676697]>Error casesiex> Nx.LinAlg.least_squares(Nx.tensor([1,2,3]),Nx.tensor([1,2]))** (ArgumentError) tensor of 1st argument must have rank 2, got rank 1 with shape {3}iex> Nx.LinAlg.least_squares(Nx.tensor([[1,2],[2,3]]),Nx.tensor([[1,2],[3,4]]))** (ArgumentError) tensor of 2nd argument must have rank 1, got rank 2 with shape {2, 2}iex> Nx.LinAlg.least_squares(Nx.tensor([[1,Complex.new(0,2)],[3,Complex.new(0,-4)]]),Nx.tensor([1,2]))** (ArgumentError) Nx.LinAlg.least_squares/2 is not yet implemented for complex inputsiex> Nx.LinAlg.least_squares(Nx.tensor([[1,2],[2,3]]),Nx.tensor([1,2,3]))** (ArgumentError) the number of rows of the matrix as the 1st argument and the number of columns of the vector as the 2nd argument must be the same, got 1st argument shape {2, 2} and 2nd argument shape {3}Link to this functionlu(tensor, opts \\ [])View SourceCalculates the A = PLU decomposition of batched square 2-D matrices A.Options:eps - Rounding error threshold that can be applied during the factorizationExamplesiex> {p,l,u}=Nx.LinAlg.lu(Nx.tensor([[1,2,3],[4,5,6],[7,8,9]]))iex> p#Nx.Tensor<s32[3][3][[0,0,1],[0,1,0],[1,0,0]]>iex> l#Nx.Tensor<f32[3][3][[1.0,0.0,0.0],[0.5714285969734192,1.0,0.0],[0.1428571492433548,2.0,1.0]]>iex> u#Nx.Tensor<f32[3][3][[7.0,8.0,9.0],[0.0,0.4285714328289032,0.8571428656578064],[0.0,0.0,0.0]]>iex> p|>Nx.dot(l)|>Nx.dot(u)#Nx.Tensor<f32[3][3][[1.0,2.0,3.0],[4.0,5.0,6.0],[7.0,8.0,9.0]]>iex> {p,l,u}=Nx.LinAlg.lu(Nx.tensor([[1,0,1],[-1,0,-1],[1,1,1]]))iex> p#Nx.Tensor<s32[3][3][[1,0,0],[0,0,1],[0,1,0]]>iex> l#Nx.Tensor<f32[3][3][[1.0,0.0,0.0],[1.0,1.0,0.0],[-1.0,0.0,1.0]]>iex> u#Nx.Tensor<f32[3][3][[1.0,0.0,1.0],[0.0,1.0,0.0],[0.0,0.0,0.0]]>iex> p|>Nx.dot(l)|>Nx.dot(u)#Nx.Tensor<f32[3][3][[1.0,0.0,1.0],[-1.0,0.0,-1.0],[1.0,1.0,1.0]]>iex> {p,l,u}=Nx.LinAlg.lu(Nx.tensor([[[9,8,7],[6,5,4],[3,2,1]],[[-1,0,-1],[1,0,1],[1,1,1]]]))iex> p#Nx.Tensor<s32[2][3][3][[[1,0,0],[0,1,0],[0,0,1]],[[1,0,0],[0,0,1],[0,1,0]]]>iex> l#Nx.Tensor<f32[2][3][3][[[1.0,0.0,0.0],[0.6666666865348816,1.0,0.0],[0.3333333432674408,2.0,1.0]],[[1.0,0.0,0.0],[-1.0,1.0,0.0],[-1.0,0.0,1.0]]]>iex> u#Nx.Tensor<f32[2][3][3][[[9.0,8.0,7.0],[0.0,-0.3333333432674408,-0.6666666865348816],[0.0,0.0,0.0]],[[-1.0,0.0,-1.0],[0.0,1.0,0.0],[0.0,0.0,0.0]]]>iex> p|>Nx.dot([2],[0],l,[1],[0])|>Nx.dot([2],[0],u,[1],[0])#Nx.Tensor<f32[2][3][3][[[9.0,8.0,7.0],[6.0,5.0,4.0],[3.0,2.0,1.0]],[[-1.0,0.0,-1.0],[1.0,0.0,1.0],[1.0,1.0,1.0]]]>iex> t=Nx.tensor([[[9,8,7],[6,5,4],[3,2,1]],[[-1,0,-1],[1,0,1],[1,1,1]]])|>Nx.vectorize(x:2)iex> {p,l,u}=Nx.LinAlg.lu(t)iex> p#Nx.Tensor<vectorized[x:2]s32[3][3][[[1,0,0],[0,1,0],[0,0,1]],[[1,0,0],[0,0,1],[0,1,0]]]>iex> l#Nx.Tensor<vectorized[x:2]f32[3][3][[[1.0,0.0,0.0],[0.6666666865348816,1.0,0.0],[0.3333333432674408,2.0,1.0]],[[1.0,0.0,0.0],[-1.0,1.0,0.0],[-1.0,0.0,1.0]]]>iex> u#Nx.Tensor<vectorized[x:2]f32[3][3][[[9.0,8.0,7.0],[0.0,-0.3333333432674408,-0.6666666865348816],[0.0,0.0,0.0]],[[-1.0,0.0,-1.0],[0.0,1.0,0.0],[0.0,0.0,0.0]]]>Error casesiex> Nx.LinAlg.lu(Nx.tensor([[1,1,1,1],[-1,4,4,-1],[4,-2,2,0]]))** (ArgumentError) tensor must be a square matrix or a batch of square matrices, got shape: {3, 4}Link to this functionmatrix_power(tensor, power)View SourceProduces the tensor taken to the given power by matrix dot-product.The input is always a tensor of batched square matrices and an integer,
and the output is a tensor of the same dimensions as the input tensor.The dot-products are unrolled inside defn.Examplesiex> Nx.LinAlg.matrix_power(Nx.tensor([[1,2],[3,4]]),0)#Nx.Tensor<s32[2][2][[1,0],[0,1]]>iex> Nx.LinAlg.matrix_power(Nx.tensor([[1,2],[3,4]]),6)#Nx.Tensor<s32[2][2][[5743,8370],[12555,18298]]>iex> Nx.LinAlg.matrix_power(Nx.eye(3),65535)#Nx.Tensor<s32[3][3][[1,0,0],[0,1,0],[0,0,1]]>iex> Nx.LinAlg.matrix_power(Nx.tensor([[1,2],[3,4]]),-1)#Nx.Tensor<f32[2][2][[-2.000000476837158,1.0000003576278687],[1.5000004768371582,-0.5000002384185791]]>iex> Nx.LinAlg.matrix_power(Nx.iota({2,2,2}),3)#Nx.Tensor<s32[2][2][2][[[6,11],[22,39]],[[514,615],[738,883]]]>iex> Nx.LinAlg.matrix_power(Nx.iota({2,2,2}),-3)#Nx.Tensor<f32[2][2][2][[[-4.875,1.375],[2.75,-0.75]],[[-110.37397766113281,76.8742904663086],[92.24915313720703,-64.2494125366211]]]>iex> Nx.LinAlg.matrix_power(Nx.tensor([[1,2],[3,4],[5,6]]),1)** (ArgumentError) matrix_power/2 expects a square matrix or a batch of square matrices, got tensor with shape: {3, 2}Link to this functionmatrix_rank(a, opts \\ [])View SourceReturn matrix rank of input M × N matrix using Singular Value Decomposition method.Approximate the number of linearly independent rows by calculating the number
of singular values greater than eps * max(singular values) * max(M, N).This also appears in Numerical recipes in the discussion of SVD solutions for
linear least squares [1].[1] W. H. Press, S. A. Teukolsky, W. T. Vetterling and B. P. Flannery,
“Numerical Recipes (3rd edition)”, Cambridge University Press, 2007, page 795.Options:eps - Rounding error threshold used to assume values as 0. Defaults to 1.0e-7Examplesiex> Nx.LinAlg.matrix_rank(Nx.tensor([[1,2],[3,4]]))#Nx.Tensor<u322>iex> Nx.LinAlg.matrix_rank(Nx.tensor([[1,1,1,1],[1,1,1,1],[1,2,3,4]]))#Nx.Tensor<u322>iex> Nx.LinAlg.matrix_rank(Nx.tensor([[1,1,1],[2,2,2],[8,9,10],[-2,1,5]]))#Nx.Tensor<u323>Error casesiex> Nx.LinAlg.matrix_rank(Nx.tensor([1,2,3]))** (ArgumentError) tensor must have rank 2, got rank 1 with shape {3}iex> Nx.LinAlg.matrix_rank(Nx.tensor([[1,Complex.new(0,2)],[3,Complex.new(0,-4)]]))** (ArgumentError) Nx.LinAlg.matrix_rank/2 is not yet implemented for complex inputsLink to this functionnorm(tensor, opts \\ [])View SourceCalculates the p-norm of a tensor.For the 0-norm, the norm is the number of non-zero elements in the tensor.Options:axes - defines the axes upon which the norm will be calculated.
Applies only on 2-norm for 2-D tensors. Default: nil.:keep_axes - whether the calculation axes should be kept with
length 1. Defaults to false:ord - defines which norm will be calculated according to the table below. Default: nil.ord2-D1-DnilFrobenius norm2-norm:nuclearNuclear norm-:frobeniusFrobenius norm-:infmax(sum(abs(x), axes: [1]))max(abs(x)):neg_infmin(sum(abs(x), axes: [1]))min(abs(x))0-Number of non-zero elements1max(sum(abs(x), axes: [0]))as below-1min(sum(abs(x), axes: [0]))as below22-normas below-2smallest singular valueas belowother-pow(sum(pow(abs(x), p)), 1/p)ExamplesVector normsiex> Nx.LinAlg.norm(Nx.tensor([3,4]))#Nx.Tensor<f325.0>iex> Nx.LinAlg.norm(Nx.tensor([3,4]),ord:1)#Nx.Tensor<f327.0>iex> Nx.LinAlg.norm(Nx.tensor([3,-4]),ord::inf)#Nx.Tensor<f324.0>iex> Nx.LinAlg.norm(Nx.tensor([3,-4]),ord::neg_inf)#Nx.Tensor<f323.0>iex> Nx.LinAlg.norm(Nx.tensor([3,-4,0,0]),ord:0)#Nx.Tensor<f322.0>Matrix normsiex> Nx.LinAlg.norm(Nx.tensor([[3,-1],[2,-4]]),ord:-1)#Nx.Tensor<f325.0>iex> Nx.LinAlg.norm(Nx.tensor([[3,-2],[2,-4]]),ord:1)#Nx.Tensor<f326.0>iex> Nx.LinAlg.norm(Nx.tensor([[3,-2],[2,-4]]),ord::neg_inf)#Nx.Tensor<f325.0>iex> Nx.LinAlg.norm(Nx.tensor([[3,-2],[2,-4]]),ord::inf)#Nx.Tensor<f326.0>iex> Nx.LinAlg.norm(Nx.tensor([[3,0],[0,-4]]),ord::frobenius)#Nx.Tensor<f325.0>iex> Nx.LinAlg.norm(Nx.tensor([[1,0,0],[0,-4,0],[0,0,9]]),ord::nuclear)#Nx.Tensor<f3214.0>iex> Nx.LinAlg.norm(Nx.tensor([[1,0,0],[0,-4,0],[0,0,9]]),ord:-2)#Nx.Tensor<f321.0>iex> Nx.LinAlg.norm(Nx.tensor([[3,0],[0,-4]]))#Nx.Tensor<f325.0>iex> Nx.LinAlg.norm(Nx.tensor([[3,4],[0,-4]]),axes:[1])#Nx.Tensor<f32[2][5.0,4.0]>iex> Nx.LinAlg.norm(Nx.tensor([[Complex.new(0,3),4],[4,0]]),axes:[0])#Nx.Tensor<f32[2][5.0,4.0]>iex> Nx.LinAlg.norm(Nx.tensor([[Complex.new(0,3),0],[4,0]]),ord::neg_inf)#Nx.Tensor<f323.0>iex> Nx.LinAlg.norm(Nx.tensor([[0,0],[0,0]]))#Nx.Tensor<f320.0>Error casesiex> Nx.LinAlg.norm(Nx.tensor([3,4]),ord::frobenius)** (ArgumentError) expected a 2-D tensor for ord: :frobenius, got a 1-D tensorLink to this functionpinv(tensor, opts \\ [])View SourceCalculates the Moore-Penrose inverse, or the pseudoinverse, of a matrix.Options:eps - Rounding error threshold used to assume values as 0. Defaults to 1.0e-10ExamplesScalar case:iex> Nx.LinAlg.pinv(2)#Nx.Tensor<f320.5>iex> Nx.LinAlg.pinv(0)#Nx.Tensor<f320.0>Vector case:iex> Nx.LinAlg.pinv(Nx.tensor([0,1,2]))#Nx.Tensor<f32[3][0.0,0.20000000298023224,0.4000000059604645]>iex> Nx.LinAlg.pinv(Nx.tensor([0,0,0]))#Nx.Tensor<f32[3][0.0,0.0,0.0]>Matrix case:iex> Nx.LinAlg.pinv(Nx.tensor([[1,1],[3,4]]))#Nx.Tensor<f32[2][2][[3.9924824237823486,-1.0052783489227295],[-3.0051186084747314,1.0071179866790771]]>iex> Nx.LinAlg.pinv(Nx.tensor([[0.5,0],[0,1],[0.5,0]]))#Nx.Tensor<f32[2][3][[0.9999999403953552,0.0,0.9999998807907104],[0.0,1.0,0.0]]>Link to this functionqr(tensor, opts \\ [])View SourceCalculates the QR decomposition of a tensor with shape {..., M, N}.Options:mode - Can be one of :reduced, :complete. Defaults to :reduced
For the following, K = min(M, N):reduced - returns q and r with shapes {..., M, K} and {..., K, N}:complete - returns q and r with shapes {..., M, M} and {..., M, N}:eps - Rounding error threshold that can be applied during the triangularization. Defaults to 1.0e-10Examplesiex> {q,r}=Nx.LinAlg.qr(Nx.tensor([[-3,2,1],[0,1,1],[0,0,-1]]))iex> q#Nx.Tensor<f32[3][3][[1.0,0.0,0.0],[0.0,1.0,0.0],[0.0,0.0,1.0]]>iex> r#Nx.Tensor<f32[3][3][[-3.0,2.0,1.0],[0.0,1.0,1.0],[0.0,0.0,-1.0]]>iex> t=Nx.tensor([[3,2,1],[0,1,1],[0,0,1]])iex> {q,r}=Nx.LinAlg.qr(t)iex> q#Nx.Tensor<f32[3][3][[1.0,0.0,0.0],[0.0,1.0,0.0],[0.0,0.0,1.0]]>iex> r#Nx.Tensor<f32[3][3][[3.0,2.0,1.0],[0.0,1.0,1.0],[0.0,0.0,1.0]]>iex> {qs,rs}=Nx.LinAlg.qr(Nx.tensor([[[-3,2,1],[0,1,1],[0,0,-1]],[[3,2,1],[0,1,1],[0,0,1]]]))iex> qs#Nx.Tensor<f32[2][3][3][[[1.0,0.0,0.0],[0.0,1.0,0.0],[0.0,0.0,1.0]],[[1.0,0.0,0.0],[0.0,1.0,0.0],[0.0,0.0,1.0]]]>iex> rs#Nx.Tensor<f32[2][3][3][[[-3.0,2.0,1.0],[0.0,1.0,1.0],[0.0,0.0,-1.0]],[[3.0,2.0,1.0],[0.0,1.0,1.0],[0.0,0.0,1.0]]]>iex> t=Nx.tensor([[3,2,1],[0,1,1],[0,0,1],[0,0,1]],type::f32)iex> {q,r}=Nx.LinAlg.qr(t,mode::reduced)iex> q#Nx.Tensor<f32[4][3][[1.0,0.0,0.0],[0.0,1.0,0.0],[0.0,0.0,0.7071068286895752],[0.0,0.0,0.7071067690849304]]>iex> r#Nx.Tensor<f32[3][3][[3.0,2.0,1.0],[0.0,1.0,1.0],[0.0,0.0,1.4142136573791504]]>iex> t=Nx.tensor([[3,2,1],[0,1,1],[0,0,1],[0,0,0]],type::f32)iex> {q,r}=Nx.LinAlg.qr(t,mode::complete)iex> q#Nx.Tensor<f32[4][4][[1.0,0.0,0.0,0.0],[0.0,1.0,0.0,0.0],[0.0,0.0,1.0,0.0],[0.0,0.0,0.0,1.0]]>iex> r#Nx.Tensor<f32[4][3][[3.0,2.0,1.0],[0.0,1.0,1.0],[0.0,0.0,1.0],[0.0,0.0,0.0]]>iex> t=Nx.tensor([[[-3,2,1],[0,1,1],[0,0,-1]],[[3,2,1],[0,1,1],[0,0,1]]])|>Nx.vectorize(x:2)iex> {qs,rs}=Nx.LinAlg.qr(t)iex> qs#Nx.Tensor<vectorized[x:2]f32[3][3][[[1.0,0.0,0.0],[0.0,1.0,0.0],[0.0,0.0,1.0]],[[1.0,0.0,0.0],[0.0,1.0,0.0],[0.0,0.0,1.0]]]>iex> rs#Nx.Tensor<vectorized[x:2]f32[3][3][[[-3.0,2.0,1.0],[0.0,1.0,1.0],[0.0,0.0,-1.0]],[[3.0,2.0,1.0],[0.0,1.0,1.0],[0.0,0.0,1.0]]]>Error casesiex> Nx.LinAlg.qr(Nx.tensor([1,2,3,4,5]))** (ArgumentError) tensor must have at least rank 2, got rank 1 with shape {5}iex> t=Nx.tensor([[-3,2,1],[0,1,1],[0,0,-1]])iex> Nx.LinAlg.qr(t,mode::error_test)** (ArgumentError) invalid :mode received. Expected one of [:reduced, :complete], received: :error_testLink to this functionsolve(a, b)View SourceSolves the system AX = B.A must have shape {..., n, n} and B must have shape {..., n, m} or {..., n}.
X has the same shape as B.Examplesiex> a=Nx.tensor([[1,3,2,1],[2,1,0,0],[1,0,1,0],[1,1,1,1]])iex> Nx.LinAlg.solve(a,Nx.tensor([-3,0,4,-2]))|>Nx.round()#Nx.Tensor<f32[4][1.0,-2.0,3.0,-4.0]>iex> a=Nx.tensor([[1,0,1],[1,1,0],[1,1,1]],type::f64)iex> Nx.LinAlg.solve(a,Nx.tensor([0,2,1]))|>Nx.round()#Nx.Tensor<f64[3][1.0,1.0,-1.0]>iex> a=Nx.tensor([[1,0,1],[1,1,0],[0,1,1]])iex> b=Nx.tensor([[2,2,3],[2,2,4],[2,0,1]])iex> Nx.LinAlg.solve(a,b)|>Nx.round()#Nx.Tensor<f32[3][3][[1.0,2.0,3.0],[1.0,0.0,1.0],[1.0,0.0,0.0]]>iex> a=Nx.tensor([[[14,10],[9,9]],[[4,11],[2,3]]])iex> b=Nx.tensor([[[2,4],[3,2]],[[1,5],[-3,-1]]])iex> Nx.LinAlg.solve(a,b)|>Nx.round()#Nx.Tensor<f32[2][2][2][[[0.0,0.0],[1.0,0.0]],[[-4.0,-3.0],[1.0,1.0]]]>iex> a=Nx.tensor([[[1,1],[0,1]],[[2,0],[0,2]]])|>Nx.vectorize(x:2)iex> b=Nx.tensor([[[2,1],[5,-1]]])|>Nx.vectorize(x:1,y:2)iex> Nx.LinAlg.solve(a,b)#Nx.Tensor<vectorized[x:2][y:2]f32[2][[[1.0,1.0],[6.0,-1.0]],[[1.0,0.5],[2.5,-0.5]]]>If the axes are named, their names are not preserved in the output:iex> a=Nx.tensor([[1,0,1],[1,1,0],[1,1,1]],names:[:x,:y])iex> Nx.LinAlg.solve(a,Nx.tensor([0,2,1],names:[:z]))|>Nx.round()#Nx.Tensor<f32[3][1.0,1.0,-1.0]>Error casesiex> Nx.LinAlg.solve(Nx.tensor([[1,0],[0,1]]),Nx.tensor([4,2,4,2]))** (ArgumentError) `b` tensor has incompatible dimensions, expected {2, 2} or {2}, got: {4}iex> Nx.LinAlg.solve(Nx.tensor([[3,0,0,0],[2,1,0,0],[1,1,1,1]]),Nx.tensor([4]))** (ArgumentError) `a` tensor has incompatible dimensions, expected a square matrix or a batch of square matrices, got: {3, 4}Link to this functionsvd(tensor, opts \\ [])View SourceCalculates the Singular Value Decomposition of batched 2-D matrices.It returns {u, s, vt} where the elements of s are sorted
from highest to lowest.Options:max_iter - integer. Defaults to 100
Number of maximum iterations before stopping the decomposition:full_matrices? - boolean. Defaults to true
If true, u and vt are of shape (M, M), (N, N). Otherwise,
the shapes are (M, K) and (K, N), where K = min(M, N).Note not all options apply to all backends, as backends may have
specific optimizations that render these mechanisms unnecessary.Examplesiex> {u,s,vt}=Nx.LinAlg.svd(Nx.tensor([[1,0,0],[0,1,0],[0,0,-1]]))iex> u#Nx.Tensor<f32[3][3][[1.0,0.0,0.0],[0.0,1.0,0.0],[0.0,0.0,-1.0]]>iex> s#Nx.Tensor<f32[3][1.0,1.0,1.0]>iex> vt#Nx.Tensor<f32[3][3][[1.0,0.0,0.0],[0.0,1.0,0.0],[0.0,0.0,1.0]]>iex> {u,s,vt}=Nx.LinAlg.svd(Nx.tensor([[2,0,0],[0,3,0],[0,0,-1],[0,0,0]]))iex> u#Nx.Tensor<f32[4][4][[0.0,0.9999999403953552,0.0,0.0],[1.0,0.0,0.0,0.0],[0.0,0.0,-1.0,0.0],[0.0,0.0,0.0,1.0]]>iex> s#Nx.Tensor<f32[3][3.0,1.9999998807907104,1.0]>iex> vt#Nx.Tensor<f32[3][3][[0.0,1.0,0.0],[1.0,0.0,0.0],[0.0,0.0,1.0]]>iex> {u,s,vt}=Nx.LinAlg.svd(Nx.tensor([[2,0,0],[0,3,0],[0,0,-1],[0,0,0]]),full_matrices?:false)iex> u#Nx.Tensor<f32[4][3][[0.0,0.9999999403953552,0.0],[1.0,0.0,0.0],[0.0,0.0,-1.0],[0.0,0.0,0.0]]>iex> s#Nx.Tensor<f32[3][3.0,1.9999998807907104,1.0]>iex> vt#Nx.Tensor<f32[3][3][[0.0,1.0,0.0],[1.0,0.0,0.0],[0.0,0.0,1.0]]>Link to this functiontriangular_solve(a, b, opts \\ [])View SourceSolve the equation a x = b for x, assuming a is a batch of triangular matrices.
Can also solve x a = b for x. See the :left_side option below.b must either be a batch of square matrices with the same dimensions as a or a batch of 1-D tensors
with as many rows as a. Batch dimensions of a and b must be the same.OptionsThe following options are defined in order of precedence:transform_a - Defines op(a), depending on its value. Can be one of::none -> op(a) = a:transpose -> op(a) = transpose(a)
Defaults to :none:lower - When true, defines the a matrix as lower triangular. If false, a is upper triangular.
Defaults to true:left_side - When true, solves the system as op(A).X = B. Otherwise, solves X.op(A) = B. Defaults to true.Examplesiex> a=Nx.tensor([[3,0,0,0],[2,1,0,0],[1,0,1,0],[1,1,1,1]])iex> Nx.LinAlg.triangular_solve(a,Nx.tensor([4,2,4,2]))#Nx.Tensor<f32[4][1.3333333730697632,-0.6666666865348816,2.6666667461395264,-1.3333333730697632]>iex> a=Nx.tensor([[1,0,0],[1,1,0],[1,1,1]],type::f64)iex> Nx.LinAlg.triangular_solve(a,Nx.tensor([1,2,1]))#Nx.Tensor<f64[3][1.0,1.0,-1.0]>iex> a=Nx.tensor([[1,0,0],[1,1,0],[0,1,1]])iex> b=Nx.tensor([[1,2,3],[2,2,4],[2,0,1]])iex> Nx.LinAlg.triangular_solve(a,b)#Nx.Tensor<f32[3][3][[1.0,2.0,3.0],[1.0,0.0,1.0],[1.0,0.0,0.0]]>iex> a=Nx.tensor([[1,1,1,1],[0,1,0,1],[0,0,1,2],[0,0,0,3]])iex> Nx.LinAlg.triangular_solve(a,Nx.tensor([2,4,2,4]),lower:false)#Nx.Tensor<f32[4][-1.3333333730697632,2.6666667461395264,-0.6666666865348816,1.3333333730697632]>iex> a=Nx.tensor([[1,0,0],[1,1,0],[1,2,1]])iex> b=Nx.tensor([[0,2,1],[1,1,0],[3,3,1]])iex> Nx.LinAlg.triangular_solve(a,b,left_side:false)#Nx.Tensor<f32[3][3][[-1.0,0.0,1.0],[0.0,1.0,0.0],[1.0,1.0,1.0]]>iex> a=Nx.tensor([[1,1,1],[0,1,1],[0,0,1]],type::f64)iex> Nx.LinAlg.triangular_solve(a,Nx.tensor([1,2,1]),transform_a::transpose,lower:false)#Nx.Tensor<f64[3][1.0,1.0,-1.0]>iex> a=Nx.tensor([[1,0,0],[1,1,0],[1,1,1]],type::f64)iex> Nx.LinAlg.triangular_solve(a,Nx.tensor([1,2,1]),transform_a::none)#Nx.Tensor<f64[3][1.0,1.0,-1.0]>iex> a=Nx.tensor([[1,0,0],[1,1,0],[1,2,1]])iex> b=Nx.tensor([[0,1,3],[2,1,3]])iex> Nx.LinAlg.triangular_solve(a,b,left_side:false)#Nx.Tensor<f32[2][3][[2.0,-5.0,3.0],[4.0,-5.0,3.0]]>iex> a=Nx.tensor([[1,0,0],[1,1,0],[1,2,1]])iex> b=Nx.tensor([[0,2],[3,0],[0,0]])iex> Nx.LinAlg.triangular_solve(a,b,left_side:true)#Nx.Tensor<f32[3][2][[0.0,2.0],[3.0,-2.0],[-6.0,2.0]]>iex> a=Nx.tensor([...> [1,0,0],...> [1,Complex.new(0,1),0],...> [Complex.new(0,1),1,1]...>])iex> b=Nx.tensor([1,-1,Complex.new(3,3)])iex> Nx.LinAlg.triangular_solve(a,b)#Nx.Tensor<c64[3][1.0+0.0i,0.0+2.0i,3.0+0.0i]>iex> a=Nx.tensor([[[1,0],[2,3]],[[4,0],[5,6]]])iex> b=Nx.tensor([[2,-1],[3,7]])iex> Nx.LinAlg.triangular_solve(a,b)#Nx.Tensor<f32[2][2][[2.0,-1.6666666269302368],[0.75,0.5416666865348816]]>iex> a=Nx.tensor([[[1,1],[0,1]],[[2,0],[0,2]]])|>Nx.vectorize(x:2)iex> b=Nx.tensor([[[2,1],[5,-1]]])|>Nx.vectorize(x:1,y:2)iex> Nx.LinAlg.triangular_solve(a,b,lower:false)#Nx.Tensor<vectorized[x:2][y:2]f32[2][[[1.0,1.0],[6.0,-1.0]],[[1.0,0.5],[2.5,-0.5]]]>Error casesiex> Nx.LinAlg.triangular_solve(Nx.tensor([[3,0,0,0],[2,1,0,0]]),Nx.tensor([4,2,4,2]))** (ArgumentError) triangular_solve/3 expected a square matrix or a batch of square matrices, got tensor with shape: {2, 4}iex> Nx.LinAlg.triangular_solve(Nx.tensor([[3,0,0,0],[2,1,0,0],[1,1,1,1],[1,1,1,1]]),Nx.tensor([4]))** (ArgumentError) incompatible dimensions for a and b on triangular solveiex> Nx.LinAlg.triangular_solve(Nx.tensor([[0,0,0,0],[0,0,0,0],[0,0,0,0],[1,1,1,1]]),Nx.tensor([4,2,4,2]))** (ArgumentError) can't solve for singular matrixiex> a=Nx.tensor([[1,0,0],[1,1,0],[1,1,1]],type::f64)iex> Nx.LinAlg.triangular_solve(a,Nx.tensor([1,2,1]),transform_a::conjugate)** (ArgumentError) complex numbers not supported yetiex> a=Nx.tensor([[1,0,0],[1,1,0],[1,1,1]],type::f64)iex> Nx.LinAlg.triangular_solve(a,Nx.tensor([1,2,1]),transform_a::other)** (ArgumentError) invalid value for :transform_a option, expected :none, :transpose, or :conjugate, got: :otherHex PackageHex Preview
            Search HexDocs
          
              Download ePub version
            
        Built using
        ExDoc (v0.34.2) for the

          Elixir programming languageNx.Pointer — Nx v0.9.2
Nx
        
          v0.9.2
        
Pages
        
            Modules
          Search documentation of NxSettingsView SourceNx.Pointer(Nx v0.9.2)Represents a reference to a value in memory.Can represent either a pointer or an IPC handle.SummaryTypest()TypesLink to this typet()View Source@type t() :: %Nx.Pointer{
  address: nil | non_neg_integer(),
  data_size: pos_integer(),
  handle: nil | binary(),
  kind: :local | :ipc
}Hex PackageHex Preview
            Search HexDocs
          
              Download ePub version
            
        Built using
        ExDoc (v0.34.2) for the

          Elixir programming languageNx.Random — Nx v0.9.2
Nx
        
          v0.9.2
        
Pages
        
            Modules
          Search documentation of NxSettingsView SourceNx.Random(Nx v0.9.2)Pseudo-random number generators.Unlike the stateful pseudo-random number generators (PRNGs)
that users of most programming languages and numerical libraries
may be accustomed to, Nx random functions require an explicit
PRNG key to be passed as a first argument (see below for more info). That key is defined by
an Nx.Tensor composed of 2 unsigned 32-bit integers, usually
generated by the Nx.Random.key/1 function:iex> Nx.Random.key(12)#Nx.Tensor<u32[2][0,12]>Or for example:iex> Nx.Random.key(System.os_time())This key can then be used in any of Nx’s random number generation
routines:iex> key=Nx.Random.key(12)iex> {uniform,_new_key}=Nx.Random.uniform(key)iex> uniform#Nx.Tensor<f320.7691127061843872>Now, when generating a new random number, you pass the new_key
to get a different number.The function in this module also have a *_split variant, which
is used when the key has been split before hand.Design and ContextIn short, Nx's PRNGs are based on a Threefry counter PRNG
associated to a functional array-oriented splitting model.
To summarize, among other requirements, Nx's PRNG aims to:Ensure reproducibilityParallelize well, both in terms of vectorization
(generating array values) and multi-replica, multi-core
computation. In particular it should not use sequencing
constraints between random function calls.The key to understanding Nx.Random keysMost Elixir users might be used to not having to keep
track of the PRNG state while their code executes.While this works fine when we're dealing with the CPU,
we can think of keeping track of the Nx.Random key as a way to
isolate multiple GPU users, much like the PRNG on different
BEAM nodes is isolated. Each key gets updated in its own
isolated sequence of calls, and thus we don't get different
results for each process using the same PRNG as we would
in the normal situation.The fact that the key is a parameter for the functions also
helps with the caching and operator fusion of the computational
graphs. Because the PRNG functions themselves are stateless,
compilers can take advantage of this to further improve execution times.SummaryFunctionschoice(key, tensor, opts \\ [])Generates random samples from a tensor.choice(key, tensor, p, opts)Generates random samples from a tensor with specified probabilities.fold_in(key, data)Folds in new data to a PRNG key.gumbel(key, opts \\ [])Sample Gumbel random values with given shape and float dtype.gumbel_split(key, opts \\ [])Same as gumbel/2, but assumes the key has been split beforehand.key(seed)Create a pseudo-random number generator (PRNG) key given an integer seed.multivariate_normal(key, mean, covariance, opts \\ [])Returns a sample from a multivariate normal distribution with given mean and covariance (matrix).
The function assumes that the covariance is a positive semi-definite matrix.
Otherwise, the result will not be normally distributed.multivariate_normal_split(key, mean, covariance, opts \\ [])Same as multivariate_normal/4 but assumes the key has already been split.normal(key, opts \\ [])Shortcut for normal(key, 0.0, 1.0, opts).normal(key, mean, standard_deviation, opts \\ [])Returns a normal distribution with the given mean and standard_deviation.normal_split(key, mean, standard_deviation, opts \\ [])Same as normal/4 but assumes the key has already been split.randint(key, min_val, max_val, opts \\ [])Sample uniform random integer values in the semi-open open interval [min_value, max_value).randint_split(key, min_val, max_val, opts \\ [])Same as randint/4 but assumes the key has already been split.shuffle(key, tensor, opts \\ [])Randomly shuffles tensor elements along an axis.split(key, opts \\ [])Splits a PRNG key into num new keys by adding a leading axis.threefry2x32_20_concat(xs, ks)uniform(key, opts \\ [])Shortcut for uniform(key, 0.0, 1.0, opts).uniform(key, min_val, max_val, opts \\ [])Sample uniform float values in the semi-open interval [min_val, max_val).uniform_split(key, min_value, max_value, opts \\ [])Same as uniform/4 but assumes the key has already been split.FunctionsLink to this functionchoice(key, tensor, opts \\ [])View SourceGenerates random samples from a tensor.Options:samples - The number of samples to take:axis - The axis along which to take samples.
If nil, the tensor is flattened beforehand.:replace - a boolean that specifies if samples will
be taken with or without replacement. Defaults to true.Examplesiex> k=Nx.Random.key(1)iex> t=Nx.iota({4,3})iex> {result,_key}=Nx.Random.choice(k,t,samples:4,axis:0)iex> result#Nx.Tensor<s32[4][3][[6,7,8],[9,10,11],[6,7,8],[0,1,2]]>iex> {result,_key}=Nx.Random.choice(k,t,samples:4,axis:0,replace:false)iex> result#Nx.Tensor<s32[4][3][[3,4,5],[9,10,11],[6,7,8],[0,1,2]]>If no axis is specified, the tensor is flattened:iex> k=Nx.Random.key(2)iex> t=Nx.iota({3,2})iex> {result,_key}=Nx.Random.choice(k,t)iex> result#Nx.Tensor<s32[1][4]>iex> {result,_key}=Nx.Random.choice(k,t,samples:6,replace:false)iex> result#Nx.Tensor<s32[6][2,0,4,5,1,3]>Link to this functionchoice(key, tensor, p, opts)View SourceGenerates random samples from a tensor with specified probabilities.The probabilities tensor must have the same size as the axis along
which the samples are being taken. If no axis is given, the size
must be equal to the input tensor's size.Options:samples - The number of samples to take:axis - The axis along which to take samples.
If nil, the tensor is flattened beforehand.:replace - a boolean that specifies if samples will
be taken with or without replacement. Defaults to true.Examplesiex> k=Nx.Random.key(1)iex> t=Nx.iota({4,3})iex> p=Nx.tensor([0.1,0.7,0.2])iex> {result,_key}=Nx.Random.choice(k,t,p,samples:3,axis:1)iex> result#Nx.Tensor<s32[4][3][[1,0,1],[4,3,4],[7,6,7],[10,9,10]]>iex> {result,_key}=Nx.Random.choice(k,t,p,samples:3,axis:1,replace:false)iex> result#Nx.Tensor<s32[4][3][[1,2,0],[4,5,3],[7,8,6],[10,11,9]]>If no axis is specified, the tensor is flattened.
Notice that in the first case we get a higher occurence
of the entries with bigger probabilities, while in the
second case, without replacements, we get those samples
first.iex> k=Nx.Random.key(2)iex> t=Nx.iota({2,3})iex> p=Nx.tensor([0.01,0.1,0.19,0.6,0.05,0.05])iex> {result,_key}=Nx.Random.choice(k,t,p)iex> result#Nx.Tensor<s32[1][3]>iex> {result,_key}=Nx.Random.choice(k,t,p,samples:6)iex> result#Nx.Tensor<s32[6][3,3,3,0,3,3]>iex> {result,_key}=Nx.Random.choice(k,t,p,samples:6,replace:false)iex> result#Nx.Tensor<s32[6][3,1,2,5,4,0]>Link to this functionfold_in(key, data)View SourceFolds in new data to a PRNG key.Examplesiex> key=Nx.Random.key(42)iex> Nx.Random.fold_in(key,99)#Nx.Tensor<u32[2][2015327502,1351855566]>iex> key=Nx.Random.key(42)iex> Nx.Random.fold_in(key,1234)#Nx.Tensor<u32[2][1356445167,2917756949]>iex> key=Nx.Random.key(42)iex> Nx.Random.fold_in(key,Nx.tensor([[1,99],[1234,13]]))#Nx.Tensor<u32[2][2][2][[[64467757,2916123636],[2015327502,1351855566]],[[1356445167,2917756949],[3514951389,229662949]]]>Link to this functiongumbel(key, opts \\ [])View SourceSample Gumbel random values with given shape and float dtype.Options:shape - the shape of the output tensor containing the
random samples. Defaults to {}:type - the floating-point output type. Defaults to {:f, 32}Examplesiex> {result,_key}=Nx.Random.gumbel(Nx.Random.key(1))iex> result#Nx.Tensor<f32-0.7294610142707825>iex> {result,_key}=Nx.Random.gumbel(Nx.Random.key(1),shape:{2,3})iex> result#Nx.Tensor<f32[2][3][[0.6247938275337219,-0.21740718185901642,0.7678327560424805],[0.7778404355049133,4.0895304679870605,0.3029090166091919]]>Link to this functiongumbel_split(key, opts \\ [])View SourceSame as gumbel/2, but assumes the key has been split beforehand.Link to this functionkey(seed)View SourceCreate a pseudo-random number generator (PRNG) key given an integer seed.Examplesiex> Nx.Random.key(12)#Nx.Tensor<u32[2][0,12]>iex> Nx.Random.key(999999999999)#Nx.Tensor<u32[2][232,3567587327]>Link to this functionmultivariate_normal(key, mean, covariance, opts \\ [])View SourceReturns a sample from a multivariate normal distribution with given mean and covariance (matrix).
The function assumes that the covariance is a positive semi-definite matrix.
Otherwise, the result will not be normally distributed.Options:type - a float type for the returned tensor:shape - batch shape of the returned tensor, i.e. the prefix of the result shape excluding the last axis:names - the names of the returned tensor:method - a decomposition method used for the covariance. Must be one of :svd, :eigh, and :cholesky.
Defaults to :cholesky. For singular covariance matrices, use :svd or :eigh.Examplesiex> key=Nx.Random.key(12)iex> {multivariate_normal,_new_key}=Nx.Random.multivariate_normal(key,Nx.tensor([0]),Nx.tensor([[1]]))iex> multivariate_normal#Nx.Tensor<f32[1][0.735927939414978]>iex> key=Nx.Random.key(12)iex> {multivariate_normal,_new_key}=Nx.Random.multivariate_normal(key,Nx.tensor([0,0]),Nx.tensor([[1,0],[0,1]]))iex> multivariate_normal#Nx.Tensor<f32[2][-1.3425945043563843,-0.40812060236930847]>iex> key=Nx.Random.key(12)iex> {multivariate_normal,_new_key}=Nx.Random.multivariate_normal(key,Nx.tensor([0]),Nx.tensor([[1]]),shape:{3,2},type::f16)iex> multivariate_normal#Nx.Tensor<f16[3][2][1][[[0.326904296875],[0.2176513671875]],[[0.316650390625],[0.1109619140625]],[[0.53955078125],[-0.8857421875]]]>iex> key=Nx.Random.key(12)iex> {multivariate_normal,_new_key}=Nx.Random.multivariate_normal(key,Nx.tensor([0,0]),Nx.tensor([[1,0],[0,1]]),shape:{3,2})iex> multivariate_normal#Nx.Tensor<f32[3][2][2][[[0.9891449809074402,1.0795185565948486],[-0.9467806220054626,1.47813880443573]],[[2.2095863819122314,-1.529456377029419],[-0.7933920621871948,1.121195673942566]],[[0.10976295918226242,-0.9959557056427002],[0.4754556119441986,1.1413804292678833]]]>Link to this functionmultivariate_normal_split(key, mean, covariance, opts \\ [])View SourceSame as multivariate_normal/4 but assumes the key has already been split.Link to this functionnormal(key, opts \\ [])View SourceShortcut for normal(key, 0.0, 1.0, opts).Link to this functionnormal(key, mean, standard_deviation, opts \\ [])View SourceReturns a normal distribution with the given mean and standard_deviation.Options:type - a float or complex type for the returned tensor:shape - shape of the returned tensor:names - the names of the returned tensorExamplesiex> key=Nx.Random.key(42)iex> {normal,_new_key}=Nx.Random.normal(key)iex> normal#Nx.Tensor<f321.3694695234298706>iex> key=Nx.Random.key(42)iex> {normal,_new_key}=Nx.Random.normal(key,0,1,shape:{3,2},type::f16)iex> normal#Nx.Tensor<f16[3][2][[-0.32568359375,-0.77197265625],[0.39208984375,0.5341796875],[0.270751953125,-2.080078125]]>iex> key=Nx.Random.key(42)iex> {normal,_new_key}=Nx.Random.normal(key,0,1,shape:{2,2},type::c64)iex> normal#Nx.Tensor<c64[2][2][[-0.7632761001586914+0.8661127686500549i,-0.14282889664173126-0.7384796142578125i],[0.678461492061615+0.4118310809135437i,-2.269538402557373-0.3689095079898834i]]>iex> key=Nx.Random.key(1337)iex> {normal,_new_key}=Nx.Random.normal(key,10,5,shape:{1_000})iex> Nx.mean(normal)#Nx.Tensor<f329.70022201538086>iex> Nx.standard_deviation(normal)#Nx.Tensor<f325.051416397094727>Link to this functionnormal_split(key, mean, standard_deviation, opts \\ [])View SourceSame as normal/4 but assumes the key has already been split.Link to this functionrandint(key, min_val, max_val, opts \\ [])View SourceSample uniform random integer values in the semi-open open interval [min_value, max_value).Options:type - the integer type for the returned tensor:shape - shape of the returned tensor:names - the names of the returned tensorExamplesiex> key=Nx.Random.key(1701)iex> {randint,_new_key}=Nx.Random.randint(key,1,100)iex> randint#Nx.Tensor<s3291>iex> key=Nx.Random.key(1701)iex> {randint,_new_key}=Nx.Random.randint(key,1,100,shape:{3,2},type::u32)iex> randint#Nx.Tensor<u32[3][2][[9,20],[19,6],[71,15]]>Link to this functionrandint_split(key, min_val, max_val, opts \\ [])View SourceSame as randint/4 but assumes the key has already been split.Link to this functionshuffle(key, tensor, opts \\ [])View SourceRandomly shuffles tensor elements along an axis.Options:axis - the axis along which to shuffle. Defaults to 0:independent - a boolean that indicates whether the permutations
are independent along the given axis. Defaults to falseExamplesiex> key=Nx.Random.key(42)iex> {shuffled,_new_key}=Nx.Random.shuffle(key,Nx.iota({3,4},axis:0))iex> shuffled#Nx.Tensor<s32[3][4][[2,2,2,2],[0,0,0,0],[1,1,1,1]]>iex> key=Nx.Random.key(10)iex> {shuffled,_new_key}=Nx.Random.shuffle(key,Nx.iota({3,4},axis:1),independent:true,axis:1)iex> shuffled#Nx.Tensor<s32[3][4][[2,1,3,0],[3,0,1,2],[2,3,0,1]]>Link to this functionsplit(key, opts \\ [])View SourceSplits a PRNG key into num new keys by adding a leading axis.Examplesiex> key=Nx.Random.key(1701)iex> Nx.Random.split(key)#Nx.Tensor<u32[2][2][[56197195,1801093307],[961309823,1704866707]]>iex> key=Nx.Random.key(1701)iex> Nx.Random.split(key,parts:4)#Nx.Tensor<u32[4][2][[4000152724,2030591747],[2287976877,2598630646],[2426625787,580268518],[3136765380,433355682]]>Link to this functionthreefry2x32_20_concat(xs, ks)View SourceLink to this functionuniform(key, opts \\ [])View SourceShortcut for uniform(key, 0.0, 1.0, opts).Link to this functionuniform(key, min_val, max_val, opts \\ [])View SourceSample uniform float values in the semi-open interval [min_val, max_val).Options:type - a float type for the returned tensor:shape - shape of the returned tensor:names - the names of the returned tensorExamplesiex> key=Nx.Random.key(1701)iex> {uniform,_new_key}=Nx.Random.uniform(key)iex> uniform#Nx.Tensor<f320.9728643894195557>iex> key=Nx.Random.key(1701)iex> {uniform,_new_key}=Nx.Random.uniform(key,shape:{3,2},type::f16)iex> uniform#Nx.Tensor<f16[3][2][[0.75390625,0.6484375],[0.7294921875,0.21484375],[0.09765625,0.0693359375]]>iex> key=Nx.Random.key(1701)iex> {uniform,_new_key}=Nx.Random.uniform(key,shape:{2,2},type::c64)iex> uniform#Nx.Tensor<c64[2][2][[0.18404805660247803+0.6546461582183838i,0.5525915622711182+0.11568140983581543i],[0.6074584722518921+0.8104375600814819i,0.247686505317688+0.21975469589233398i]]>Link to this functionuniform_split(key, min_value, max_value, opts \\ [])View SourceSame as uniform/4 but assumes the key has already been split.Hex PackageHex Preview
            Search HexDocs
          
              Download ePub version
            
        Built using
        ExDoc (v0.34.2) for the

          Elixir programming languageNx.Serving — Nx v0.9.2
Nx
        
          v0.9.2
        
Pages
        
            Modules
          Search documentation of NxSettingsView SourceNx.Servingbehaviour(Nx v0.9.2)Serving encapsulates client and server work to perform batched requests.Servings can be executed on the fly, without starting a server, but most
often they are used to run servers that batch requests until a given size
or timeout is reached.More specifically, servings are a mechanism to apply a computation on a
Nx.Batch, with hooks for preprocessing input from and postprocessing
output for the client. Thus we can think of an instance of Nx.Serving.t/0
(a serving) as something that encapsulates batches of Nx computations.Inline/serverless workflowFirst, let's define a simple numerical definition function:defmoduleMyDefndoimportNx.Defndefnprint_and_multiply(x)dox=print_value(x,label:"debug")x*2endendThe function prints the given tensor and doubles its contents.
We can use new/1 to create a serving that will return a JIT
or AOT compiled function to execute on batches of tensors:iex> serving=Nx.Serving.new(fnopts->Nx.Defn.jit(&MyDefn.print_and_multiply/1,opts)end)iex> batch=Nx.Batch.stack([Nx.tensor([1,2,3])])iex> Nx.Serving.run(serving,batch)debug:#Nx.Tensor<s64[1][3][[1,2,3]]>#Nx.Tensor<s64[1][3][[2,4,6]]>We started the serving by passing a function that receives
compiler options and returns a JIT or AOT compiled function.
We called Nx.Defn.jit/2 passing the options received as
argument, which will customize the JIT/AOT compilation.You should see two values printed. The former is the result of
Nx.Defn.Kernel.print_value/1, which shows the tensor that was
actually part of the computation and how it was batched.
The latter is the result of the computation.When defining a Nx.Serving, we can also customize how the data is
batched by using the client_preprocessing as well as the result by
using client_postprocessing hooks. Let's give it another try,
this time using jit/2 to create the serving, which automatically
wraps the given function in Nx.Defn.jit/2 for us:iex> serving=(...> Nx.Serving.jit(&MyDefn.print_and_multiply/1)...> |>Nx.Serving.client_preprocessing(fninput->{Nx.Batch.stack(input),:client_info}end)...> |>Nx.Serving.client_postprocessing(&{&1,&2})...> )iex> Nx.Serving.run(serving,[Nx.tensor([1,2]),Nx.tensor([3,4])])debug:#Nx.Tensor<s64[2][2][[1,2],[3,4]]>{{#Nx.Tensor<s64[2][2][[2,4],[6,8]]>,:server_info},:client_info}You can see the results are a bit different now. First of all, notice that
we were able to run the serving passing a list of tensors. Our custom
client_preprocessing function stacks those tensors into a batch of two
entries and returns a tuple with a Nx.Batch struct and additional client
information which we represent as the atom :client_info. The default
client preprocessing simply enforces a batch (or a stream of batches)
was given and returns no client information.Then the result is a triplet tuple, returned by the client
postprocessing function, containing the result, the server information
(which we will later learn how to customize), and the client information.
From this, we can infer the default implementation of client_postprocessing
simply returns the result, discarding the server and client information.So far, Nx.Serving has not given us much. It has simply encapsulated the
execution of a function. Its full power comes when we start running our own
Nx.Serving process. That's when we will also learn why we have a client_
prefix in some of the function names.Stateful/process workflowNx.Serving allows us to define an Elixir process to handle requests.
This process provides several features, such as batching up to a given
size or time, partitioning, and distribution over a group of nodes.To do so, we need to start a Nx.Serving process with a serving inside
a supervision tree:children=[{Nx.Serving,serving:Nx.Serving.jit(&MyDefn.print_and_multiply/1),name:MyServing,batch_size:10,batch_timeout:100}]Supervisor.start_child(children,strategy::one_for_one)Note: in your actual application, you want to make sure
Nx.Serving comes early in your supervision tree, for example
before your web application endpoint or your data processing
pipelines, as those processes may end-up hitting Nx.Serving.Now you can send batched runs to said process:iex> batch=Nx.Batch.stack([Nx.tensor([1,2,3]),Nx.tensor([4,5,6])])iex> Nx.Serving.batched_run(MyServing,batch)debug:#Nx.Tensor<s64[2][3][[1,2,3],[4,5,6]]>#Nx.Tensor<s64[2][3][[2,4,6],[8,10,12]]>In the example, we pushed a batch of 2 and eventually got a reply.
The process will wait for requests from other processes, for up to
100 milliseconds or until it gets 10 entries. Then it merges all
batches together and once the result is computed, it slices and
distributes those responses to each caller.If there is any client_preprocessing function, it will be executed
before the batch is sent to the server. If there is any client_postprocessing
function, it will be executed after getting the response from the
server.PartitioningYou can start several partitions under the same serving by passing
partitions: true when starting the serving. The number of partitions
will be determined according your compiler and for which host it is
compiling.For example, when creating the serving, you may pass the following
defn_options:Nx.Serving.new(computation,compiler:EXLA,client::cuda)Now when booting up the serving:children=[{Nx.Serving,serving:serving,name:MyServing,batch_size:10,batch_timeout:100,partitions:true}]If you have two GPUs, batched_run/3 will now gather batches and send
them to the GPUs as they become available to process requests.Cross-device operationsWhen partitions: true is set, you will receive results from
different GPU devices and Nx won't automatically transfer data
across devices to avoid surprising performance pitfalls, which
may lead to errors. In such cases, you probably want to transfer
tensors back to host on your serving execution.DistributionAll Nx.Servings are distributed by default. If the current machine
does not have an instance of Nx.Serving running, batched_run/3 will
automatically look for one in the cluster. The nodes do not need to run
the same code and applications. It is only required that they run the
same Nx version.The load balancing between servings is done randomly by default, however,
the number of partitions are considered if the partitions: true option is also given.
For example, if you have a node with 2 GPUs and another with 4, the latter
will receive the double of requests compared to the former.Furthermore, the load balancing allows for assigning weights to servings.
Similarly to the number of partitions, when running a serving with distribution_weight: 1
and another one with distribution_weight: 2, the latter will receive double the requests
compared to the former.batched_run/3 receives an optional distributed_preprocessing callback as
third argument for preprocessing the input for distributed requests. When
using libraries like EXLA or Torchx, the tensor is often allocated in memory
inside a third-party library so it is necessary to either transfer or copy
the tensor to the binary backend before sending it to another node.
This can be done by passing either Nx.backend_transfer/1 or Nx.backend_copy/1
as third argument:Nx.Serving.batched_run(MyDistributedServing,input,&Nx.backend_copy(&1,Nx.BinaryBackend))Use backend_transfer/1 if you know the input will no longer be used.Similarly, the serving has a distributed_postprocessing callback which is
called on the remote machine before sending the reply to the caller. It can
be used to transfer resources to the binary backend before sending them over
the network.The servings are dispatched using Erlang Distribution. You can use
Node.connect/1 to manually connect nodes. In a production setup, this is
often done with the help of libraries like libcluster.Advanced notesModule-based servingIn the examples so far, we have been using the default version of
Nx.Serving, which executes the given function for each batch.However, we can also use new/2 to start a module-based version of
Nx.Serving which gives us more control over both inline and process
workflows. A simple module implementation of a Nx.Serving could look
like this:defmoduleMyServingdo@behaviourNx.Servingdefnpprint_and_multiply(x)dox=print_value({:debug,x})x*2end@impltruedefinit(_inline_or_process,:unused_arg,[defn_options])do{:ok,Nx.Defn.jit(&print_and_multiply/1,defn_options)}end@impltruedefhandle_batch(batch,0,function)do{:execute,fn->{function.(batch),:server_info}end,function}endendIt has two functions. The first, init/3, receives the type of serving
(:inline or :process) and the serving argument. In this step,
we capture print_and_multiply/1as a jitted function.The second function is called handle_batch/3. This function
receives a Nx.Batch and returns a function to execute.
The function itself must return a two element-tuple: the batched
results and some server information. The server information can
be any value and we set it to the atom :server_info.Now let's give it a try by defining a serving with our module and
then running it on a batch:iex> serving=Nx.Serving.new(MyServing,:unused_arg)iex> batch=Nx.Batch.stack([Nx.tensor([1,2,3])])iex> Nx.Serving.run(serving,batch){:debug,#Nx.Tensor<s64[1][3][[1,2,3]]>}#Nx.Tensor<s64[1][3][[2,4,6]]>From here on, you use start_link/1 to start this serving in your
supervision and even customize client_preprocessing/1 and
client_postprocessing/1 callbacks to this serving, as seen in the
previous sections.Note in our implementation above assumes it won't run partitioned.
In partitioned mode, init/3 may receive multiple defn_options
as the third argument and handle_batch/3 may receive another partition
besides 0.StreamingNx.Serving allows both inputs and outputs to be streamed.In order to stream inputs, you only need to return a stream of Nx.Batch
from the client_preprocessing callback. Serving will automatically take
care of streaming the inputs in, regardless if using run/2 or batched_run/2.
It is recommended that the streaming batches have the same size as batch_size,
to avoid triggering batch_timeout on every iteration (except for the last one
which may be incomplete).To stream outputs, you must invoke streaming/2 with any additional
streaming configuration. When this is invoked, the client_postprocessing
will receive a stream which you can further manipulate lazily using the
functions in the Stream module. streaming/2 also allows you to configure
hooks and stream values directly from Nx.Defn hooks. However, when hook
streaming is enabled, certain capabilities are removed: you cannot stream
inputs nor have batches larger than the configured batch_size.You can enable both input and output streaming at once.Batch keysSometimes it may be necessary to execute different functions under the
same serving. For example, sequence transformers must pad the sequence
to a given length. However, if you are batching, the length must be
padded upfront. If the length is too small, you have to either discard
data or support only small inputs. If the length is too large, then you
decrease performance with the extra padding.Batch keys provide a mechanism to accumulate different batches, based on
their key, which execute independently. As an example, we will do a
serving which performs different operations based on the batch key,
but it could also be used to perform the same operation for different
templates:iex> args=[Nx.template({10},:s32)]iex> serving=Nx.Serving.new(fn...> :double,opts->Nx.Defn.compile(&Nx.multiply(&1,2),args,opts)...> :half,opts->Nx.Defn.compile(&Nx.divide(&1,2),args,opts)...> end)iex> double_batch=Nx.Batch.concatenate([Nx.iota({10})])|>Nx.Batch.key(:double)iex> Nx.Serving.run(serving,double_batch)#Nx.Tensor<s32[10][0,2,4,6,8,10,12,14,16,18]>iex> half_batch=Nx.Batch.concatenate([Nx.iota({10})])|>Nx.Batch.key(:half)iex> Nx.Serving.run(serving,half_batch)#Nx.Tensor<f32[10][0.0,0.5,1.0,1.5,2.0,2.5,3.0,3.5,4.0,4.5]>When using a process-based serving, you must specify the supported
:batch_keys when the process is started. The batch keys will be
available inside the defn_options passed as the third argument of
the init/3 callback. The batch keys will also be verified
when the batch is returned from the client-preprocessing callback.SummaryTypesclient_info()client_postprocessing()client_preprocessing()distributed_postprocessing()distributed_preprocessing()metadata()t()Callbackshandle_batch(t, partition, state)Receives a batch, a partition, and returns a function to execute the batch.init(type, arg, list)The callback used to initialize the serving.Functionsbatch_size(serving, batch_size)Sets the batch size for this serving.batched_run(name, input, distributed_preprocessing \\ &Function.identity/1)Runs the given input on the serving process given by name.client_postprocessing(serving, function)Sets the client postprocessing function.client_preprocessing(serving, function)Sets the client preprocessing function.defn_options(serving, defn_options)Sets the defn options of this serving.distributed_postprocessing(serving, function)Sets the distributed postprocessing function.jit(fun, defn_options \\ [])Creates a new serving by jitting the given fun with defn_options.new(function, defn_options \\ [])Creates a new function serving.new(module, arg, defn_options)Creates a new module-based serving.process_options(serving, opts)Sets the process options of this serving.run(serving, input)Runs serving with the given input inline with the current process.start_link(opts)Starts a Nx.Serving process to batch requests to a given serving.streaming(serving, opts \\ [])Configure the serving to stream its results.TypesLink to this typeclient_info()View Source@type client_info() :: term()Link to this typeclient_postprocessing()View Source@type client_postprocessing() :: ({Nx.Container.t(), metadata()}, client_info() ->
                              term())Link to this typeclient_preprocessing()View Source@type client_preprocessing() :: (term() ->
                             {Nx.Batch.t() | Enumerable.t(Nx.Batch.t()),
                              client_info()})Link to this typedistributed_postprocessing()View Source@type distributed_postprocessing() :: (term() -> term())Link to this typedistributed_preprocessing()View Source@type distributed_preprocessing() :: (term() -> term())Link to this typemetadata()View Source@type metadata() :: term()Link to this typet()View Source@type t() :: %Nx.Serving{
  arg: term(),
  batch_size: nil | pos_integer(),
  client_postprocessing: client_postprocessing(),
  client_preprocessing: client_preprocessing(),
  defn_options: keyword(),
  distributed_postprocessing: distributed_postprocessing(),
  module: atom(),
  process_options: keyword(),
  streaming: nil | %{hooks: [atom()]}
}CallbacksLink to this callbackhandle_batch(t, partition, state)View Source@callback handle_batch(Nx.Batch.t(), partition :: non_neg_integer(), state) ::
  {:execute, (-> {Nx.Container.t(), metadata()}), state}
when state: term()Receives a batch, a partition, and returns a function to execute the batch.In case of serving processes, the function is executed is an
separate process.Link to this callbackinit(type, arg, list)View Source@callback init(type :: :inline | :process, arg :: term(), [defn_options :: keyword()]) ::
  {:ok, state :: term()}The callback used to initialize the serving.The first argument reveals if the serving is executed inline,
such as by calling run/2, by started with the process.
The second argument is the serving argument given to new/2.
The third argument option is a list of compiler options to be
used to compile each partition the serving will run.It must return {:ok, state}, where the state can be any term.FunctionsLink to this functionbatch_size(serving, batch_size)View SourceSets the batch size for this serving.This batch size is used to split batches given to both run/2 and
batched_run/2, enforcing that the batch size never goes over a limit.
If you only want to batch within the serving process, you must set
:batch_size via process_options/2 (or on start_link/1).Note that :batch_size only guarantees a batch does not go over a limit.
Batches are not automatically padded to the batch size. Such can be done
as necessary inside your serving function by calling Nx.Batch.pad/2.Why batch on run/2?By default, run/2 does not place a limit on its input size. It always
processes inputs directly within the current process. On the other hand,
batched_run/2 always sends your input to a separate process, which
will batch and execute the serving only once the batch is full or a
timeout has elapsed.However, in some situations, an input given to run/2 needs to be
broken into several batches. If we were to very large batches to our
computation, the computation could require too much memory. In such
cases, setting a batch size even on run/2 is beneficial, because
Nx.Serving takes care of splitting a large batch into smaller ones
that do not exceed the batch_size value.Link to this functionbatched_run(name, input, distributed_preprocessing \\ &Function.identity/1)View SourceRuns the given input on the serving process given by name.name is either an atom representing a local or distributed
serving process. First it will attempt to dispatch locally, then it
falls back to the distributed serving. You may specify
{:local, name} to force a local lookup or {:distributed, name}
to force a distributed one.The client_preprocessing callback will be invoked on the input
which is then sent to the server. The server will batch requests
and send a response either when the batch is full or on timeout.
Then client_postprocessing is invoked on the response. See the
module documentation for more information. In the distributed case,
the callbacks are invoked in the distributed node, but still outside of
the serving process.Note that you cannot batch an input larger than the configured
:batch_size in the server.Distributed modeTo run in distributed mode, the nodes do not need to run the same
code and applications. It is only required that they run the
same Nx version.If the current node is running a serving given by name locally
and {:distributed, name} is used, the request will use the same
distribution mechanisms instead of being handled locally, which
is useful for testing locally without a need to spawn nodes.This function receives an optional distributed_preprocessing callback as
third argument for preprocessing the input for distributed requests. When
using libraries like EXLA or Torchx, the tensor is often allocated in memory
inside a third-party library so it may be necessary to either transfer or copy
the tensor to the binary backend before sending it to another node.
This can be done by passing either Nx.backend_transfer/1 or Nx.backend_copy/1
as third argument:Nx.Serving.batched_run(MyDistributedServing,input,&Nx.backend_copy/1)Use backend_transfer/1 if you know the input will no longer be used.Similarly, the serving has a distributed_postprocessing callback which can do
equivalent before sending the reply to the caller.Link to this functionclient_postprocessing(serving, function)View SourceSets the client postprocessing function.The client postprocessing receives a tuple with the
{output, metadata} or a stream as first argument.
The second argument is always the additional information
returned by the client preprocessing.The default implementation returns either the output or
the stream.Link to this functionclient_preprocessing(serving, function)View SourceSets the client preprocessing function.The default implementation expects a Nx.Batch or a stream of
Nx.Batch to be given as input and return them as is.Link to this functiondefn_options(serving, defn_options)View SourceSets the defn options of this serving.These are the options supported by Nx.Defn.default_options/1.Link to this functiondistributed_postprocessing(serving, function)View SourceSets the distributed postprocessing function.The default implementation is Function.identity/1.Link to this functionjit(fun, defn_options \\ [])View SourceCreates a new serving by jitting the given fun with defn_options.This is equivalent to:new(fnopts->Nx.Defn.jit(fun,opts)end,defn_options)Link to this functionnew(function, defn_options \\ [])View SourceCreates a new function serving.It expects a single- or double-arity function. If a single-arity
function is given, it receives the compiler options and must
return a JIT (via Nx.Defn.jit/2) or AOT compiled (via
Nx.Defn.compile/3) one-arity function.If a double-arity function is given, it receives the batch
key as first argument and the compiler options as second argument.
It must return a JIT (via Nx.Defn.jit/2) or AOT compiled
(via Nx.Defn.compile/3) one-arity function, but in practice
it will be a Nx.Defn.compile/3, since the purpose of the
batch key is often to precompile different versions of the
same function upfront. The batch keys can be given on
start_link/1.The function will be called with the arguments returned by the
client_preprocessing callback.Link to this functionnew(module, arg, defn_options)View SourceCreates a new module-based serving.It expects a module and an argument that is given to its init
callback.A third optional argument called defn_options are additional
compiler options which will be given to the module. Those options
will be merged into Nx.Defn.default_options/0.Link to this functionprocess_options(serving, opts)View SourceSets the process options of this serving.These are the same options as supported on start_link/1,
except :name and :serving itself.Link to this functionrun(serving, input)View SourceRuns serving with the given input inline with the current process.The serving is executed immediately, without waiting or batching inputs
from other processes. If a batch_size/2 is specified, then the input may
be split or padded, but they are still executed immediately inline.Link to this functionstart_link(opts)View SourceStarts a Nx.Serving process to batch requests to a given serving.OptionsAll options, except :name and :serving, can also be set via
process_options/2.:name - an atom with the name of the process:serving - a Nx.Serving struct with the serving configuration:batch_keys - all available batch keys. Batch keys allows Nx.Serving
to accumulate different batches with different properties. Defaults to
[:default]:batch_size - the maximum batch size. A default value can be set with
batch_size/2, which applies to both run/2 and batched_run/2.
Setting this option only affects batched_run/2 and it defaults to 1
if none is set. Note batches received by the serving are not automatically
padded to the batch size, such can be done with Nx.Batch.pad/2.:batch_timeout - the maximum time to wait, in milliseconds,
before executing the batch (defaults to 100ms):partitions - when true, starts several partitions under this serving.
The number of partitions will be determined according to your compiler
and for which host it is compiling. See the module docs for more information:distribution_weight - weight used for load balancing when running
a distributed serving. Defaults to 1.
If it is set to a higher number w, the serving process will receive,
on average, w times the number of requests compared to the
default. Note that the weight is multiplied with the number of
partitions, if partitioning is enabled.:shutdown - the maximum time for the serving to shutdown. This will
block until the existing computation finishes (defaults to 30_000ms):hibernate_after and :spawn_opt - configure the underlying serving
workers (see GenServer.start_link/3)Link to this functionstreaming(serving, opts \\ [])View SourceConfigure the serving to stream its results.Once run/2 or batched_run/2 are invoked, it will then
return a stream. The stream must be consumed in the same
process that calls run/2 or batched_run/2.Batches will be streamed as they arrive. You may also opt-in
to stream Nx.Defn hooks.Options:hooks - a list of hook names that will become streaming eventsImplementation detailsClient postprocessingOnce streaming is enabled, the client postprocessing callback
will receive a stream which will emit events for each hook
in the shape of:{hook_name,term()}The stream will also receive events in the shape of
{:batch, output, metadata} as batches are processed by the
serving. The client postprocessing is often expected to call
Stream.transform/3 to process those events into something
usable by callers.If the :hooks option is given, only a single :batch event
is emitted, at the end, as detailed next.Batch limitsIf you are streaming hooks, the serving server can no longer break
batch and you are unable to push a payload bigger than :batch_size.
For example, imagine you have a batch_size of 3 and you push three
batches of two elements (AA, BB, and CC). Without hooks, the batches
will be consumed as:AAB->BCCWith streaming, we can't break the batch BB, as above, so we will
consistently pad with zeroes:AA0->BB0->CC0In practice, this should not be a major problem, as you should
generally avoid having a batch size that is not a multiple of the
most common batches.Hex PackageHex Preview
            Search HexDocs
          
              Download ePub version
            
        Built using
        ExDoc (v0.34.2) for the

          Elixir programming languageNx.Stream — Nx v0.9.2
Nx
        
          v0.9.2
        
Pages
        
            Modules
          Search documentation of NxSettingsView SourceNx.Streamprotocol(Nx v0.9.2)The protocol for streaming data in and out of backends.SummaryTypest()All the types that implement this protocol.Functionsdone(stream)Returns the output of the stream.recv(stream)Receives data from the stream.send(stream, tensor)Sends a tensor.TypesLink to this typet()View Source@type t() :: term()All the types that implement this protocol.FunctionsLink to this functiondone(stream)View SourceReturns the output of the stream.It may be a tensor, a tuple of tensors, or a map of tensors.Link to this functionrecv(stream)View SourceReceives data from the stream.It may be a tensor, a tuple of tensors, or a map of tensors.Link to this functionsend(stream, tensor)View SourceSends a tensor.Returns the given tensor.Hex PackageHex Preview
            Search HexDocs
          
              Download ePub version
            
        Built using
        ExDoc (v0.34.2) for the

          Elixir programming languageNx.TemplateBackend — Nx v0.9.2
Nx
        
          v0.9.2
        
Pages
        
            Modules
          Search documentation of NxSettingsView SourceNx.TemplateBackend(Nx v0.9.2)An opaque backend written that is used as template
to declare the type, shape, and names of tensors to
be expected in the future.It doesn't perform any operation, it always raises.Hex PackageHex Preview
            Search HexDocs
          
              Download ePub version
            
        Built using
        ExDoc (v0.34.2) for the

          Elixir programming languageNx.Tensor — Nx v0.9.2
Nx
        
          v0.9.2
        
Pages
        
            Modules
          Search documentation of NxSettingsView SourceNx.Tensor(Nx v0.9.2)The tensor struct and the behaviour for backends.Nx.Tensor is a generic container for multidimensional data structures.
It contains the tensor type, shape, and names. The data itself is a
struct that points to a backend responsible for controlling the data.
The backend behaviour is described in Nx.Backend.The tensor has the following fields::data - the tensor backend and its data:shape - the tensor shape:type - the tensor type:names - the tensor names:vectorized_axes - a tuple that encodes names and sizes for vectorizationIn general it is discouraged to access those fields directly. Use
the functions in the Nx module instead. Backends have to access those
fields but it cannot update them, except for the :data field itself.SummaryTypesaxes()axis()data()name()shape()t()t(data)type()TypesLink to this typeaxes()View Source@type axes() :: [axis()]Link to this typeaxis()View Source@type axis() :: name() | integer()Link to this typedata()View Source@type data() :: Nx.Backend.t()Link to this typename()View Source@type name() :: atom()Link to this typeshape()View Source@type shape() :: tuple()Link to this typet()View Source@type t() :: %Nx.Tensor{
  data: data(),
  names: [name()],
  shape: shape(),
  type: type(),
  vectorized_axes: term()
}Link to this typet(data)View Source@type t(data) :: %Nx.Tensor{
  data: data,
  names: [name()],
  shape: shape(),
  type: type(),
  vectorized_axes: term()
}Link to this typetype()View Source@type type() :: Nx.Type.t()Hex PackageHex Preview
            Search HexDocs
          
              Download ePub version
            
        Built using
        ExDoc (v0.34.2) for the

          Elixir programming languageNx.Type — Nx v0.9.2
Nx
        
          v0.9.2
        
Pages
        
            Modules
          Search documentation of NxSettingsView SourceNx.Type(Nx v0.9.2)Conveniences for working with types.A type is a two-element tuple with the name and the size.
The respective sizes for the types are the following::s - signed integer (2, 4, 8, 16, 32, 64):u - unsigned integer (2, 4, 8, 16, 32, 64):f - float (8, 16, 32, 64):bf - a brain floating point (16):c - a complex number, represented as a pair of floats (64, 128)Each type has an equivalent atom representation, for example
{:s, 8} can be expressed as :s8. When working with user-given
types make sure to call normalize!/1 to get the canonical
representation.Note: there is a special type used by the defn compiler
which is {:tuple, size}, that represents a tuple. Said types
do not appear on user code, only on compiler implementations,
and therefore are not handled by the functions in this module.This module can be used in defn.SummaryTypesshort_t()t()Functionscast_number!(type, int)Casts the given number to type.complex?(arg)Returns true if the type is a complex number.e_binary(type)Returns $e$ as a binary for the given typeepsilon_binary(type)Returns the machine epsilon for the given typeeuler_gamma_binary(type)Returns Euler–Mascheroni constant ($\gamma$) as a binary for the given typefloat?(arg)Returns true if the type is a float in Elixir.infer(value)Infers the type of the given number.infinity_binary(type)Returns infinity as a binary for the given type.integer?(arg)Returns true if the type is an integer in Elixir.max_binary(type)Returns the maximum possible value for the given type.max_finite_binary(type)Returns the maximum possible finite value for the given type.merge(left, right)Merges the given types finding a suitable representation for both.merge_number(arg1, integer)Merges the given types with the type of a number.min_binary(type)Returns the minimum possible value for the given type.min_finite_binary(type)Returns the minimum possible finite value for the given type.nan_binary(type)Returns infinity as a binary for the given type.neg_infinity_binary(type)Returns negative infinity as a binary for the given type.normalize!(type)Validates and normalizes the given type tuple.pi_binary(type)Returns $\pi$ as a binary for the given typesmallest_positive_normal_binary(type)Returns the smallest positive number as a binary for the given typeto_aggregate(type)Converts the given type to an aggregation precision.to_complex(arg1)Converts the given type to a complex representation with
the minimum size necessary.to_floating(type)Converts the given type to a floating point representation
with the minimum size necessary.to_real(arg1)Converts the given type to a real number representation
with the minimum size necessary.to_string(arg)Returns a string representation of the given type.TypesLink to this typeshort_t()View Source@type short_t() ::
  :s8
  | :s16
  | :s32
  | :s64
  | :u8
  | :u16
  | :u32
  | :u64
  | :f8
  | :f16
  | :f32
  | :f64
  | :bf16
  | :c64
  | :c128Link to this typet()View Source@type t() ::
  {:s, 2}
  | {:s, 4}
  | {:s, 8}
  | {:s, 16}
  | {:s, 32}
  | {:s, 64}
  | {:u, 2}
  | {:u, 4}
  | {:u, 8}
  | {:u, 16}
  | {:u, 32}
  | {:u, 64}
  | {:f, 8}
  | {:f, 16}
  | {:f, 32}
  | {:f, 64}
  | {:bf, 16}
  | {:c, 64}
  | {:c, 128}
  | {:tuple, non_neg_integer()}FunctionsLink to this functioncast_number!(type, int)View SourceCasts the given number to type.It does not handle overflow/underflow,
returning the number as is, but cast.Examplesiex> Nx.Type.cast_number!({:u,8},10)10iex> Nx.Type.cast_number!({:s,8},10)10iex> Nx.Type.cast_number!({:s,8},-10)-10iex> Nx.Type.cast_number!({:f,32},10)10.0iex> Nx.Type.cast_number!({:bf,16},-10)-10.0iex> Nx.Type.cast_number!({:f,32},10.0)10.0iex> Nx.Type.cast_number!({:bf,16},-10.0)-10.0iex> Nx.Type.cast_number!({:c,64},10)%Complex{im:0.0,re:10.0}iex> Nx.Type.cast_number!({:u,8},-10)** (ArgumentError) cannot cast number -10 to {:u, 8}iex> Nx.Type.cast_number!({:s,8},10.0)** (ArgumentError) cannot cast number 10.0 to {:s, 8}Link to this functioncomplex?(arg)View SourceReturns true if the type is a complex number.Examplesiex> Nx.Type.complex?({:c,64})trueiex> Nx.Type.complex?({:f,64})falseLink to this functione_binary(type)View SourceReturns $e$ as a binary for the given typeLink to this functionepsilon_binary(type)View SourceReturns the machine epsilon for the given typeLink to this functioneuler_gamma_binary(type)View SourceReturns Euler–Mascheroni constant ($\gamma$) as a binary for the given typeLink to this functionfloat?(arg)View SourceReturns true if the type is a float in Elixir.Examplesiex> Nx.Type.float?({:f,32})trueiex> Nx.Type.float?({:bf,16})trueiex> Nx.Type.float?({:u,64})falseLink to this functioninfer(value)View SourceInfers the type of the given number.Examplesiex> Nx.Type.infer(1){:s,32}iex> Nx.Type.infer(1.0){:f,32}iex> Nx.Type.infer(Complex.new(1)){:c,64}Link to this functioninfinity_binary(type)View SourceReturns infinity as a binary for the given type.Link to this functioninteger?(arg)View SourceReturns true if the type is an integer in Elixir.Examplesiex> Nx.Type.integer?({:s,8})trueiex> Nx.Type.integer?({:u,64})trueiex> Nx.Type.integer?({:f,64})falseLink to this functionmax_binary(type)View SourceReturns the maximum possible value for the given type.Link to this functionmax_finite_binary(type)View SourceReturns the maximum possible finite value for the given type.Link to this functionmerge(left, right)View SourceMerges the given types finding a suitable representation for both.Types have the following precedence:c>f>bf>s>uIf the types are the same, they are merged to the highest size.
If they are different, the one with the highest precedence wins,
as long as the size of the max(big, small * 2)) fits under 64
bits. Otherwise it casts to f64.In the case of complex numbers, the maximum bit size is 128 bits
because they are composed of two floats.Examplesiex> Nx.Type.merge({:s,8},{:s,8}){:s,8}iex> Nx.Type.merge({:s,8},{:s,64}){:s,64}iex> Nx.Type.merge({:s,8},{:u,8}){:s,16}iex> Nx.Type.merge({:s,16},{:u,8}){:s,16}iex> Nx.Type.merge({:s,8},{:u,16}){:s,32}iex> Nx.Type.merge({:s,32},{:u,8}){:s,32}iex> Nx.Type.merge({:s,8},{:u,32}){:s,64}iex> Nx.Type.merge({:s,64},{:u,8}){:s,64}iex> Nx.Type.merge({:s,8},{:u,64}){:s,64}iex> Nx.Type.merge({:u,8},{:f,32}){:f,32}iex> Nx.Type.merge({:u,64},{:f,32}){:f,32}iex> Nx.Type.merge({:s,8},{:f,32}){:f,32}iex> Nx.Type.merge({:s,64},{:f,32}){:f,32}iex> Nx.Type.merge({:u,8},{:f,64}){:f,64}iex> Nx.Type.merge({:u,64},{:f,64}){:f,64}iex> Nx.Type.merge({:s,8},{:f,64}){:f,64}iex> Nx.Type.merge({:s,64},{:f,64}){:f,64}iex> Nx.Type.merge({:u,8},{:bf,16}){:bf,16}iex> Nx.Type.merge({:u,64},{:bf,16}){:bf,16}iex> Nx.Type.merge({:s,8},{:bf,16}){:bf,16}iex> Nx.Type.merge({:s,64},{:bf,16}){:bf,16}iex> Nx.Type.merge({:f,32},{:bf,16}){:f,32}iex> Nx.Type.merge({:f,64},{:bf,16}){:f,64}iex> Nx.Type.merge({:c,64},{:f,32}){:c,64}iex> Nx.Type.merge({:c,64},{:c,64}){:c,64}iex> Nx.Type.merge({:c,128},{:c,64}){:c,128}Link to this functionmerge_number(arg1, integer)View SourceMerges the given types with the type of a number.We attempt to keep the original type and its size as best
as possible.Examplesiex> Nx.Type.merge_number({:u,8},0){:u,8}iex> Nx.Type.merge_number({:u,8},255){:u,8}iex> Nx.Type.merge_number({:u,8},256){:u,16}iex> Nx.Type.merge_number({:u,8},-1){:s,16}iex> Nx.Type.merge_number({:u,8},-32767){:s,16}iex> Nx.Type.merge_number({:u,8},-32768){:s,16}iex> Nx.Type.merge_number({:u,8},-32769){:s,32}iex> Nx.Type.merge_number({:s,8},0){:s,8}iex> Nx.Type.merge_number({:s,8},127){:s,8}iex> Nx.Type.merge_number({:s,8},-128){:s,8}iex> Nx.Type.merge_number({:s,8},128){:s,16}iex> Nx.Type.merge_number({:s,8},-129){:s,16}iex> Nx.Type.merge_number({:s,8},1.0){:f,32}iex> Nx.Type.merge_number({:u,64},-1337){:s,64}iex> Nx.Type.merge_number({:f,32},1){:f,32}iex> Nx.Type.merge_number({:f,32},1.0){:f,32}iex> Nx.Type.merge_number({:f,64},1.0){:f,64}Link to this functionmin_binary(type)View SourceReturns the minimum possible value for the given type.Link to this functionmin_finite_binary(type)View SourceReturns the minimum possible finite value for the given type.Link to this functionnan_binary(type)View SourceReturns infinity as a binary for the given type.Link to this functionneg_infinity_binary(type)View SourceReturns negative infinity as a binary for the given type.Link to this functionnormalize!(type)View SourceValidates and normalizes the given type tuple.It returns the type tuple or raises.Accepts both the tuple format and the short atom format.Examplesiex> Nx.Type.normalize!({:u,8}){:u,8}iex> Nx.Type.normalize!(:u8){:u,8}iex> Nx.Type.normalize!({:u,0})** (ArgumentError) invalid numerical type: {:u, 0} (see Nx.Type docs for all supported types)iex> Nx.Type.normalize!({:k,8})** (ArgumentError) invalid numerical type: {:k, 8} (see Nx.Type docs for all supported types)Link to this functionpi_binary(type)View SourceReturns $\pi$ as a binary for the given typeLink to this functionsmallest_positive_normal_binary(type)View SourceReturns the smallest positive number as a binary for the given typeLink to this functionto_aggregate(type)View SourceConverts the given type to an aggregation precision.Examplesiex> Nx.Type.to_aggregate({:s,8}){:s,32}iex> Nx.Type.to_aggregate({:u,16}){:u,32}iex> Nx.Type.to_aggregate({:s,64}){:s,64}iex> Nx.Type.to_aggregate({:bf,16}){:bf,16}iex> Nx.Type.to_aggregate({:f,32}){:f,32}iex> Nx.Type.to_aggregate({:c,64}){:c,64}Link to this functionto_complex(arg1)View SourceConverts the given type to a complex representation with
the minimum size necessary.Examplesiex> Nx.Type.to_complex({:s,64}){:c,64}iex> Nx.Type.to_complex({:bf,16}){:c,64}iex> Nx.Type.to_complex({:f,32}){:c,64}iex> Nx.Type.to_complex({:c,64}){:c,64}iex> Nx.Type.to_complex({:f,64}){:c,128}iex> Nx.Type.to_complex({:c,128}){:c,128}Link to this functionto_floating(type)View SourceConverts the given type to a floating point representation
with the minimum size necessary.Note both float and complex are floating point representations.Examplesiex> Nx.Type.to_floating({:s,8}){:f,32}iex> Nx.Type.to_floating({:s,32}){:f,32}iex> Nx.Type.to_floating({:bf,16}){:bf,16}iex> Nx.Type.to_floating({:f,32}){:f,32}iex> Nx.Type.to_floating({:c,64}){:c,64}Link to this functionto_real(arg1)View SourceConverts the given type to a real number representation
with the minimum size necessary.Examplesiex> Nx.Type.to_real({:s,8}){:f,32}iex> Nx.Type.to_real({:s,64}){:f,32}iex> Nx.Type.to_real({:bf,16}){:bf,16}iex> Nx.Type.to_real({:c,64}){:f,32}iex> Nx.Type.to_real({:c,128}){:f,64}iex> Nx.Type.to_real({:f,32}){:f,32}iex> Nx.Type.to_real({:f,64}){:f,64}Link to this functionto_string(arg)View SourceReturns a string representation of the given type.Examplesiex> Nx.Type.to_string({:s,8})"s8"iex> Nx.Type.to_string({:s,16})"s16"iex> Nx.Type.to_string({:s,32})"s32"iex> Nx.Type.to_string({:s,64})"s64"iex> Nx.Type.to_string({:u,8})"u8"iex> Nx.Type.to_string({:u,16})"u16"iex> Nx.Type.to_string({:u,32})"u32"iex> Nx.Type.to_string({:u,64})"u64"iex> Nx.Type.to_string({:f,16})"f16"iex> Nx.Type.to_string({:bf,16})"bf16"iex> Nx.Type.to_string({:f,32})"f32"iex> Nx.Type.to_string({:f,64})"f64"Hex PackageHex Preview
            Search HexDocs
          
              Download ePub version
            
        Built using
        ExDoc (v0.34.2) for the

          Elixir programming languageNx — Nx v0.9.2
Nx
        
          v0.9.2
        
Pages
        
            Modules
          Search documentation of NxSettingsView SourceNx(Nx v0.9.2)Numerical Elixir.The Nx library is a collection of functions and data
types to work with Numerical Elixir. This module defines
the main entry point for building and working with said
data-structures. For example, to create an n-dimensional
tensor, do:iex> t=Nx.tensor([[1,2],[3,4]])iex> Nx.shape(t){2,2}Nx also provides the so-called numerical definitions under
the Nx.Defn module. They are a subset of Elixir tailored for
numerical computations. For example, it overrides Elixir's
default operators so they are tensor-aware:defnsoftmax(t)doNx.exp(t)/Nx.sum(Nx.exp(t))endCode inside defn functions can also be given to custom compilers,
which can compile said functions just-in-time (JIT) to run on the
CPU or on the GPU.ReferencesHere is a general outline of the main references in this library:For an introduction, see our Intro to Nx guideThis module provides the main API for working with tensorsNx.Defn provides numerical definitions, CPU/GPU compilation, gradients, and moreNx.LinAlg provides functions related to linear algebraNx.Constants declares many constants commonly used in numerical codeContinue reading this documentation for an overview of creating,
broadcasting, and accessing/slicing Nx tensors.Creating tensorsThe main APIs for creating tensors are tensor/2, from_binary/2,
iota/2, eye/2, and broadcast/3.The tensor types can be one of:unsigned integers (u2, u4, u8, u16, u32, u64)signed integers (s2, s4, s8, s16, s32, s64)floats (f8, f16, f32, f64)brain floats (bf16)and complex numbers (c64, c128)The types are tracked as tuples:iex> Nx.tensor([1,2,3],type:{:f,32})#Nx.Tensor<f32[3][1.0,2.0,3.0]>But a shortcut atom notation is also available:iex> Nx.tensor([1,2,3],type::f32)#Nx.Tensor<f32[3][1.0,2.0,3.0]>The tensor dimensions can also be named, via the :names option
available to all creation functions:iex> Nx.iota({2,3},names:[:x,:y])#Nx.Tensor<s32[x:2][y:3][[0,1,2],[3,4,5]]>Finally, for creating vectors and matrices, a sigil notation
is available:iex> importNx,only::sigilsiex> ~VEC[123]f32#Nx.Tensor<f32[3][1.0,2.0,3.0]>iex> importNx,only::sigilsiex> ~MAT'''
...> 1 2 3
...> 4 5 6
...> '''s32#Nx.Tensor<s32[2][3][[1,2,3],[4,5,6]]>All other APIs accept exclusively numbers or tensors, unless
explicitly noted otherwise.BroadcastingBroadcasting allows operations on two tensors of different shapes
to match. For example, most often operations between tensors have
the same shape:iex> a=Nx.tensor([1,2,3])iex> b=Nx.tensor([10,20,30])iex> Nx.add(a,b)#Nx.Tensor<s32[3][11,22,33]>Now let's imagine you want to multiply a large tensor of dimensions
1000x1000x1000 by 2. If you had to create a similarly large tensor
only to perform this operation, it would be inefficient. Therefore,
you can simply multiply this large tensor by the scalar 2, and Nx
will propagate its dimensions at the time the operation happens,
without allocating a large intermediate tensor:iex> Nx.multiply(Nx.tensor([1,2,3]),2)#Nx.Tensor<s32[3][2,4,6]>In practice, broadcasting is not restricted only to scalars; it
is a general algorithm that applies to all dimensions of a tensor.
When broadcasting, Nx compares the shapes of the two tensors,
starting with the trailing ones, such that:If the dimensions have equal size, then they are compatibleIf one of the dimensions have size of 1, it is "broadcast"
to match the dimension of the otherIn case one tensor has more dimensions than the other, the missing
dimensions are considered to be of size one. Here are some examples
of how broadcast would work when multiplying two tensors with the
following shapes:s32[3]*s64#=> s32[3]s32[255][255][3]*s32[3]#=> s32[255][255][3]s32[2][1]*s[1][2]#=> s32[2][2]s32[5][1][4][1]*s32[3][4][5]#=> s32[5][3][4][5]If any of the dimensions do not match or are not 1, an error is
raised.Access syntax (slicing)Nx tensors implement Elixir's access syntax. This allows developers
to slice tensors up and easily access sub-dimensions and values.Access accepts integers:iex> t=Nx.tensor([[1,2],[3,4]])iex> t[0]#Nx.Tensor<s32[2][1,2]>iex> t[1]#Nx.Tensor<s32[2][3,4]>iex> t[1][1]#Nx.Tensor<s324>If a negative index is given, it accesses the element from the back:iex> t=Nx.tensor([[1,2],[3,4]])iex> t[-1][-1]#Nx.Tensor<s324>Out of bound access will raise:iex> Nx.tensor([1,2])[2]** (ArgumentError) index 2 is out of bounds for axis 0 in shape {2}iex> Nx.tensor([1,2])[-3]** (ArgumentError) index -3 is out of bounds for axis 0 in shape {2}The index can also be another tensor. If the tensor is a scalar, it must
be a value between 0 and the dimension size, and it behaves the same as
an integer. Out of bound dynamic indexes are always clamped to the tensor
dimensions:iex> two=Nx.tensor(2)iex> t=Nx.tensor([[1,2],[3,4]])iex> t[two][two]#Nx.Tensor<s324>For example, a minus_one dynamic index will be clamped to zero:iex> minus_one=Nx.tensor(-1)iex> t=Nx.tensor([[1,2],[3,4]])iex> t[minus_one][minus_one]#Nx.Tensor<s321>A multi-dimensional tensor uses its values to fetch the leading
dimension of the tensor, placing them within the shape of the
indexing tensor. It is equivalent to take/3:iex> t=Nx.tensor([[1,2],[3,4]])iex> t[Nx.tensor([1,0])]#Nx.Tensor<s32[2][2][[3,4],[1,2]]>The example shows how the retrieved indexes are nested
with the accessed shape and that you may also access
repeated indices:iex> t=Nx.tensor([[1,2],[3,4]])iex> t[Nx.tensor([[1,0,1]])]#Nx.Tensor<s32[1][3][2][[[3,4],[1,2],[3,4]]]>Access also accepts ranges. Ranges in Elixir are inclusive:iex> t=Nx.tensor([[1,2],[3,4],[5,6],[7,8]])iex> t[0..1]#Nx.Tensor<s32[2][2][[1,2],[3,4]]>Ranges can receive negative positions and they will read from
the back. In such cases, the range step must be explicitly given
and the right-side of the range must be equal or greater than
the left-side:iex> t=Nx.tensor([[1,2],[3,4],[5,6],[7,8]])iex> t[1..-2//1]#Nx.Tensor<s32[2][2][[3,4],[5,6]]>As you can see, accessing with a range does not eliminate the
accessed axis. This means that, if you try to cascade ranges,
you will always be filtering the highest dimension:iex> t=Nx.tensor([[1,2],[3,4],[5,6],[7,8]])iex> t[1..-1//1]# Drop the first "row"#Nx.Tensor<s32[3][2][[3,4],[5,6],[7,8]]>iex> t[1..-1//1][1..-1//1]# Drop the first "row" twice#Nx.Tensor<s32[2][2][[5,6],[7,8]]>Therefore, if you want to slice across multiple dimensions, you can wrap
the ranges in a list:iex> t=Nx.tensor([[1,2],[3,4],[5,6],[7,8]])iex> t[[1..-1//1,1..-1//1]]# Drop the first "row" and the first "column"#Nx.Tensor<s32[3][1][[4],[6],[8]]>You can also use .. as the full-slice range, which means you want to
keep a given dimension as is:iex> t=Nx.tensor([[1,2],[3,4],[5,6],[7,8]])iex> t[[..,1..-1//1]]# Drop only the first "column"#Nx.Tensor<s32[4][1][[2],[4],[6],[8]]>You can mix both ranges and integers in the list too:iex> t=Nx.tensor([[1,2,3],[4,5,6],[7,8,9],[10,11,12]])iex> t[[1..2,2]]#Nx.Tensor<s32[2][6,9]>If the list has less elements than axes, the remaining dimensions
are returned in full:iex> t=Nx.tensor([[1,2,3],[4,5,6],[7,8,9],[10,11,12]])iex> t[[1..2]]#Nx.Tensor<s32[2][3][[4,5,6],[7,8,9]]>The access syntax also pairs nicely with named tensors. By using named
tensors, you can pass only the axis you want to slice, leaving the other
axes intact:iex> t=Nx.tensor([[1,2,3],[4,5,6],[7,8,9],[10,11,12]],names:[:x,:y])iex> t[x:1..2]#Nx.Tensor<s32[x:2][y:3][[4,5,6],[7,8,9]]>iex> t[x:1..2,y:0..1]#Nx.Tensor<s32[x:2][y:2][[4,5],[7,8]]>iex> t[x:1,y:0..1]#Nx.Tensor<s32[y:2][4,5]>For a more complex slicing rules, including strides, you
can always fallback to Nx.slice/4.BackendsThe Nx library has built-in support for multiple backends.
A tensor is always handled by a backend, the default backend
being Nx.BinaryBackend, which means the tensor is allocated
as a binary within the Erlang VM.Most often backends are used to provide a completely different
implementation of tensor operations, often accelerated to the GPU.
In such cases, you want to guarantee all tensors are allocated in
the new backend. This can be done by configuring your runtime:# config/runtime.exsimportConfigconfig:nx,default_backend:EXLA.BackendIn your notebooks and on Mix.install/2, you might:Mix.install([{:nx,">= 0.0.0"}],config:[nx:[default_backend:EXLA.Backend]])Or by calling Nx.global_default_backend/1 (less preferrable):Nx.global_default_backend(EXLA.Backend)To pass options to the backend, replacing EXLA.Backend by
{EXLA.Backend, client: :cuda} or similar. See the documentation
for EXLA and Torchx
for installation and GPU support.To implement your own backend, check the Nx.Tensor behaviour.SummaryGuardsis_tensor(t)Checks whether the value is a valid numerical value.Functions: Aggregatesall(tensor, opts \\ [])Returns a scalar tensor of value 1 if all of the
tensor values are not zero. Otherwise the value is 0.all_close(a, b, opts \\ [])Returns a scalar tensor of value 1 if all element-wise values
are within tolerance of b. Otherwise returns value 0.any(tensor, opts \\ [])Returns a scalar tensor of value 1 if any of the
tensor values are not zero. Otherwise the value is 0.argmax(tensor, opts \\ [])Returns the indices of the maximum values.argmin(tensor, opts \\ [])Returns the indices of the minimum values.covariance(tensor, opts \\ [])A shortcut to covariance/3 with either opts or mean as second argument.covariance(tensor, mean, opts)Computes the covariance matrix of the input tensor.logsumexp(tensor, opts \\ [])Returns the logarithm of the sum of the exponentials of tensor elements.mean(tensor, opts \\ [])Returns the mean for the tensor.median(tensor, opts \\ [])Returns the median for the tensor.mode(tensor, opts \\ [])Returns the mode of a tensor.product(tensor, opts \\ [])Returns the product for the tensor.reduce(tensor, acc, opts \\ [], fun)Reduces over a tensor with the given accumulator.reduce_max(tensor, opts \\ [])Returns the maximum values of the tensor.reduce_min(tensor, opts \\ [])Returns the minimum values of the tensor.standard_deviation(tensor, opts \\ [])Finds the standard deviation of a tensor.sum(tensor, opts \\ [])Returns the sum for the tensor.variance(tensor, opts \\ [])Finds the variance of a tensor.weighted_mean(tensor, weights, opts \\ [])Returns the weighted mean for the tensor and the weights.Functions: Backendbackend_copy(tensor_or_container, backend \\ Nx.BinaryBackend)Copies data to the given backend.backend_deallocate(tensor_or_container)Deallocates data in a device.backend_transfer(tensor_or_container, backend \\ Nx.BinaryBackend)Transfers data to the given backend.default_backend()Gets the default backend for the current process.default_backend(backend)Sets the given backend as default in the current process.global_default_backend(backend)Sets the default backend globally.with_default_backend(backend, fun)Invokes the given function temporarily setting backend as the
default backend.Functions: Conversiondeserialize(data, opts \\ [])Deserializes a serialized representation of a tensor or a container
with the given options.load_numpy!(data)Loads a .npy file into a tensor.load_numpy_archive!(archive)Loads a .npz archive into a list of tensors.serialize(tensor_or_container, opts \\ [])Serializes the given tensor or container of tensors to iodata.to_batched(tensor, batch_size, opts \\ [])Converts the underlying tensor to a stream of tensor batches.to_binary(tensor, opts \\ [])Returns the underlying tensor as a binary.to_flat_list(tensor, opts \\ [])Returns the underlying tensor as a flat list.to_heatmap(tensor, opts \\ [])Returns a heatmap struct with the tensor data.to_list(tensor)Converts the tensor into a list reflecting its structure.to_number(tensor)Returns the underlying tensor as a number.to_template(tensor_or_container)Converts a tensor (or tuples and maps of tensors) to tensor templates.to_tensor(t)Converts a data structure into a tensor.Functions: Creationbf16(tensor)Short-hand function for creating tensor of type bf16.eye(n_or_tensor_or_shape, opts \\ [])Creates the identity matrix of size n.f8(tensor)Short-hand function for creating tensor of type f8.f16(tensor)Short-hand function for creating tensor of type f16.f32(tensor)Short-hand function for creating tensor of type f32.f64(tensor)Short-hand function for creating tensor of type f64.from_binary(binary, type, opts \\ [])Creates a one-dimensional tensor from a binary with the given type.from_pointer(backend, pointer, type, shape, opts \\ [])Creates an Nx-tensor from an already-allocated memory space.iota(tensor_or_shape, opts \\ [])Creates a tensor with the given shape which increments
along the provided axis. You may optionally provide dimension
names.linspace(start, stop, opts \\ [])Creates a tensor of shape {n} with linearly spaced samples between start and stop.make_diagonal(tensor, opts \\ [])Creates a diagonal tensor from a 1D tensor.put_diagonal(tensor, diagonal, opts \\ [])Puts the individual values from a 1D diagonal into the diagonal indices
of the given 2D tensor.s2(tensor)Short-hand function for creating tensor of type s2.s4(tensor)Short-hand function for creating tensor of type s4.s8(tensor)Short-hand function for creating tensor of type s8.s16(tensor)Short-hand function for creating tensor of type s16.s32(tensor)Short-hand function for creating tensor of type s32.s64(tensor)Short-hand function for creating tensor of type s64.sigil_MAT(arg, modifiers)A convenient ~MAT sigil for building matrices (two-dimensional tensors).sigil_VEC(arg, modifiers)A convenient ~VEC sigil for building vectors (one-dimensional tensors).take_diagonal(tensor, opts \\ [])Extracts the diagonal of batched matrices.template(shape, type, opts \\ [])Creates a tensor template.tensor(arg, opts \\ [])Builds a tensor.to_pointer(tensor, opts \\ [])Returns an Nx.Pointer that represents either a local pointer or an IPC handle for the given tensor.tri(n, m, opts \\ [])An array with ones at and below the given diagonal and zeros elsewhere.tril(tensor, opts \\ [])Lower triangle of a matrix.triu(tensor, opts \\ [])Upper triangle of an array.u2(tensor)Short-hand function for creating tensor of type u2.u4(tensor)Short-hand function for creating tensor of type u4.u8(tensor)Short-hand function for creating tensor of type u8.u16(tensor)Short-hand function for creating tensor of type u16.u32(tensor)Short-hand function for creating tensor of type u32.u64(tensor)Short-hand function for creating tensor of type u64.Functions: Cumulativecumulative_max(tensor, opts \\ [])Returns the cumulative maximum of elements along an axis.cumulative_min(tensor, opts \\ [])Returns the cumulative minimum of elements along an axis.cumulative_product(tensor, opts \\ [])Returns the cumulative product of elements along an axis.cumulative_sum(tensor, opts \\ [])Returns the cumulative sum of elements along an axis.Functions: Element-wiseabs(tensor)Computes the absolute value of each element in the tensor.acos(tensor)Calculates the inverse cosine of each element in the tensor.acosh(tensor)Calculates the inverse hyperbolic cosine of each element in the tensor.add(left, right)Element-wise addition of two tensors.asin(tensor)Calculates the inverse sine of each element in the tensor.asinh(tensor)Calculates the inverse hyperbolic sine of each element in the tensor.atan2(left, right)Element-wise arc tangent of two tensors.atan(tensor)Calculates the inverse tangent of each element in the tensor.atanh(tensor)Calculates the inverse hyperbolic tangent of each element in the tensor.bitwise_and(left, right)Element-wise bitwise AND of two tensors.bitwise_not(tensor)Applies bitwise not to each element in the tensor.bitwise_or(left, right)Element-wise bitwise OR of two tensors.bitwise_xor(left, right)Element-wise bitwise XOR of two tensors.cbrt(tensor)Calculates the cube root of each element in the tensor.ceil(tensor)Calculates the ceil of each element in the tensor.clip(tensor, min, max)Clips the values of the tensor on the closed
interval [min, max].complex(real, imag)Constructs a complex tensor from two equally-shaped tensors.conjugate(tensor)Calculates the complex conjugate of each element in the tensor.cos(tensor)Calculates the cosine of each element in the tensor.cosh(tensor)Calculates the hyperbolic cosine of each element in the tensor.count_leading_zeros(tensor)Counts the number of leading zeros of each element in the tensor.divide(left, right)Element-wise division of two tensors.equal(left, right)Element-wise equality comparison of two tensors.erf(tensor)Calculates the error function of each element in the tensor.erf_inv(tensor)Calculates the inverse error function of each element in the tensor.erfc(tensor)Calculates the one minus error function of each element in the tensor.exp(tensor)Calculates the exponential of each element in the tensor.expm1(tensor)Calculates the exponential minus one of each element in the tensor.fill(tensor, value, opts \\ [])Replaces every value in tensor with value.floor(tensor)Calculates the floor of each element in the tensor.greater(left, right)Element-wise greater than comparison of two tensors.greater_equal(left, right)Element-wise greater than or equal comparison of two tensors.imag(tensor)Returns the imaginary component of each entry in a complex tensor
as a floating point tensor.is_infinity(tensor)Determines if each element in tensor is Inf or -Inf.is_nan(tensor)Determines if each element in tensor is a NaN.left_shift(left, right)Element-wise left shift of two tensors.less(left, right)Element-wise less than comparison of two tensors.less_equal(left, right)Element-wise less than or equal comparison of two tensors.log1p(tensor)Calculates the natural log plus one of each element in the tensor.log2(tensor)Calculates the element-wise logarithm of a tensor with base 2.log10(tensor)Calculates the element-wise logarithm of a tensor with base 10.log(tensor)Calculates the natural log of each element in the tensor.log(tensor, base)Calculates the element-wise logarithm of a tensor with given base.logical_and(left, right)Element-wise logical and of two tensors.logical_not(tensor)Element-wise logical not a tensor.logical_or(left, right)Element-wise logical or of two tensors.logical_xor(left, right)Element-wise logical xor of two tensors.max(left, right)Element-wise maximum of two tensors.min(left, right)Element-wise minimum of two tensors.multiply(left, right)Element-wise multiplication of two tensors.negate(tensor)Negates each element in the tensor.not_equal(left, right)Element-wise not-equal comparison of two tensors.phase(tensor)Calculates the complex phase angle of each element in the tensor.
$$
phase(z) = atan2(b, a), z = a + bi \in \Complex
$$population_count(tensor)Computes the bitwise population count of each element in the tensor.pow(left, right)Element-wise power of two tensors.quotient(left, right)Element-wise integer division of two tensors.real(tensor)Returns the real component of each entry in a complex tensor
as a floating point tensor.remainder(left, right)Element-wise remainder of two tensors.right_shift(left, right)Element-wise right shift of two tensors.round(tensor)Calculates the round (away from zero) of each element in the tensor.rsqrt(tensor)Calculates the reverse square root of each element in the tensor.select(pred, on_true, on_false)Constructs a tensor from two tensors, based on a predicate.sigmoid(tensor)Calculates the sigmoid of each element in the tensor.sign(tensor)Computes the sign of each element in the tensor.sin(tensor)Calculates the sine of each element in the tensor.sinh(tensor)Calculates the hyperbolic sine of each element in the tensor.sqrt(tensor)Calculates the square root of each element in the tensor.subtract(left, right)Element-wise subtraction of two tensors.tan(tensor)Calculates the tangent of each element in the tensor.tanh(tensor)Calculates the hyperbolic tangent of each element in the tensor.Functions: Indexedgather(tensor, indices, opts \\ [])Builds a new tensor by taking individual values from the original
tensor at the given indices.indexed_add(target, indices, updates, opts \\ [])Performs an indexed add operation on the target tensor,
adding the updates into the corresponding indices positions.indexed_put(target, indices, updates, opts \\ [])Puts individual values from updates into the given tensor at the corresponding indices.put_slice(tensor, start_indices, slice)Puts the given slice into the given tensor at the given
start_indices.slice(tensor, start_indices, lengths, opts \\ [])Slices a tensor from start_indices with lengths.slice_along_axis(tensor, start_index, len, opts \\ [])Slices a tensor along the given axis.split(tensor, split, opts \\ [])Split a tensor into train and test subsets.take(tensor, indices, opts \\ [])Takes and concatenates slices along an axis.take_along_axis(tensor, indices, opts \\ [])Takes the values from a tensor given an indices tensor, along the specified axis.Functions: N-dimargsort(tensor, opts \\ [])Sorts the tensor along the given axis according
to the given direction and returns the corresponding indices
of the original tensor in the new sorted positions.concatenate(tensors, opts \\ [])Concatenates tensors along the given axis.conv(tensor, kernel, opts \\ [])Computes an n-D convolution (where n >= 3) as used in neural networks.diff(tensor, opts \\ [])Calculate the n-th discrete difference along the given axis.dot(t1, t2)Returns the dot product of two tensors.dot(t1, contract_axes1, t2, contract_axes2)Computes the generalized dot product between two tensors, given
the contracting axes.dot(t1_in, contract_axes1, batch_axes1, t2_in, contract_axes2, batch_axes2)Computes the generalized dot product between two tensors, given
the contracting and batch axes.fft2(tensor, opts \\ [])Calculates the 2D DFT of the given tensor.fft(tensor, opts \\ [])Calculates the DFT of the given tensor.ifft2(tensor, opts \\ [])Calculates the Inverse 2D DFT of the given tensor.ifft(tensor, opts \\ [])Calculates the Inverse DFT of the given tensor.outer(t1, t2)Computes the outer product of two tensors.reverse(tensor, opts \\ [])Reverses the tensor in the given dimensions.sort(tensor, opts \\ [])Sorts the tensor along the given axis according
to the given direction.stack(tensors, opts \\ [])Stacks a list of tensors with the same shape along a new axis.top_k(tensor, opts \\ [])Returns a tuple of {values, indices} for the top k
values in last dimension of the tensor.Functions: Shapeaxes(shape)Returns all of the axes in a tensor.axis_index(tensor, axis)Returns the index of the given axis in the tensor.axis_size(tensor, axis)Returns the size of a given axis of a tensor.bit_size(tensor)Returns the bit size of the data in the tensor
computed from its shape and type.broadcast(tensor, shape, opts \\ [])Broadcasts tensor to the given broadcast_shape.byte_size(tensor)Returns the byte size of the data in the tensor
computed from its shape and type.compatible?(left, right)Checks if two tensors have the same shape, type, and compatible names.flat_size(tensor)Returns the number of elements in the tensor (including vectorized axes).flatten(tensor, opts \\ [])Flattens a n-dimensional tensor to a 1-dimensional tensor.names(a)Returns all of the names in a tensor.new_axis(tensor, axis, name \\ nil)Adds a new axis of size 1 with optional name.pad(tensor, pad_value, padding_config)Pads a tensor with a given value.rank(shape)Returns the rank of a tensor.reflect(tensor, opts \\ [])Pads a tensor of rank 1 or greater along the given axes through periodic reflections.rename(tensor, names)Adds (or overrides) the given names to the tensor.reshape(tensor, new_shape, opts \\ [])Changes the shape of a tensor.shape(number)Returns the shape of the tensor as a tuple.size(shape)Returns the number of elements in the tensor.squeeze(tensor, opts \\ [])Squeezes the given size 1 dimensions out of the tensor.tile(tensor, repetitions)Creates a new tensor by repeating the input tensor
along the given axes.transpose(tensor, opts \\ [])Transposes a tensor to the given axes.Functions: Vectorizationbroadcast_vectors(tensors_or_containers, opts \\ [])Broadcasts vectorized axes, ensuring they end up with the same final size.devectorize(tensor_or_container, opts \\ [])Transforms a vectorized tensor back into a regular tensor.reshape_vectors(tensors_or_containers, opts \\ [])Reshapes input tensors so that they are all vectorized with the same vectors.revectorize(tensor, target_axes, opts \\ [])Changes the disposition of the vectorized axes of a tensor or Nx.Container.vectorize(tensor, name_or_axes)Transforms a tensor into a vectorized tensor.Functions: Typeas_type(tensor, type)Changes the type of a tensor.bitcast(tensor, type)Changes the type of a tensor, using a bitcast.type(tensor)Returns the type of the tensor.Functions: Windowwindow_max(tensor, window_dimensions, opts \\ [])Returns the maximum over each window of size window_dimensions
in the given tensor, producing a tensor that contains the same
number of elements as valid positions of the window.window_mean(tensor, window_dimensions, opts \\ [])Averages over each window of size window_dimensions in the
given tensor, producing a tensor that contains the same
number of elements as valid positions of the window.window_min(tensor, window_dimensions, opts \\ [])Returns the minimum over each window of size window_dimensions
in the given tensor, producing a tensor that contains the same
number of elements as valid positions of the window.window_product(tensor, window_dimensions, opts \\ [])Returns the product over each window of size window_dimensions
in the given tensor, producing a tensor that contains the same
number of elements as valid positions of the window.window_reduce(tensor, acc, window_dimensions, opts \\ [], fun)Reduces over each window of size dimensions
in the given tensor, producing a tensor that contains the same
number of elements as valid positions of the window.window_scatter_max(tensor, source, init_value, window_dimensions, opts \\ [])Performs a window_reduce to select the maximum index in each
window of the input tensor according to and scatters source tensor
to corresponding maximum indices in the output tensor.window_scatter_min(tensor, source, init_value, window_dimensions, opts \\ [])Performs a window_reduce to select the minimum index in each
window of the input tensor according to and scatters source tensor
to corresponding minimum indices in the output tensor.window_sum(tensor, window_dimensions, opts \\ [])Sums over each window of size window_dimensions in the
given tensor, producing a tensor that contains the same
number of elements as valid positions of the window.Typesaxes()axis()shape()t()Represents a numerical value.template()GuardsLink to this macrois_tensor(t)View Source(macro)Checks whether the value is a valid numerical value.Returns true if the value is a number, a non-finite atom (like :infinity),
a Complex number or an Nx.Tensor.See also: t/0Functions: AggregatesLink to this functionall(tensor, opts \\ [])View SourceReturns a scalar tensor of value 1 if all of the
tensor values are not zero. Otherwise the value is 0.If the :axes option is given, it aggregates over
the given dimensions, effectively removing them.
axes: [0] implies aggregating over the highest order
dimension and so forth. If the axis is negative, then
counts the axis from the back. For example, axes: [-1]
will always aggregate all rows.You may optionally set :keep_axes to true, which will
retain the rank of the input tensor by setting the reduced
axes to size 1.Examplesiex> Nx.all(Nx.tensor([0,1,2]))#Nx.Tensor<u80>iex> Nx.all(Nx.tensor([[-1,0,1],[2,3,4]],names:[:x,:y]),axes:[:x])#Nx.Tensor<u8[y:3][1,0,1]>iex> Nx.all(Nx.tensor([[-1,0,1],[2,3,4]],names:[:x,:y]),axes:[:y])#Nx.Tensor<u8[x:2][0,1]>Keeping axesiex> Nx.all(Nx.tensor([[-1,0,1],[2,3,4]],names:[:x,:y]),axes:[:y],keep_axes:true)#Nx.Tensor<u8[x:2][y:1][[0],[1]]>Vectorized tensorsiex> t=Nx.vectorize(Nx.tensor([[0,1],[1,1]]),:x)iex> Nx.all(t,axes:[0],keep_axes:true)#Nx.Tensor<vectorized[x:2]u8[1][[0],[1]]>iex> t=Nx.vectorize(Nx.tensor([1,0]),:x)iex> Nx.all(t)#Nx.Tensor<vectorized[x:2]u8[1,0]>Link to this functionall_close(a, b, opts \\ [])View SourceReturns a scalar tensor of value 1 if all element-wise values
are within tolerance of b. Otherwise returns value 0.You may set the absolute tolerance, :atol and relative tolerance
:rtol. Given tolerances, this functions returns 1 ifabsolute(a-b)<=(atol+rtol*absolute(b))is true for all elements of a and b.Options:rtol - relative tolerance between numbers, as described above. Defaults to 1.0e-5:atol - absolute tolerance between numbers, as described above. Defaults to 1.0e-8:equal_nan - if false, NaN will always compare as false.
Otherwise NaN will only equal NaN. Defaults to falseExamplesiex> Nx.all_close(Nx.tensor([1.0e10,1.0e-7]),Nx.tensor([1.00001e10,1.0e-8]))#Nx.Tensor<u80>iex> Nx.all_close(Nx.tensor([1.0e-8,1.0e-8]),Nx.tensor([1.0e-8,1.0e-9]))#Nx.Tensor<u81>Although NaN by definition isn't equal to itself, so this implementation
also considers all NaNs different from each other by default:iex> Nx.all_close(Nx.tensor(:nan),Nx.tensor(:nan))#Nx.Tensor<u80>iex> Nx.all_close(Nx.tensor(:nan),Nx.tensor(0))#Nx.Tensor<u80>We can change this behavior with the :equal_nan option:iex> t=Nx.tensor([:nan,1])iex> Nx.all_close(t,t,equal_nan:true)# nan == nan -> true#Nx.Tensor<u81>iex> Nx.all_close(t,t,equal_nan:false)# nan == nan -> false, default behavior#Nx.Tensor<u80>Infinities behave as expected, being "close" to themselves but not
to other numbers:iex> Nx.all_close(Nx.tensor(:infinity),Nx.tensor(:infinity))#Nx.Tensor<u81>iex> Nx.all_close(Nx.tensor(:infinity),Nx.tensor(:neg_infinity))#Nx.Tensor<u80>iex> Nx.all_close(Nx.tensor(1.0e30),Nx.tensor(:infinity))#Nx.Tensor<u80>Vectorized tensorsVectorized inputs have their vectorized axes broadcast together
before calculations are performed.iex> x=Nx.tensor([0,1])|>Nx.vectorize(:x)iex> Nx.all_close(x,x)#Nx.Tensor<vectorized[x:2]u8[1,1]>iex> x=Nx.tensor([0,1,2])|>Nx.vectorize(:x)iex> y=Nx.tensor([0,1])|>Nx.vectorize(:y)iex> Nx.all_close(x,y)#Nx.Tensor<vectorized[x:3][y:2]u8[[1,0],[0,1],[0,0]]>Link to this functionany(tensor, opts \\ [])View SourceReturns a scalar tensor of value 1 if any of the
tensor values are not zero. Otherwise the value is 0.If the :axes option is given, it aggregates over
the given dimensions, effectively removing them.
axes: [0] implies aggregating over the highest order
dimension and so forth. If the axis is negative, then
counts the axis from the back. For example, axes: [-1]
will always aggregate all rows.You may optionally set :keep_axes to true, which will
retain the rank of the input tensor by setting the reduced
axes to size 1.Examplesiex> Nx.any(Nx.tensor([0,1,2]))#Nx.Tensor<u81>iex> Nx.any(Nx.tensor([[0,1,0],[0,1,2]],names:[:x,:y]),axes:[:x])#Nx.Tensor<u8[y:3][0,1,1]>iex> Nx.any(Nx.tensor([[0,1,0],[0,1,2]],names:[:x,:y]),axes:[:y])#Nx.Tensor<u8[x:2][1,1]>Keeping axesiex> Nx.any(Nx.tensor([[0,1,0],[0,1,2]],names:[:x,:y]),axes:[:y],keep_axes:true)#Nx.Tensor<u8[x:2][y:1][[1],[1]]>Vectorized tensorsiex> t=Nx.vectorize(Nx.tensor([[0,1],[0,0]]),:x)iex> Nx.any(t,axes:[0],keep_axes:true)#Nx.Tensor<vectorized[x:2]u8[1][[1],[0]]>Link to this functionargmax(tensor, opts \\ [])View SourceReturns the indices of the maximum values.Options:axis - the axis to aggregate on. If no axis is given,
returns the index of the absolute maximum value in the tensor.:keep_axis - whether or not to keep the reduced axis with
a size of 1. Defaults to false.:tie_break - how to break ties. one of :high, or :low.
default behavior is to always return the lower index.:type - The type of the resulting tensor. Defaults to :s32.Examplesiex> Nx.argmax(4)#Nx.Tensor<s320>iex> t=Nx.tensor([[[4,2,3],[1,-5,3]],[[6,2,3],[4,8,3]]])iex> Nx.argmax(t)#Nx.Tensor<s3210>If a tensor of floats is given, it still returns integers:iex> Nx.argmax(Nx.tensor([2.0,4.0]))#Nx.Tensor<s321>If the tensor includes any NaNs, returns the index of any of them
(NaNs are not equal, hence tie-break does not apply):iex> Nx.argmax(Nx.tensor([2.0,:nan,4.0]))#Nx.Tensor<s321>Aggregating over an axisiex> t=Nx.tensor([[[4,2,3],[1,-5,3]],[[6,2,3],[4,8,3]]])iex> Nx.argmax(t,axis:0)#Nx.Tensor<s32[2][3][[1,0,0],[1,1,0]]>iex> t=Nx.tensor([[[4,2,3],[1,-5,3]],[[6,2,3],[4,8,3]]],names:[:x,:y,:z])iex> Nx.argmax(t,axis::y)#Nx.Tensor<s32[x:2][z:3][[0,0,0],[0,1,0]]>iex> t=Nx.tensor([[[4,2,3],[1,-5,3]],[[6,2,3],[4,8,3]]],names:[:x,:y,:z])iex> Nx.argmax(t,axis::z)#Nx.Tensor<s32[x:2][y:2][[0,2],[0,1]]>Tie breaksiex> t=Nx.tensor([[[4,2,3],[1,-5,3]],[[6,2,3],[4,8,3]]],names:[:x,:y,:z])iex> Nx.argmax(t,tie_break::low,axis::y)#Nx.Tensor<s32[x:2][z:3][[0,0,0],[0,1,0]]>iex> t=Nx.tensor([[[4,2,3],[1,-5,3]],[[6,2,3],[4,8,3]]],names:[:x,:y,:z])iex> Nx.argmax(t,tie_break::high,axis::y,type::u32)#Nx.Tensor<u32[x:2][z:3][[0,0,1],[0,1,1]]>Keep axisiex> t=Nx.tensor([[[4,2,3],[1,-5,3]],[[6,2,3],[4,8,3]]],names:[:x,:y,:z])iex> Nx.argmax(t,axis::y,keep_axis:true)#Nx.Tensor<s32[x:2][y:1][z:3][[[0,0,0]],[[0,1,0]]]>Vectorized tensorsiex> v=Nx.tensor([[1,2,3],[6,5,4]])|>Nx.vectorize(:x)iex> Nx.argmax(v)#Nx.Tensor<vectorized[x:2]s32[2,0]>iex> Nx.argmax(v,axis:0)#Nx.Tensor<vectorized[x:2]s32[2,0]>iex> Nx.argmax(v,keep_axis:true)#Nx.Tensor<vectorized[x:2]s32[1][[2],[0]]>Link to this functionargmin(tensor, opts \\ [])View SourceReturns the indices of the minimum values.Options:axis - the axis to aggregate on. If no axis is given,
returns the index of the absolute minimum value in the tensor.:keep_axis - whether or not to keep the reduced axis with
a size of 1. Defaults to false.:tie_break - how to break ties. one of :high, or :low.
Default behavior is to always return the lower index.:type - The type of the resulting tensor. Defaults to :s32.Examplesiex> Nx.argmin(4)#Nx.Tensor<s320>iex> t=Nx.tensor([[[4,2,3],[1,-5,3]],[[6,2,3],[4,8,3]]])iex> Nx.argmin(t)#Nx.Tensor<s324>If a tensor of floats is given, it still returns integers:iex> Nx.argmin(Nx.tensor([2.0,4.0]))#Nx.Tensor<s320>If the tensor includes any NaNs, returns the index of any of them
(NaNs are not equal, hence tie-break does not apply):iex> Nx.argmin(Nx.tensor([2.0,:nan,4.0]))#Nx.Tensor<s321>Aggregating over an axisiex> t=Nx.tensor([[[4,2,3],[1,-5,3]],[[6,2,3],[4,8,3]]])iex> Nx.argmin(t,axis:0)#Nx.Tensor<s32[2][3][[0,0,0],[0,0,0]]>iex> t=Nx.tensor([[[4,2,3],[1,-5,3]],[[6,2,3],[4,8,3]]],names:[:x,:y,:z])iex> Nx.argmin(t,axis:1)#Nx.Tensor<s32[x:2][z:3][[1,1,0],[1,0,0]]>iex> t=Nx.tensor([[[4,2,3],[1,-5,3]],[[6,2,3],[4,8,3]]],names:[:x,:y,:z])iex> Nx.argmin(t,axis::z)#Nx.Tensor<s32[x:2][y:2][[1,1],[1,2]]>Tie breaksiex> t=Nx.tensor([[[4,2,3],[1,-5,3]],[[6,2,3],[4,8,3]]],names:[:x,:y,:z])iex> Nx.argmin(t,tie_break::low,axis::y)#Nx.Tensor<s32[x:2][z:3][[1,1,0],[1,0,0]]>iex> t=Nx.tensor([[[4,2,3],[1,-5,3]],[[6,2,3],[4,8,3]]],names:[:x,:y,:z])iex> Nx.argmin(t,tie_break::high,axis::y,type::u32)#Nx.Tensor<u32[x:2][z:3][[1,1,1],[1,0,1]]>Keep axisiex> t=Nx.tensor([[[4,2,3],[1,-5,3]],[[6,2,3],[4,8,3]]],names:[:x,:y,:z])iex> Nx.argmin(t,axis::y,keep_axis:true)#Nx.Tensor<s32[x:2][y:1][z:3][[[1,1,0]],[[1,0,0]]]>Vectorized tensorsiex> v=Nx.tensor([[1,2,3],[6,5,4]])|>Nx.vectorize(:x)iex> Nx.argmin(v)#Nx.Tensor<vectorized[x:2]s32[0,2]>iex> Nx.argmin(v,axis:0)#Nx.Tensor<vectorized[x:2]s32[0,2]>iex> Nx.argmin(v,keep_axis:true)#Nx.Tensor<vectorized[x:2]s32[1][[0],[2]]>Link to this functioncovariance(tensor, opts \\ [])View Source@spec covariance(tensor :: Nx.Tensor.t(), opts :: Keyword.t()) :: Nx.Tensor.t()@spec covariance(tensor :: Nx.Tensor.t(), opts :: Keyword.t()) :: Nx.Tensor.t()@spec covariance(tensor :: Nx.Tensor.t(), mean :: Nx.Tensor.t()) :: Nx.Tensor.t()A shortcut to covariance/3 with either opts or mean as second argument.Link to this functioncovariance(tensor, mean, opts)View Source@spec covariance(tensor :: Nx.Tensor.t(), mean :: Nx.Tensor.t(), opts :: Keyword.t()) ::
  Nx.Tensor.t()Computes the covariance matrix of the input tensor.The covariance of two random variables X and Y is calculated as $Cov(X, Y) = \frac{1}{N}\sum_{i=0}^{N-1}{(X_i - \overline{X}) * (Y_i - \overline{Y})}$.The tensor must be at least of rank 2, with shape {n, d}. Any additional
dimension will be treated as batch dimensions.The column mean can be provided as the second argument and it must be
a tensor of shape {..., d}, where the batch shape is broadcastable with
that of the input tensor. If not provided, the mean is estimated using Nx.mean/2.If the :ddof (delta degrees of freedom) option is given, the divisor n - ddof
is used for the sum of the products.Examplesiex> Nx.covariance(Nx.tensor([[1,2],[3,4],[5,6]]))#Nx.Tensor<f32[2][2][[2.6666667461395264,2.6666667461395264],[2.6666667461395264,2.6666667461395264]]>iex> Nx.covariance(Nx.tensor([[[1,2],[3,4],[5,6]],[[7,8],[9,10],[11,12]]]))#Nx.Tensor<f32[2][2][2][[[2.6666667461395264,2.6666667461395264],[2.6666667461395264,2.6666667461395264]],[[2.6666667461395264,2.6666667461395264],[2.6666667461395264,2.6666667461395264]]]>iex> Nx.covariance(Nx.tensor([[1,2],[3,4],[5,6]]),ddof:1)#Nx.Tensor<f32[2][2][[4.0,4.0],[4.0,4.0]]>iex> Nx.covariance(Nx.tensor([[1,2],[3,4],[5,6]]),Nx.tensor([4,3]))#Nx.Tensor<f32[2][2][[3.6666667461395264,1.6666666269302368],[1.6666666269302368,3.6666667461395264]]>Link to this functionlogsumexp(tensor, opts \\ [])View SourceReturns the logarithm of the sum of the exponentials of tensor elements.If the :axes option is given, it aggregates over
the given dimensions, effectively removing them.
axes: [0] implies aggregating over the highest order
dimension and so forth. If the axis is negative, then
counts the axis from the back. For example, axes: [-1]
will always aggregate all rows.You may optionally set :keep_axes to true, which will
retain the rank of the input tensor by setting the reduced
axes to size 1.Exponentials can be scaled before summation by multiplying
them with :exp_scaling_factor option. It must be of the same shape
as the input tensor or broadcastable to it.Examplesiex> Nx.logsumexp(Nx.tensor([1,2,3,4,5,6]))#Nx.Tensor<f326.456193447113037>iex> Nx.logsumexp(Nx.tensor([1,2,3,4,5,6]),exp_scaling_factor:0.5)#Nx.Tensor<f325.7630462646484375>iex> t=Nx.tensor([1,2,3,4,5,6])iex> a=Nx.tensor([-1,-1,-1,1,1,1])iex> Nx.logsumexp(t,exp_scaling_factor:a)#Nx.Tensor<f326.356536865234375>iex> Nx.logsumexp(Nx.tensor([[1,2],[3,4],[5,6]]))#Nx.Tensor<f326.456193447113037>Aggregating over an axisiex> t=Nx.tensor([[1,2],[3,4],[5,6]],names:[:x,:y])iex> Nx.logsumexp(t,axes:[:x])#Nx.Tensor<f32[y:2][5.1429314613342285,6.1429314613342285]>iex> t=Nx.tensor([[1,2],[3,4],[5,6]],names:[:x,:y])iex> Nx.logsumexp(t,axes:[:y])#Nx.Tensor<f32[x:3][2.3132617473602295,4.31326150894165,6.31326150894165]>iex> t=Nx.tensor([[[1,2],[3,4]],[[5,6],[7,8]]],names:[:x,:y,:z])iex> Nx.logsumexp(t,axes:[:x,:z])#Nx.Tensor<f32[y:2][6.331411361694336,8.331411361694336]>Keeping axesiex> t=Nx.tensor([[[1,2],[3,4]],[[5,6],[7,8]]],names:[:x,:y,:z])iex> Nx.logsumexp(t,axes:[:x,:z],keep_axes:true)#Nx.Tensor<f32[x:1][y:2][z:1][[[6.331411361694336],[8.331411361694336]]]>Vectorized tensorsiex> t=Nx.vectorize(Nx.tensor([[1,2],[3,4],[5,6]]),:x)iex> Nx.logsumexp(t,axes:[0],keep_axes:true)#Nx.Tensor<vectorized[x:3]f32[1][[2.3132617473602295],[4.31326150894165],[6.31326150894165]]>Link to this functionmean(tensor, opts \\ [])View SourceReturns the mean for the tensor.If the :axes option is given, it aggregates over
that dimension, effectively removing it. axes: [0]
implies aggregating over the highest order dimension
and so forth. If the axis is negative, then counts
the axis from the back. For example, axes: [-1] will
always aggregate all rows.You may optionally set :keep_axes to true, which will
retain the rank of the input tensor by setting the averaged
axes to size 1.Examplesiex> Nx.mean(Nx.tensor(42))#Nx.Tensor<f3242.0>iex> Nx.mean(Nx.tensor([1,2,3]))#Nx.Tensor<f322.0>Aggregating over an axisiex> Nx.mean(Nx.tensor([1,2,3]),axes:[0])#Nx.Tensor<f322.0>iex> Nx.mean(Nx.tensor([1,2,3],type::u8,names:[:x]),axes:[:x])#Nx.Tensor<f322.0>iex> t=Nx.tensor(Nx.iota({2,2,3}),names:[:x,:y,:z])iex> Nx.mean(t,axes:[:x])#Nx.Tensor<f32[y:2][z:3][[3.0,4.0,5.0],[6.0,7.0,8.0]]>iex> t=Nx.tensor(Nx.iota({2,2,3}),names:[:x,:y,:z])iex> Nx.mean(t,axes:[:x,:z])#Nx.Tensor<f32[y:2][4.0,7.0]>iex> t=Nx.tensor(Nx.iota({2,2,3}),names:[:x,:y,:z])iex> Nx.mean(t,axes:[-1])#Nx.Tensor<f32[x:2][y:2][[1.0,4.0],[7.0,10.0]]>Keeping axesiex> t=Nx.tensor(Nx.iota({2,2,3}),names:[:x,:y,:z])iex> Nx.mean(t,axes:[-1],keep_axes:true)#Nx.Tensor<f32[x:2][y:2][z:1][[[1.0],[4.0]],[[7.0],[10.0]]]>Vectorized tensorsiex> t=Nx.iota({2,5},vectorized_axes:[x:2])iex> Nx.mean(t)#Nx.Tensor<vectorized[x:2]f32[4.5,4.5]>iex> Nx.mean(t,axes:[0])#Nx.Tensor<vectorized[x:2]f32[5][[2.5,3.5,4.5,5.5,6.5],[2.5,3.5,4.5,5.5,6.5]]>iex> Nx.mean(t,axes:[1])#Nx.Tensor<vectorized[x:2]f32[2][[2.0,7.0],[2.0,7.0]]>Link to this functionmedian(tensor, opts \\ [])View SourceReturns the median for the tensor.The median is the value in the middle of a data set.If the :axis option is given, it aggregates over
that dimension, effectively removing it. axis: 0
implies aggregating over the highest order dimension
and so forth. If the axis is negative, then the axis will
be counted from the back. For example, axis: -1 will
always aggregate over the last dimension.You may optionally set :keep_axis to true, which will
retain the rank of the input tensor by setting the reduced
axis to size 1.Examplesiex> Nx.median(Nx.tensor(42))#Nx.Tensor<s3242>iex> Nx.median(Nx.tensor([1,2,3]))#Nx.Tensor<s322>iex> Nx.median(Nx.tensor([1,2]))#Nx.Tensor<f321.5>iex> Nx.median(Nx.iota({2,3,3}))#Nx.Tensor<f328.5>Aggregating over an axisiex> Nx.median(Nx.tensor([[1,2,3],[4,5,6]],names:[:x,:y]),axis:0)#Nx.Tensor<f32[y:3][2.5,3.5,4.5]>iex> Nx.median(Nx.tensor([[1,2,3],[4,5,6]],names:[:x,:y]),axis::y)#Nx.Tensor<s32[x:2][2,5]>iex> t=Nx.tensor(Nx.iota({2,2,3}),names:[:x,:y,:z])iex> Nx.median(t,axis::x)#Nx.Tensor<f32[y:2][z:3][[3.0,4.0,5.0],[6.0,7.0,8.0]]>iex> t=Nx.tensor([[[1,2,2],[3,4,2]],[[4,5,2],[7,9,2]]])iex> Nx.median(t,axis:-1)#Nx.Tensor<s32[2][2][[2,3],[4,7]]>Keeping axisiex> t=Nx.tensor([[[1,2,2],[3,4,2]],[[4,5,2],[7,9,2]]])iex> Nx.median(t,axis:-1,keep_axis:true)#Nx.Tensor<s32[2][2][1][[[2],[3]],[[4],[7]]]>Vectorized tensorsFor vectorized inputs, :axis refers to the
non-vectorized shape:iex> Nx.median(Nx.tensor([[1,2,3],[4,5,6]])|>Nx.vectorize(:x),axis:0)#Nx.Tensor<vectorized[x:2]s32[2,5]>Link to this functionmode(tensor, opts \\ [])View SourceReturns the mode of a tensor.The mode is the value that appears most often.If the :axis option is given, it aggregates over
that dimension, effectively removing it. axis: 0
implies aggregating over the highest order dimension
and so forth. If the axis is negative, then the axis will
be counted from the back. For example, axis: -1 will
always aggregate over the last dimension.You may optionally set :keep_axis to true, which will
retain the rank of the input tensor by setting the reduced
axis to size 1.Examplesiex> Nx.mode(Nx.tensor(42))#Nx.Tensor<s3242>iex> Nx.mode(Nx.tensor([[1]]))#Nx.Tensor<s321>iex> Nx.mode(Nx.tensor([1,2,2,3,5]))#Nx.Tensor<s322>iex> Nx.mode(Nx.tensor([[1,2,2,3,5],[1,1,76,8,1]]))#Nx.Tensor<s321>Aggregating over an axisiex> Nx.mode(Nx.tensor([[1,2,2,3,5],[1,1,76,8,1]]),axis:0)#Nx.Tensor<s32[5][1,1,2,3,1]>iex> Nx.mode(Nx.tensor([[1,2,2,3,5],[1,1,76,8,1]]),axis:1)#Nx.Tensor<s32[2][2,1]>iex> Nx.mode(Nx.tensor([[[1]]]),axis:1)#Nx.Tensor<s32[1][1][[1]]>Keeping axisiex> Nx.mode(Nx.tensor([[1,2,2,3,5],[1,1,76,8,1]]),axis:1,keep_axis:true)#Nx.Tensor<s32[2][1][[2],[1]]>Vectorized tensorsFor vectorized tensors, :axis refers to the non-vectorized shape:iex> t=Nx.tensor([[[1,2,2,3,5],[1,1,76,8,1]],[[1,2,2,2,5],[5,2,2,2,1]]])|>Nx.vectorize(:x)iex> Nx.mode(t,axis:0)#Nx.Tensor<vectorized[x:2]s32[5][[1,1,2,3,1],[1,2,2,2,1]]>iex> Nx.mode(t,axis:1)#Nx.Tensor<vectorized[x:2]s32[2][[2,1],[2,2]]>Link to this functionproduct(tensor, opts \\ [])View SourceReturns the product for the tensor.If the :axes option is given, it aggregates over
the given dimensions, effectively removing them.
axes: [0] implies aggregating over the highest order
dimension and so forth. If the axis is negative, then
counts the axis from the back. For example, axes: [-1]
will always aggregate all rows.You may optionally set :keep_axes to true, which will
retain the rank of the input tensor by setting the multiplied
axes to size 1.ExamplesBy default the product always returns a scalar:iex> Nx.product(Nx.tensor(42))#Nx.Tensor<s3242>iex> Nx.product(Nx.tensor([1,2,3]))#Nx.Tensor<s326>iex> Nx.product(Nx.tensor([[1.0,2.0],[3.0,4.0]]))#Nx.Tensor<f3224.0>Giving a tensor with low precision casts it to a higher
precision to make sure the sum does not overflow:iex> Nx.product(Nx.tensor([[10,20],[30,40]],type::u8,names:[:x,:y]))#Nx.Tensor<u32240000>iex> Nx.product(Nx.tensor([[10,20],[30,40]],type::s8,names:[:x,:y]))#Nx.Tensor<s32240000>Aggregating over an axisiex> Nx.product(Nx.tensor([1,2,3]),axes:[0])#Nx.Tensor<s326>Same tensor over different axes combinations:iex> t=Nx.iota({2,2,3},names:[:x,:y,:z])iex> Nx.product(t,axes:[:x])#Nx.Tensor<s32[y:2][z:3][[0,7,16],[27,40,55]]>iex> Nx.product(t,axes:[:y])#Nx.Tensor<s32[x:2][z:3][[0,4,10],[54,70,88]]>iex> Nx.product(t,axes:[:x,:z])#Nx.Tensor<s32[y:2][0,59400]>iex> Nx.product(t,axes:[:z])#Nx.Tensor<s32[x:2][y:2][[0,60],[336,990]]>iex> Nx.product(t,axes:[-3])#Nx.Tensor<s32[y:2][z:3][[0,7,16],[27,40,55]]>Keeping axesiex> t=Nx.iota({2,2,3},names:[:x,:y,:z])iex> Nx.product(t,axes:[:z],keep_axes:true)#Nx.Tensor<s32[x:2][y:2][z:1][[[0],[60]],[[336],[990]]]>Vectorized tensorsiex> t=Nx.vectorize(Nx.tensor([[1,2],[3,4]]),:x)iex> Nx.product(t,axes:[0],keep_axes:true)#Nx.Tensor<vectorized[x:2]s32[1][[2],[12]]>Errorsiex> Nx.product(Nx.tensor([[1,2]]),axes:[2])** (ArgumentError) given axis (2) invalid for shape with rank 2Link to this functionreduce(tensor, acc, opts \\ [], fun)View SourceReduces over a tensor with the given accumulator.The given fun will receive two tensors and it must
return the reduced value.The tensor may be reduced in parallel and the reducer
function can be called with arguments in any order, the
initial accumulator may be given multiples, and it may
be non-deterministic. Therefore, the reduction function
should be associative (or as close as possible to
associativity considered floats themselves are not
strictly associative).By default, it reduces all dimensions of the tensor and
return a scalar. If the :axes option is given, it
aggregates over multiple dimensions, effectively removing
them. axes: [0] implies aggregating over the highest
order dimension and so forth. If the axis is negative,
then counts the axis from the back. For example,
axes: [-1] will always aggregate all rows.The type of the returned tensor will be computed based on
the given tensor and the initial value. For example,
a tensor of integers with a float accumulator will be
cast to float, as done by most binary operators. You can
also pass a :type option to change this behaviour.You may optionally set :keep_axes to true, which will
retain the rank of the input tensor by setting the reduced
axes to size 1.LimitationsGiven this function relies on anonymous functions, it
may not be available or efficient on all Nx backends.
Therefore, you should avoid using reduce/4 whenever
possible. Instead, use functions sum/2, reduce_max/2,
all/1, and so forth.Inside defn, consider using Nx.Defn.Kernel.while/4 instead.Examplesiex> Nx.reduce(Nx.tensor(42),0,fnx,y->Nx.add(x,y)end)#Nx.Tensor<s3242>iex> Nx.reduce(Nx.tensor([1,2,3]),0,fnx,y->Nx.add(x,y)end)#Nx.Tensor<s326>iex> Nx.reduce(Nx.tensor([[1.0,2.0],[3.0,4.0]]),0,fnx,y->Nx.add(x,y)end)#Nx.Tensor<f3210.0>Aggregating over axesiex> t=Nx.tensor([1,2,3],names:[:x])iex> Nx.reduce(t,0,[axes:[:x]],fnx,y->Nx.add(x,y)end)#Nx.Tensor<s326>iex> t=Nx.tensor([[[1,2,3],[4,5,6]],[[7,8,9],[10,11,12]]],names:[:x,:y,:z])iex> Nx.reduce(t,0,[axes:[:x]],fnx,y->Nx.add(x,y)end)#Nx.Tensor<s32[y:2][z:3][[8,10,12],[14,16,18]]>iex> t=Nx.tensor([[[1,2,3],[4,5,6]],[[7,8,9],[10,11,12]]],names:[:x,:y,:z])iex> Nx.reduce(t,0,[axes:[:y]],fnx,y->Nx.add(x,y)end)#Nx.Tensor<s32[x:2][z:3][[5,7,9],[17,19,21]]>iex> t=Nx.tensor([[[1,2,3],[4,5,6]],[[7,8,9],[10,11,12]]],names:[:x,:y,:z])iex> Nx.reduce(t,0,[axes:[:x,2]],fnx,y->Nx.add(x,y)end)#Nx.Tensor<s32[y:2][30,48]>iex> t=Nx.tensor([[[1,2,3],[4,5,6]],[[7,8,9],[10,11,12]]],names:[:x,:y,:z])iex> Nx.reduce(t,0,[axes:[-1]],fnx,y->Nx.add(x,y)end)#Nx.Tensor<s32[x:2][y:2][[6,15],[24,33]]>iex> t=Nx.tensor([[[1,2,3],[4,5,6]],[[7,8,9],[10,11,12]]],names:[:x,:y,:z])iex> Nx.reduce(t,0,[axes:[:x]],fnx,y->Nx.add(x,y)end)#Nx.Tensor<s32[y:2][z:3][[8,10,12],[14,16,18]]>iex> t=Nx.tensor([[[1,2,3],[4,5,6]],[[7,8,9],[10,11,12]]],names:[:x,:y,:z])iex> Nx.reduce(t,0,[axes:[:x],keep_axes:true],fnx,y->Nx.add(x,y)end)#Nx.Tensor<s32[x:1][y:2][z:3][[[8,10,12],[14,16,18]]]>Vectorized tensorsOnly tensor can be vectorized. Normal behavior of reduce/4
is applied to each corresponding entry. :axes refers to the
non-vectorized shape.iex> t=Nx.tensor([[[1,2,3],[4,5,6]],[[10,20,30],[40,50,60]]])|>Nx.vectorize(:x)iex> Nx.reduce(t,10,[axes:[1]],&Nx.add/2)#Nx.Tensor<vectorized[x:2]s32[2][[16,25],[70,160]]>Link to this functionreduce_max(tensor, opts \\ [])View SourceReturns the maximum values of the tensor.If the :axes option is given, it aggregates over
the given dimensions, effectively removing them.
axes: [0] implies aggregating over the highest order
dimension and so forth. If the axis is negative, then
counts the axis from the back. For example, axes: [-1]
will always aggregate all rows.You may optionally set :keep_axes to true, which will
retain the rank of the input tensor by setting the reduced
axes to size 1.Examplesiex> Nx.reduce_max(Nx.tensor(42))#Nx.Tensor<s3242>iex> Nx.reduce_max(Nx.tensor(42.0))#Nx.Tensor<f3242.0>iex> Nx.reduce_max(Nx.tensor([1,2,3]))#Nx.Tensor<s323>Aggregating over an axisiex> t=Nx.tensor([[3,1,4],[2,1,1]],names:[:x,:y])iex> Nx.reduce_max(t,axes:[:x])#Nx.Tensor<s32[y:3][3,1,4]>iex> t=Nx.tensor([[3,1,4],[2,1,1]],names:[:x,:y])iex> Nx.reduce_max(t,axes:[:y])#Nx.Tensor<s32[x:2][4,2]>iex> t=Nx.tensor([[[1,2],[4,5]],[[2,4],[3,8]]],names:[:x,:y,:z])iex> Nx.reduce_max(t,axes:[:x,:z])#Nx.Tensor<s32[y:2][4,8]>Keeping axesiex> t=Nx.tensor([[[1,2],[4,5]],[[2,4],[3,8]]],names:[:x,:y,:z])iex> Nx.reduce_max(t,axes:[:x,:z],keep_axes:true)#Nx.Tensor<s32[x:1][y:2][z:1][[[4],[8]]]>Vectorized tensorsiex> t=Nx.vectorize(Nx.tensor([[1,2],[3,4]]),:x)iex> Nx.reduce_max(t,axes:[0],keep_axes:true)#Nx.Tensor<vectorized[x:2]s32[1][[2],[4]]>Link to this functionreduce_min(tensor, opts \\ [])View SourceReturns the minimum values of the tensor.If the :axes option is given, it aggregates over
the given dimensions, effectively removing them.
axes: [0] implies aggregating over the highest order
dimension and so forth. If the axis is negative, then
counts the axis from the back. For example, axes: [-1]
will always aggregate all rows.You may optionally set :keep_axes to true, which will
retain the rank of the input tensor by setting the reduced
axes to size 1.Examplesiex> Nx.reduce_min(Nx.tensor(42))#Nx.Tensor<s3242>iex> Nx.reduce_min(Nx.tensor(42.0))#Nx.Tensor<f3242.0>iex> Nx.reduce_min(Nx.tensor([1,2,3]))#Nx.Tensor<s321>Aggregating over an axisiex> t=Nx.tensor([[3,1,4],[2,1,1]],names:[:x,:y])iex> Nx.reduce_min(t,axes:[:x])#Nx.Tensor<s32[y:3][2,1,1]>iex> t=Nx.tensor([[3,1,4],[2,1,1]],names:[:x,:y])iex> Nx.reduce_min(t,axes:[:y])#Nx.Tensor<s32[x:2][1,1]>iex> t=Nx.tensor([[[1,2],[4,5]],[[2,4],[3,8]]],names:[:x,:y,:z])iex> Nx.reduce_min(t,axes:[:x,:z])#Nx.Tensor<s32[y:2][1,3]>Keeping axesiex> t=Nx.tensor([[[1,2],[4,5]],[[2,4],[3,8]]],names:[:x,:y,:z])iex> Nx.reduce_min(t,axes:[:x,:z],keep_axes:true)#Nx.Tensor<s32[x:1][y:2][z:1][[[1],[3]]]>Vectorized tensorsiex> t=Nx.vectorize(Nx.tensor([[1,2],[3,4]]),:x)iex> Nx.reduce_min(t,axes:[0],keep_axes:true)#Nx.Tensor<vectorized[x:2]s32[1][[1],[3]]>Link to this functionstandard_deviation(tensor, opts \\ [])View Source@spec standard_deviation(tensor :: Nx.Tensor.t(), opts :: Keyword.t()) ::
  Nx.Tensor.t()Finds the standard deviation of a tensor.The standard deviation is taken as the square root of the variance.
If the :ddof (delta degrees of freedom) option is given, the divisor
n - ddof is used to calculate the variance. See variance/2.Examplesiex> Nx.standard_deviation(Nx.tensor([[1,2],[3,4]]))#Nx.Tensor<f321.1180340051651>iex> Nx.standard_deviation(Nx.tensor([[1,2],[3,4]]),ddof:1)#Nx.Tensor<f321.29099440574646>iex> Nx.standard_deviation(Nx.tensor([[1,2],[10,20]]),axes:[0])#Nx.Tensor<f32[2][4.5,9.0]>iex> Nx.standard_deviation(Nx.tensor([[1,2],[10,20]]),axes:[1])#Nx.Tensor<f32[2][0.5,5.0]>iex> Nx.standard_deviation(Nx.tensor([[1,2],[10,20]]),axes:[0],ddof:1)#Nx.Tensor<f32[2][6.363961219787598,12.727922439575195]>iex> Nx.standard_deviation(Nx.tensor([[1,2],[10,20]]),axes:[1],ddof:1)#Nx.Tensor<f32[2][0.7071067690849304,7.071067810058594]>Keeping axesiex> Nx.standard_deviation(Nx.tensor([[1,2],[10,20]]),keep_axes:true)#Nx.Tensor<f32[1][1][[7.628073215484619]]>Vectorized tensorsiex> Nx.standard_deviation(Nx.tensor([[1,2],[0,4]])|>Nx.vectorize(:x))#Nx.Tensor<vectorized[x:2]f32[0.5,2.0]>Link to this functionsum(tensor, opts \\ [])View SourceReturns the sum for the tensor.If the :axes option is given, it aggregates over
the given dimensions, effectively removing them.
axes: [0] implies aggregating over the highest order
dimension and so forth. If the axis is negative, then
counts the axis from the back. For example, axes: [-1]
will always aggregate all rows.You may optionally set :keep_axes to true, which will
retain the rank of the input tensor by setting the summed
axes to size 1.ExamplesBy default the sum always returns a scalar:iex> Nx.sum(Nx.tensor(42))#Nx.Tensor<s3242>iex> Nx.sum(Nx.tensor([1,2,3]))#Nx.Tensor<s326>iex> Nx.sum(Nx.tensor([[1.0,2.0],[3.0,4.0]]))#Nx.Tensor<f3210.0>Giving a tensor with low precision casts it to a higher
precision to make sure the sum does not overflow:iex> Nx.sum(Nx.tensor([[101,102],[103,104]],type::s8))#Nx.Tensor<s32410>iex> Nx.sum(Nx.tensor([[101,102],[103,104]],type::s16))#Nx.Tensor<s32410>Aggregating over an axisiex> Nx.sum(Nx.tensor([1,2,3]),axes:[0])#Nx.Tensor<s326>Same tensor over different axes combinations:iex> t=Nx.iota({2,2,3},names:[:x,:y,:z])iex> Nx.sum(t,axes:[:x])#Nx.Tensor<s32[y:2][z:3][[6,8,10],[12,14,16]]>iex> Nx.sum(t,axes:[:y])#Nx.Tensor<s32[x:2][z:3][[3,5,7],[15,17,19]]>iex> Nx.sum(t,axes:[:z])#Nx.Tensor<s32[x:2][y:2][[3,12],[21,30]]>iex> Nx.sum(t,axes:[:x,:z])#Nx.Tensor<s32[y:2][24,42]>iex> Nx.sum(t,axes:[-3])#Nx.Tensor<s32[y:2][z:3][[6,8,10],[12,14,16]]>Keeping axesiex> t=Nx.tensor([[1,2],[3,4]],names:[:x,:y])iex> Nx.sum(t,axes:[:x],keep_axes:true)#Nx.Tensor<s32[x:1][y:2][[4,6]]>Vectorized tensorsiex> t=Nx.tensor([[[[1,2]],[[3,4]]],[[[5,6]],[[7,8]]]])|>Nx.vectorize(:x)|>Nx.vectorize(:y)#Nx.Tensor<vectorized[x:2][y:2]s32[1][2][[[[1,2]],[[3,4]]],[[[5,6]],[[7,8]]]]>iex> Nx.sum(t)#Nx.Tensor<vectorized[x:2][y:2]s32[[3,7],[11,15]]>iex> Nx.sum(t,axes:[0])#Nx.Tensor<vectorized[x:2][y:2]s32[2][[[1,2],[3,4]],[[5,6],[7,8]]]>Errorsiex> Nx.sum(Nx.tensor([[1,2]]),axes:[2])** (ArgumentError) given axis (2) invalid for shape with rank 2Link to this functionvariance(tensor, opts \\ [])View Source@spec variance(tensor :: Nx.Tensor.t(), opts :: Keyword.t()) :: Nx.Tensor.t()Finds the variance of a tensor.The variance is the average of the squared deviations from the mean.
The mean is typically calculated as sum(tensor) / n, where n is the total
of elements. If, however, :ddof (delta degrees of freedom) is specified, the
divisor n - ddof is used instead.Examplesiex> Nx.variance(Nx.tensor([[1,2],[3,4]]))#Nx.Tensor<f321.25>iex> Nx.variance(Nx.tensor([[1,2],[3,4]]),ddof:1)#Nx.Tensor<f321.6666666269302368>iex> Nx.variance(Nx.tensor([[1,2],[3,4]]),axes:[0])#Nx.Tensor<f32[2][1.0,1.0]>iex> Nx.variance(Nx.tensor([[1,2],[3,4]]),axes:[1])#Nx.Tensor<f32[2][0.25,0.25]>iex> Nx.variance(Nx.tensor([[1,2],[3,4]]),axes:[0],ddof:1)#Nx.Tensor<f32[2][2.0,2.0]>iex> Nx.variance(Nx.tensor([[1,2],[3,4]]),axes:[1],ddof:1)#Nx.Tensor<f32[2][0.5,0.5]>Keeping axesiex> Nx.variance(Nx.tensor([[1,2],[3,4]]),axes:[1],keep_axes:true)#Nx.Tensor<f32[2][1][[0.25],[0.25]]>Vectorized tensorsiex> Nx.variance(Nx.tensor([[1,2],[0,4]])|>Nx.vectorize(:x))#Nx.Tensor<vectorized[x:2]f32[0.25,4.0]>Link to this functionweighted_mean(tensor, weights, opts \\ [])View SourceReturns the weighted mean for the tensor and the weights.If the :axes option is given, it aggregates over
those dimensions, effectively removing them. axes: [0]
implies aggregating over the highest order dimension
and so forth. If the axes are negative, then the axes will
be counted from the back. For example, axes: [-1] will
always aggregate over the last dimension.You may optionally set :keep_axes to true, which will
retain the rank of the input tensor by setting the averaged
axes to size 1.Examplesiex> Nx.weighted_mean(Nx.tensor(42),Nx.tensor(2))#Nx.Tensor<f3242.0>iex> Nx.weighted_mean(Nx.tensor([1,2,3]),Nx.tensor([3,2,1]))#Nx.Tensor<f321.6666666269302368>Aggregating over axesiex> Nx.weighted_mean(Nx.tensor([1,2,3],names:[:x]),Nx.tensor([4,5,6]),axes:[0])#Nx.Tensor<f322.133333444595337>iex> Nx.weighted_mean(Nx.tensor([1,2,3],type::u8,names:[:x]),Nx.tensor([1,3,5]),axes:[:x])#Nx.Tensor<f322.444444417953491>iex> t=Nx.iota({3,4})iex> weights=Nx.tensor([1,2,3,4])iex> Nx.weighted_mean(t,weights,axes:[1])#Nx.Tensor<f32[3][2.0,6.0,10.0]>iex> t=Nx.iota({2,4,4,1})iex> weights=Nx.broadcast(2,{4,4})iex> Nx.weighted_mean(t,weights,axes:[1,2])#Nx.Tensor<f32[2][1][[7.5],[23.5]]>Keeping axesiex> t=Nx.tensor(Nx.iota({2,2,3}),names:[:x,:y,:z])iex> weights=Nx.tensor([[[0,1,2],[1,1,0]],[[-1,1,-1],[1,1,-1]]])iex> Nx.weighted_mean(t,weights,axes:[-1],keep_axes:true)#Nx.Tensor<f32[x:2][y:2][z:1][[[1.6666666269302368],[3.5]],[[7.0],[8.0]]]>Vectorized tensorsiex> t=Nx.tensor([[1,2,3],[1,1,1]])|>Nx.vectorize(:x)#Nx.Tensor<vectorized[x:2]s32[3][[1,2,3],[1,1,1]]>iex> w=Nx.tensor([[1,1,1],[0,0,1]])|>Nx.vectorize(:y)#Nx.Tensor<vectorized[y:2]s32[3][[1,1,1],[0,0,1]]>iex> Nx.weighted_mean(t,w)#Nx.Tensor<vectorized[x:2][y:2]f32[[2.0,3.0],[1.0,1.0]]>Functions: BackendLink to this functionbackend_copy(tensor_or_container, backend \\ Nx.BinaryBackend)View SourceCopies data to the given backend.If a backend is not given, Nx.Tensor is used, which means
the given tensor backend will pick the most appropriate
backend to copy the data to.Note this function keeps the data in the original backend.
Therefore, use this function with care, as it may duplicate
large amounts of data across backends. Generally speaking,
you may want to use backend_transfer/2, unless you explicitly
want to copy the data.Note:Nx.default_backend/1 does not affect the behaviour of
this function.This function cannot be used in defn.Examples  iex> Nx.backend_copy(Nx.tensor([[1, 2, 3], [4, 5, 6]]))
  #Nx.Tensor<s32[2][3][[1,2,3],[4,5,6]]Link to this functionbackend_deallocate(tensor_or_container)View SourceDeallocates data in a device.It returns either :ok or :already_deallocated.Note: This function cannot be used in defn.Link to this functionbackend_transfer(tensor_or_container, backend \\ Nx.BinaryBackend)View SourceTransfers data to the given backend.This operation can be seen as an equivalent to backend_copy/3
followed by a backend_deallocate/1 on the initial tensor:new_tensor=Nx.backend_copy(old_tensor,new_backend)Nx.backend_deallocate(old_tensor)If a backend is not given, Nx.Tensor is used, which means
the given tensor backend will pick the most appropriate
backend to transfer to.For Elixir's builtin tensor, transferring to another backend
will call new_backend.from_binary(tensor, binary, opts).
Transferring from a mutable backend, such as GPU memory,
implies the data is copied from the GPU to the Erlang VM
and then deallocated from the device.Note:Nx.default_backend/1 does not affect the behaviour of this function.This function cannot be used in defn.ExamplesTransfer a tensor to an EXLA device backend, stored in the GPU:device_tensor=Nx.backend_transfer(tensor,{EXLA.Backend,client::cuda})Transfer the device tensor back to an Elixir tensor:tensor=Nx.backend_transfer(device_tensor)Link to this functiondefault_backend()View SourceGets the default backend for the current process.Note: This function cannot be used in defn.Link to this functiondefault_backend(backend)View SourceSets the given backend as default in the current process.The default backend is stored only in the process dictionary.
This means if you start a separate process, such as Task,
the default backend must be set on the new process too.Due to this reason, this function is mostly used for scripting
and testing. In your applications, you must prefer to set the
backend in your config files:config:nx,:default_backend,{EXLA.Backend,device::cuda}In your notebooks and on Mix.install/2, you might:Mix.install([{:nx,">= 0.0.0"}],config:[nx:[default_backend:{EXLA.Backend,device::cuda}]])Or use Nx.global_default_backend/1 as it changes the
default backend on all processes.The function returns the value that was previously set as backend.Note: This function cannot be used in defn.ExamplesNx.default_backend({EXLA.Backend,device::cuda})#=> {Nx.BinaryBackend, []}Link to this functionglobal_default_backend(backend)View SourceSets the default backend globally.You must avoid calling this function at runtime. It is mostly
useful during scripts or code notebooks to set a default.If you need to configure a global default backend in your
applications, it is generally preferred to do so in your
config/*.exs files:config:nx,:default_backend,{EXLA.Backend,[]}In your notebooks and on Mix.install/2, you might:Mix.install([{:nx,">= 0.0.0"}],config:[nx:[default_backend:{EXLA.Backend,device::cuda}]])The function returns the value that was previously set as global backend.Link to this functionwith_default_backend(backend, fun)View SourceInvokes the given function temporarily setting backend as the
default backend.Functions: ConversionLink to this functiondeserialize(data, opts \\ [])View SourceDeserializes a serialized representation of a tensor or a container
with the given options.It is the opposite of Nx.serialize/2.Note: This function cannot be used in defn.Examplesiex> a=Nx.tensor([1,2,3])iex> serialized_a=Nx.serialize(a)iex> Nx.deserialize(serialized_a)#Nx.Tensor<s32[3][1,2,3]>iex> container={Nx.vectorize(Nx.tensor([1,2,3]),:x),%{b:Nx.tensor([4,5,6])}}iex> serialized_container=Nx.serialize(container)iex> {a,%{b:b}}=Nx.deserialize(serialized_container)iex> a#Nx.Tensor<vectorized[x:3]s32[1,2,3]>iex> b#Nx.Tensor<s32[3][4,5,6]>Link to this functionload_numpy!(data)View Source@spec load_numpy!(data :: binary()) :: Nx.Tensor.t()Loads a .npy file into a tensor.An .npy file stores a single array created from Python's
NumPy library. This function can be useful for loading data
originally created or intended to be loaded from NumPy into
Elixir.This function will raise if the archive or any of its contents
are invalid.Note: This function cannot be used in defn.Examples"array.npy"|>File.read!()|>Nx.load_numpy!()#=>#Nx.Tensor<s32[3][1,2,3]>Link to this functionload_numpy_archive!(archive)View Source@spec load_numpy_archive!(data :: binary()) :: [{name :: binary(), Nx.Tensor.t()}]Loads a .npz archive into a list of tensors.An .npz file is a zipped, possibly compressed
archive containing multiple .npy files.It returns a list of two elements tuples, where
the tensor name is first and the serialized tensor
is second. The list is returned in the same order
as in the archive. Use Map.new/1 afterwards if
you want to access the list elements by name.It will raise if the archive or any of its contents
are invalid.Note: This function cannot be used in defn.Examples"archive.npz"|>File.read!()|>Nx.load_numpy_archive!()#=>[{"foo",#Nx.Tensor<s32[3][1,2,3]>},{"bar",#Nx.Tensor<f64[5][-1.0,-0.5,0.0,0.5,1.0]>}]Link to this functionserialize(tensor_or_container, opts \\ [])View SourceSerializes the given tensor or container of tensors to iodata.You may pass any tensor or Nx.Container to serialization.
Opposite to other functions in this module, Nx.LazyContainer
cannot be serialized and they must be explicitly converted
to tensors before (that's because lazy containers do not preserve
their shape).opts controls the serialization options. For example, you can choose
to compress the given tensor or container of tensors by passing a
compression level:Nx.serialize(tensor,compressed:9)Compression level corresponds to compression options in :erlang.term_to_iovec/2.iodata is a list of binaries that can be written to any io device,
such as a file or a socket. You can ensure the result is a binary by
calling IO.iodata_to_binary/1.Note: This function cannot be used in defn.Examplesiex> a=Nx.tensor([1,2,3])iex> serialized_a=Nx.serialize(a)iex> Nx.deserialize(serialized_a)#Nx.Tensor<s32[3][1,2,3]>iex> container={Nx.tensor([1,2,3]),%{b:Nx.tensor([4,5,6])}}iex> serialized_container=Nx.serialize(container)iex> {a,%{b:b}}=Nx.deserialize(serialized_container)iex> a#Nx.Tensor<s32[3][1,2,3]>iex> b#Nx.Tensor<s32[3][4,5,6]>Link to this functionto_batched(tensor, batch_size, opts \\ [])View SourceConverts the underlying tensor to a stream of tensor batches.The first dimension (axis 0) is divided by batch_size.
In case the dimension cannot be evenly divided by
batch_size, you may specify what to do with leftover
data using :leftover. :leftover must be one of :repeat
or :discard. :repeat repeats the first n values to
make the last batch match the desired batch size. :discard
discards excess elements.Note: This function cannot be used in defn.ExamplesIn the examples below we immediately pipe to Enum.to_list/1
for convenience, but in practice you want to lazily traverse
the batches to avoid allocating multiple tensors at once in
certain backends:iex> [first,second]=Nx.to_batched(Nx.iota({2,2,2}),1)|>Enum.to_list()iex> first#Nx.Tensor<s32[1][2][2][[[0,1],[2,3]]]>iex> second#Nx.Tensor<s32[1][2][2][[[4,5],[6,7]]]>If the batch size would result in uneven batches, you can repeat or discard excess data.
By default, we repeat:iex> [first,second,third]=Nx.to_batched(Nx.iota({5,2},names:[:x,:y]),2)|>Enum.to_list()iex> first#Nx.Tensor<s32[x:2][y:2][[0,1],[2,3]]>iex> second#Nx.Tensor<s32[x:2][y:2][[4,5],[6,7]]>iex> third#Nx.Tensor<s32[x:2][y:2][[8,9],[0,1]]>But you can also discard:iex> [first,second]=Nx.to_batched(Nx.iota({5,2},names:[:x,:y]),2,leftover::discard)|>Enum.to_list()iex> first#Nx.Tensor<s32[x:2][y:2][[0,1],[2,3]]>iex> second#Nx.Tensor<s32[x:2][y:2][[4,5],[6,7]]>Vectorized tensorsSimilarly to to_list/1 and to_binary/1, to_batched/2 will
ignore vectorization to perform calculations. Because the output
still contains tensors, however, they will still be vectorized.iex> t=Nx.iota({2,2,2})|>Nx.vectorize(x:2)iex> [first,second]=Nx.to_batched(t,1)|>Enum.to_list()iex> first#Nx.Tensor<vectorized[x:1]s32[2][2][[[0,1],[2,3]]]>iex> second#Nx.Tensor<vectorized[x:1]s32[2][2][[[4,5],[6,7]]]>iex> t=Nx.iota({2,2,2})|>Nx.vectorize(x:2,y:2)iex> [first,second]=Nx.to_batched(t,1)|>Enum.to_list()iex> first#Nx.Tensor<vectorized[x:1][y:2]s32[2][[[0,1],[2,3]]]>iex> second#Nx.Tensor<vectorized[x:1][y:2]s32[2][[[4,5],[6,7]]]>Same rules about uneven batches still apply:iex> t=Nx.iota({5,2},names:[:x,:y])|>Nx.vectorize(:x)iex> [first,second,third]=Nx.to_batched(t,2)|>Enum.to_list()iex> first#Nx.Tensor<vectorized[x:2]s32[y:2][[0,1],[2,3]]>iex> second#Nx.Tensor<vectorized[x:2]s32[y:2][[4,5],[6,7]]>iex> third#Nx.Tensor<vectorized[x:2]s32[y:2][[8,9],[0,1]]>Because we're dealing with vectorized tensors, a vectorized
scalar tensor can also be batched.iex> t=Nx.tensor([1,2,3])|>Nx.vectorize(:x)iex> [first,second]=t|>Nx.to_batched(2)|>Enum.to_list()iex> first#Nx.Tensor<vectorized[x:2]s32[1,2]>iex> second#Nx.Tensor<vectorized[x:2]s32[3,1]>Link to this functionto_binary(tensor, opts \\ [])View SourceReturns the underlying tensor as a binary.It returns the in-memory binary representation of
the tensor in a row-major fashion. The binary is
in the system endianness, which has to be taken into
account if the binary is meant to be serialized to
other systems.This function cannot be used in defn.Potentially expensive operationConverting a tensor to a binary can potentially be a very
expensive operation, as it may copy a GPU tensor fully to
the machine memory.Binaries vs bitstringsIf a tensor of type u2/u4/s2/s4 is given to this function,
this function may not return a binary (where the number of bits
is divisible by 8) but rather a bitstring (where the number of
bits may not be divisible by 8).Options:limit - limit the number of entries represented in the binaryExamplesiex> Nx.to_binary(1)<<1::32-native>>iex> Nx.to_binary(Nx.tensor([1.0,2.0,3.0]))<<1.0::float-32-native,2.0::float-32-native,3.0::float-32-native>>iex> Nx.to_binary(Nx.tensor([1.0,2.0,3.0]),limit:2)<<1.0::float-32-native,2.0::float-32-native>>Vectorized tensorsto_binary/2 disregards the vectorized axes before calculating the data to be returned:iex> Nx.to_binary(Nx.vectorize(Nx.tensor([[1,2],[3,4]]),:x))<<1::32-native,2::32-native,3::32-native,4::32-native>>iex> Nx.to_binary(Nx.vectorize(Nx.tensor([1,2,3]),:x),limit:2)<<1::32-native,2::32-native>>Link to this functionto_flat_list(tensor, opts \\ [])View SourceReturns the underlying tensor as a flat list.Negative infinity (-Inf), infinity (Inf), and "not a number" (NaN)
will be represented by the atoms :neg_infinity, :infinity, and
:nan respectively.Note: This function cannot be used in defn.Examplesiex> Nx.to_flat_list(1)[1]iex> Nx.to_flat_list(Nx.tensor([1.0,2.0,3.0]))[1.0,2.0,3.0]iex> Nx.to_flat_list(Nx.tensor([1.0,2.0,3.0]),limit:2)[1.0,2.0]Non-finite numbers are returned as atoms:iex> t=Nx.tensor([:neg_infinity,:nan,:infinity])iex> Nx.to_flat_list(t)[:neg_infinity,:nan,:infinity]Vectorized tensorsto_flat_list/2 disregards the vectorized axes before calculating the data to be returned.
Like to_binary/1, :limit refers to the flattened devectorized data.iex> t=Nx.vectorize(Nx.tensor([[1],[2],[3],[4]]),:x)iex> Nx.to_flat_list(t)[1,2,3,4]iex> Nx.to_flat_list(t,limit:2)[1,2]Link to this functionto_heatmap(tensor, opts \\ [])View SourceReturns a heatmap struct with the tensor data.On terminals, coloring is done via ANSI colors. If ANSI
is not enabled, the tensor is normalized to show numbers
between 0 and 9.Terminal coloringColoring is enabled by default on most Unix terminals.
It is also available on Windows consoles from Windows
10, although it must be explicitly enabled for the current
user in the registry by running the following command:regaddHKCU\Console/vVirtualTerminalLevel/tREG_DWORD/d1After running the command above, you must restart your current
console.Options:ansi_enabled - forces ansi to be enabled or disabled.
Defaults to IO.ANSI.enabled?/0:ansi_whitespace - which whitespace character to use when
printing. By default it uses "\u3000", which is a full-width
whitespace which often prints more precise shapesLink to this functionto_list(tensor)View SourceConverts the tensor into a list reflecting its structure.Negative infinity (-Inf), infinity (Inf), and "not a number" (NaN)
will be represented by the atoms :neg_infinity, :infinity, and
:nan respectively.It raises if a scalar tensor is given, use to_number/1 instead.Note: This function cannot be used in defn.Examplesiex> Nx.iota({2,3})|>Nx.to_list()[[0,1,2],[3,4,5]]iex> Nx.tensor(123)|>Nx.to_list()** (ArgumentError) cannot convert a scalar tensor to a list, got: #Nx.Tensor<s32123>Vectorized tensorsto_list/1 disregards the vectorized axes before calculating the data to be returned.
The special case below shows that a vectorized tensor with inner scalar shape will
still be converted to a list accordingly:iex> %{shape:{}}=t=Nx.vectorize(Nx.tensor([1,2,3]),:x)iex> Nx.to_list(t)# recall that normally, shape == {} would raise![1,2,3]Link to this functionto_number(tensor)View SourceReturns the underlying tensor as a number.Negative infinity (-Inf), infinity (Inf), and "not a number" (NaN)
will be represented by the atoms :neg_infinity, :infinity, and
:nan respectively.If the tensor has a dimension or is vectorized, it raises.Note: This function cannot be used in defn.Examplesiex> Nx.to_number(1)1iex> Nx.to_number(Nx.tensor([1.0,2.0,3.0]))** (ArgumentError) cannot convert tensor of shape {3} to numberiex> Nx.to_number(Nx.vectorize(Nx.tensor([1]),:x))** (ArgumentError) cannot convert vectorized tensor with axes [x: 1] and shape {} to numberLink to this functionto_template(tensor_or_container)View SourceConverts a tensor (or tuples and maps of tensors) to tensor templates.Templates are useful when you need to pass types and shapes to
operations and the data is not yet available.For convenience, this function accepts tensors and any container
(such as maps and tuples as defined by the Nx.LazyContainer protocol)
and recursively converts all tensors to templates.Examplesiex> Nx.iota({2,3})|>Nx.to_template()#Nx.Tensor<s32[2][3]Nx.TemplateBackend>iex> {int,float}=Nx.to_template({1,2.0})iex> int#Nx.Tensor<s32Nx.TemplateBackend>iex> float#Nx.Tensor<f32Nx.TemplateBackend>Although note it is impossible to perform any operation on a tensor template:iex> t=Nx.iota({2,3})|>Nx.to_template()iex> Nx.abs(t)** (RuntimeError) cannot perform operations on a Nx.TemplateBackend tensorTo build a template from scratch, use template/3.Link to this functionto_tensor(t)View SourceConverts a data structure into a tensor.This function only converts types which are automatically
cast to tensors throughout Nx API: numbers, complex numbers,
tensors themselves, and implementations of Nx.LazyContainer
(and Nx.Container).If your goal is to create tensors from lists, see tensor/2.
If you want to create a tensor from binary, see from_binary/3.
If you want to convert a data structure with several tensors at
once into a single one, see stack/2 or concatenate/2 instead.Functions: CreationLink to this functionbf16(tensor)View SourceShort-hand function for creating tensor of type bf16.This is just an alias for Nx.tensor(tensor, type: bf16).Link to this functioneye(n_or_tensor_or_shape, opts \\ [])View SourceCreates the identity matrix of size n.Options:type - the type of the tensor:names - the names of the tensor dimensions:backend - the backend to allocate the tensor on. It is either
an atom or a tuple in the shape {backend, options}. This option
is ignored inside defn:vectorized_axes - a keyword list of axis_name: axis_size.
If given, the resulting tensor will be vectorized accordingly.
Vectorization is not supported via tensor inputs.Examplesiex> Nx.eye(2)#Nx.Tensor<s32[2][2][[1,0],[0,1]]>iex> Nx.eye(3,type::f32,names:[:height,:width])#Nx.Tensor<f32[height:3][width:3][[1.0,0.0,0.0],[0.0,1.0,0.0],[0.0,0.0,1.0]]>The first argument can also be a shape of a matrix:iex> Nx.eye({1,2})#Nx.Tensor<s32[1][2][[1,0]]>The shape can also represent a tensor batch. In this case,
the last two axes will represent the same identity matrix.iex> Nx.eye({2,4,3})#Nx.Tensor<s32[2][4][3][[[1,0,0],[0,1,0],[0,0,1],[0,0,0]],[[1,0,0],[0,1,0],[0,0,1],[0,0,0]]]>Vectorized tensorsIf given, vectorized axes, are added as leading dimensions to the tensor,
effectively broadcasting the base shape along them.iex> Nx.eye({3},vectorized_axes:[x:1,y:2])#Nx.Tensor<vectorized[x:1][y:2]s32[3][[[1,0,0],[1,0,0]]]>iex> Nx.eye({2,3},vectorized_axes:[x:2])#Nx.Tensor<vectorized[x:2]s32[2][3][[[1,0,0],[0,1,0]],[[1,0,0],[0,1,0]]]>Link to this functionf8(tensor)View SourceShort-hand function for creating tensor of type f8.This is just an alias for Nx.tensor(tensor, type: f8).Link to this functionf16(tensor)View SourceShort-hand function for creating tensor of type f16.This is just an alias for Nx.tensor(tensor, type: f16).Link to this functionf32(tensor)View SourceShort-hand function for creating tensor of type f32.This is just an alias for Nx.tensor(tensor, type: f32).Link to this functionf64(tensor)View SourceShort-hand function for creating tensor of type f64.This is just an alias for Nx.tensor(tensor, type: f64).Link to this functionfrom_binary(binary, type, opts \\ [])View SourceCreates a one-dimensional tensor from a binary with the given type.If the binary size does not match its type, an error is raised.Examplesiex> Nx.from_binary(<<1,2,3,4>>,:s8)#Nx.Tensor<s8[4][1,2,3,4]>The atom notation for types is also supported:iex> Nx.from_binary(<<12.3::float-64-native>>,:f64)#Nx.Tensor<f64[1][12.3]>An error is raised for incompatible sizes:iex> Nx.from_binary(<<1,2,3,4>>,:f64)** (ArgumentError) binary does not match the given sizeOptions:backend - the backend to allocate the tensor on. It is either
an atom or a tuple in the shape {backend, options}. This option
is ignored inside defnLink to this functionfrom_pointer(backend, pointer, type, shape, opts \\ [])View SourceCreates an Nx-tensor from an already-allocated memory space.This function should be used with caution, as it can lead to segmentation faults.The backend argument is either the backend module (such as Nx.BinaryBackend),
or a tuple of {module, keyword()} with specific backend configuration.
pointer is the corresponding value that would be returned from
a call to get_pointer/2.OptionsBesides the options listed below, all other options are forwarded to the
underlying implementation.:names - refer to tensor/2Examplespointer=%Nx.Pointer{kind::local,address:1234}Nx.from_pointer(MyBackend,pointer,{:s,32},{1,3})#Nx.Tensor<s32[1][3][[10,20,30]]>pointer=%Nx.Pointer{kind::ipc,handle:"some-ipc-handle"}Nx.from_pointer({MyBackend,some::opt},pointer,{:s,32},{1,3},names:[nil,:col])#Nx.Tensor<s32[1][col:3][[10,20,30]]>Link to this functioniota(tensor_or_shape, opts \\ [])View SourceCreates a tensor with the given shape which increments
along the provided axis. You may optionally provide dimension
names.If no axis is provided, index counts up at each element.If a tensor or a number are given, the shape and names are taken from the tensor.Options:type - the type of the tensor:axis - an axis to repeat the iota over:names - the names of the tensor dimensions:backend - the backend to allocate the tensor on. It is either
an atom or a tuple in the shape {backend, options}. This option
is ignored inside defn:vectorized_axes - a keyword list of axis_name: axis_size.
If given, the resulting tensor will be vectorized accordingly.
Vectorization is not supported via tensor inputs.Examplesiex> Nx.iota({})#Nx.Tensor<s320>iex> Nx.iota({5})#Nx.Tensor<s32[5][0,1,2,3,4]>iex> Nx.iota({3,2,3},names:[:batch,:height,:width])#Nx.Tensor<s32[batch:3][height:2][width:3][[[0,1,2],[3,4,5]],[[6,7,8],[9,10,11]],[[12,13,14],[15,16,17]]]>iex> Nx.iota({3,3},axis:1,names:[:batch,nil])#Nx.Tensor<s32[batch:3][3][[0,1,2],[0,1,2],[0,1,2]]>iex> Nx.iota({3,3},axis:-1)#Nx.Tensor<s32[3][3][[0,1,2],[0,1,2],[0,1,2]]>iex> Nx.iota({3,4,3},axis:0,type::f64)#Nx.Tensor<f64[3][4][3][[[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0]],[[1.0,1.0,1.0],[1.0,1.0,1.0],[1.0,1.0,1.0],[1.0,1.0,1.0]],[[2.0,2.0,2.0],[2.0,2.0,2.0],[2.0,2.0,2.0],[2.0,2.0,2.0]]]>iex> Nx.iota({1,3,2},axis:2)#Nx.Tensor<s32[1][3][2][[[0,1],[0,1],[0,1]]]>iex> Nx.iota({2,3},axis:0,vectorized_axes:[x:1,y:2])#Nx.Tensor<vectorized[x:1][y:2]s32[2][3][[[[0,0,0],[1,1,1]],[[0,0,0],[1,1,1]]]]>Link to this functionlinspace(start, stop, opts \\ [])View SourceCreates a tensor of shape {n} with linearly spaced samples between start and stop.Options:n - The number of samples in the tensor.:name - Optional name for the output axis.:type - Optional type for the output. Defaults to {:f, 32}:endpoint - Boolean that indicates whether to include stop
as the last point in the output. Defaults to trueExamplesiex> Nx.linspace(5,8,n:5)#Nx.Tensor<f32[5][5.0,5.75,6.5,7.25,8.0]>iex> Nx.linspace(0,10,n:5,endpoint:false,name::x)#Nx.Tensor<f32[x:5][0.0,2.0,4.0,6.0,8.0]>For integer types, the results might not be what's expected.
When endpoint: true (the default), the step is given by
step = (stop - start) / (n - 1), which means that instead
of a step of 3 in the example below, we get a step close to
3.42. The results are calculated first and only cast in the
end, so that the :endpoint condition is respected.iex> Nx.linspace(0,24,n:8,type:{:u,8},endpoint:true)#Nx.Tensor<u8[8][0,3,6,10,13,17,20,24]>iex> Nx.linspace(0,24,n:8,type:{:s,32},endpoint:false)#Nx.Tensor<s32[8][0,3,6,9,12,15,18,21]>One can also pass two higher order tensors with the same shape {j, k, ...}, in which case
the output will be of shape {j, k, ..., n}.  iex> Nx.linspace(Nx.tensor([[[0, 10]]]), Nx.tensor([[[10, 100]]]), n: 10, name: :samples, type: {:u, 8})
  #Nx.Tensor<u8[1][1][2][samples:10][[[[0,1,2,3,4,5,6,7,8,10],[10,20,30,40,50,60,70,80,90,100]]]]Vectorized tensorsiex> Nx.linspace(0,Nx.vectorize(Nx.tensor([10,20]),:x),n:5)#Nx.Tensor<vectorized[x:2]f32[5][[0.0,2.5,5.0,7.5,10.0],[0.0,5.0,10.0,15.0,20.0]]>iex> start=Nx.vectorize(Nx.tensor([0,1]),:x)iex> stop=Nx.vectorize(Nx.tensor([10,20]),:y)iex> Nx.linspace(start,stop,n:5)#Nx.Tensor<vectorized[x:2][y:2]f32[5][[[0.0,2.5,5.0,7.5,10.0],[0.0,5.0,10.0,15.0,20.0]],[[1.0,3.25,5.5,7.75,10.0],[1.0,5.75,10.5,15.25,20.0]]]>iex> start=Nx.vectorize(Nx.tensor([0,1]),:x)iex> stop=Nx.vectorize(Nx.tensor([10,10]),:x)iex> Nx.linspace(start,stop,n:5)#Nx.Tensor<vectorized[x:2]f32[5][[0.0,2.5,5.0,7.5,10.0],[1.0,3.25,5.5,7.75,10.0]]>Error casesiex> Nx.linspace(0,24,n:1.0)** (ArgumentError) expected n to be a non-negative integer, got: 1.0iex> Nx.linspace(Nx.tensor([[0,1]]),Nx.tensor([1,2,3]),n:2)** (ArgumentError) expected start and stop to have the same shape. Got shapes {1, 2} and {3}Link to this functionmake_diagonal(tensor, opts \\ [])View SourceCreates a diagonal tensor from a 1D tensor.Converse of take_diagonal/2.The returned tensor will be a square matrix of dimensions equal
to the size of the tensor. If an offset is given, the absolute value
of the offset is added to the matrix dimensions sizes.Options:offset - offset used for making the diagonal.
Use offset > 0 for diagonals above the main diagonal,
and offset < 0 for diagonals below the main diagonal.
Defaults to 0.Examples  Given a 1D tensor:iex> Nx.make_diagonal(Nx.tensor([1,2,3,4]))#Nx.Tensor<s32[4][4][[1,0,0,0],[0,2,0,0],[0,0,3,0],[0,0,0,4]]>  Given a 1D tensor with an offset:iex> Nx.make_diagonal(Nx.tensor([1,2,3]),offset:1)#Nx.Tensor<s32[4][4][[0,1,0,0],[0,0,2,0],[0,0,0,3],[0,0,0,0]]>iex> Nx.make_diagonal(Nx.tensor([1,2,3]),offset:-1)#Nx.Tensor<s32[4][4][[0,0,0,0],[1,0,0,0],[0,2,0,0],[0,0,3,0]]>  You can also have offsets with an abs greater than the tensor length:iex> Nx.make_diagonal(Nx.tensor([1,2,3]),offset:-4)#Nx.Tensor<s32[7][7][[0,0,0,0,0,0,0],[0,0,0,0,0,0,0],[0,0,0,0,0,0,0],[0,0,0,0,0,0,0],[1,0,0,0,0,0,0],[0,2,0,0,0,0,0],[0,0,3,0,0,0,0]]>iex> Nx.make_diagonal(Nx.tensor([1,2,3]),offset:4)#Nx.Tensor<s32[7][7][[0,0,0,0,1,0,0],[0,0,0,0,0,2,0],[0,0,0,0,0,0,3],[0,0,0,0,0,0,0],[0,0,0,0,0,0,0],[0,0,0,0,0,0,0],[0,0,0,0,0,0,0]]>Vectorized tensorsiex> t=Nx.vectorize(Nx.tensor([[1,2],[3,4]]),:x)iex> Nx.make_diagonal(t,offset:1)#Nx.Tensor<vectorized[x:2]s32[3][3][[[0,1,0],[0,0,2],[0,0,0]],[[0,3,0],[0,0,4],[0,0,0]]]>iex> Nx.make_diagonal(t,offset:-1)#Nx.Tensor<vectorized[x:2]s32[3][3][[[0,0,0],[1,0,0],[0,2,0]],[[0,0,0],[3,0,0],[0,4,0]]]>Error casesiex> Nx.make_diagonal(Nx.tensor([[0,0],[0,1]]))** (ArgumentError) make_diagonal/2 expects tensor of rank 1, got tensor of rank: 2Link to this functionput_diagonal(tensor, diagonal, opts \\ [])View SourcePuts the individual values from a 1D diagonal into the diagonal indices
of the given 2D tensor.See also: take_diagonal/2, make_diagonal/2.ExamplesGiven a 2D tensor and a 1D diagonal:iex> t=Nx.broadcast(0,{4,4})#Nx.Tensor<s32[4][4][[0,0,0,0],[0,0,0,0],[0,0,0,0],[0,0,0,0]]>iex> Nx.put_diagonal(t,Nx.tensor([1,2,3,4]))#Nx.Tensor<s32[4][4][[1,0,0,0],[0,2,0,0],[0,0,3,0],[0,0,0,4]]>iex> t=Nx.broadcast(0,{4,3})#Nx.Tensor<s32[4][3][[0,0,0],[0,0,0],[0,0,0],[0,0,0]]>iex> Nx.put_diagonal(t,Nx.tensor([1,2,3]))#Nx.Tensor<s32[4][3][[1,0,0],[0,2,0],[0,0,3],[0,0,0]]>Given a 2D tensor and a 1D diagonal with a positive offset:iex> Nx.put_diagonal(Nx.broadcast(0,{4,4}),Nx.tensor([1,2,3]),offset:1)#Nx.Tensor<s32[4][4][[0,1,0,0],[0,0,2,0],[0,0,0,3],[0,0,0,0]]>iex> Nx.put_diagonal(Nx.broadcast(0,{4,3}),Nx.tensor([1,2]),offset:1)#Nx.Tensor<s32[4][3][[0,1,0],[0,0,2],[0,0,0],[0,0,0]]>Given a 2D tensor and a 1D diagonal with a negative offset:iex> Nx.put_diagonal(Nx.broadcast(0,{4,4}),Nx.tensor([1,2,3]),offset:-1)#Nx.Tensor<s32[4][4][[0,0,0,0],[1,0,0,0],[0,2,0,0],[0,0,3,0]]>iex> Nx.put_diagonal(Nx.broadcast(0,{4,3}),Nx.tensor([1,2,3]),offset:-1)#Nx.Tensor<s32[4][3][[0,0,0],[1,0,0],[0,2,0],[0,0,3]]>Options:offset - offset used for putting the diagonal.
Use offset > 0 for diagonals above the main diagonal,
and offset < 0 for diagonals below the main diagonal.
Defaults to 0.Error casesGiven an invalid tensor:iex> Nx.put_diagonal(Nx.iota({3,3,3}),Nx.iota({3}))** (ArgumentError) put_diagonal/3 expects tensor of rank 2, got tensor of rank: 3Given invalid diagonals:iex> Nx.put_diagonal(Nx.iota({3,3}),Nx.iota({3,3}))** (ArgumentError) put_diagonal/3 expects diagonal of rank 1, got tensor of rank: 2iex> Nx.put_diagonal(Nx.iota({3,3}),Nx.iota({2}))** (ArgumentError) expected diagonal tensor of length: 3, got diagonal tensor of length: 2iex> Nx.put_diagonal(Nx.iota({3,3}),Nx.iota({3}),offset:1)** (ArgumentError) expected diagonal tensor of length: 2, got diagonal tensor of length: 3Given invalid offsets:iex> Nx.put_diagonal(Nx.iota({3,3}),Nx.iota({3}),offset:4)** (ArgumentError) offset must be less than length of axis 1 when positive, got: 4iex> Nx.put_diagonal(Nx.iota({3,3}),Nx.iota({3}),offset:-3)** (ArgumentError) absolute value of offset must be less than length of axis 0 when negative, got: -3Link to this functions2(tensor)View SourceShort-hand function for creating tensor of type s2.This is just an alias for Nx.tensor(tensor, type: s2).Link to this functions4(tensor)View SourceShort-hand function for creating tensor of type s4.This is just an alias for Nx.tensor(tensor, type: s4).Link to this functions8(tensor)View SourceShort-hand function for creating tensor of type s8.This is just an alias for Nx.tensor(tensor, type: s8).Link to this functions16(tensor)View SourceShort-hand function for creating tensor of type s16.This is just an alias for Nx.tensor(tensor, type: s16).Link to this functions32(tensor)View SourceShort-hand function for creating tensor of type s32.This is just an alias for Nx.tensor(tensor, type: s32).Link to this functions64(tensor)View SourceShort-hand function for creating tensor of type s64.This is just an alias for Nx.tensor(tensor, type: s64).Link to this macrosigil_MAT(arg, modifiers)View Source(macro)A convenient ~MAT sigil for building matrices (two-dimensional tensors).ExamplesBefore using sigils, you must first import them:importNx,only::sigilsThen you use the sigil to create matrices. The sigil:~MAT<-1001020000300004>Is equivalent to:Nx.tensor([[-1,0,0,1],[0,2,0,0],[0,0,3,0],[0,0,0,4]])If the tensor has any complex type, it defaults to c64.
If the tensor has any float type, it defaults to f32.
Otherwise, it is s64. You can specify the tensor type
as a sigil modifier:iex> importNx,only::sigilsiex> ~MAT[0.10.20.30.4]f16#Nx.Tensor<f16[1][4][[0.0999755859375,0.199951171875,0.300048828125,0.39990234375]]>iex> ~MAT[1+1i2-2.0i-3]#Nx.Tensor<c64[1][3][[1.0+1.0i,2.0-2.0i,-3.0+0.0i]]>iex> ~MAT[1InfNaN]#Nx.Tensor<f32[1][3][[1.0,Inf,NaN]]>iex> ~MAT[1iInfNaN]#Nx.Tensor<c64[1][3][[0.0+1.0i,Inf+0.0i,NaN+0.0i]]>iex> ~MAT[1iInf+2iNaN-Infi]#Nx.Tensor<c64[1][3][[0.0+1.0i,Inf+2.0i,NaN-Infi]]>Link to this macrosigil_VEC(arg, modifiers)View Source(macro)A convenient ~VEC sigil for building vectors (one-dimensional tensors).ExamplesBefore using sigils, you must first import them:importNx,only::sigilsThen you use the sigil to create vectors. The sigil:~VEC[-1001]Is equivalent to:Nx.tensor([-1,0,0,1])If the tensor has any complex type, it defaults to c64.
If the tensor has any float type, it defaults to f32.
Otherwise, it is s64. You can specify the tensor type
as a sigil modifier:iex> importNx,only::sigilsiex> ~VEC[0.10.20.30.4]f16#Nx.Tensor<f16[4][0.0999755859375,0.199951171875,0.300048828125,0.39990234375]>iex> ~VEC[1+1i2-2.0i-3]#Nx.Tensor<c64[3][1.0+1.0i,2.0-2.0i,-3.0+0.0i]>iex> ~VEC[1InfNaN]#Nx.Tensor<f32[3][1.0,Inf,NaN]>iex> ~VEC[1iInfNaN]#Nx.Tensor<c64[3][0.0+1.0i,Inf+0.0i,NaN+0.0i]>iex> ~VEC[1iInf+2iNaN-Infi]#Nx.Tensor<c64[3][0.0+1.0i,Inf+2.0i,NaN-Infi]>Link to this functiontake_diagonal(tensor, opts \\ [])View SourceExtracts the diagonal of batched matrices.Converse of make_diagonal/2.ExamplesGiven a matrix without offset:iex> Nx.take_diagonal(Nx.tensor([...> [0,1,2],...> [3,4,5],...> [6,7,8]...> ]))#Nx.Tensor<s32[3][0,4,8]>And if given a matrix along with an offset:iex> Nx.take_diagonal(Nx.iota({3,3}),offset:1)#Nx.Tensor<s32[2][1,5]>iex> Nx.take_diagonal(Nx.iota({3,3}),offset:-1)#Nx.Tensor<s32[2][3,7]>Given batched matrix:iex> Nx.take_diagonal(Nx.iota({3,2,2}))#Nx.Tensor<s32[3][2][[0,3],[4,7],[8,11]]>iex> Nx.take_diagonal(Nx.iota({3,2,2}),offset:-1)#Nx.Tensor<s32[3][1][[2],[6],[10]]>Options:offset - offset used for extracting the diagonal.
Use offset > 0 for diagonals above the main diagonal,
and offset < 0 for diagonals below the main diagonal.
Defaults to 0.Error casesiex> Nx.take_diagonal(Nx.tensor([0,1,2]))** (ArgumentError) take_diagonal/2 expects tensor of rank 2 or higher, got tensor of rank: 1iex> Nx.take_diagonal(Nx.iota({3,3}),offset:3)** (ArgumentError) offset must be less than length of axis 1 when positive, got: 3iex> Nx.take_diagonal(Nx.iota({3,3}),offset:-4)** (ArgumentError) absolute value of offset must be less than length of axis 0 when negative, got: -4Link to this functiontemplate(shape, type, opts \\ [])View SourceCreates a tensor template.You can't perform any operation on this tensor.
It exists exclusively to define APIs that say
a tensor with a certain type, shape, and names
is expected in the future.Examplesiex> Nx.template({2,3},:f32)#Nx.Tensor<f32[2][3]Nx.TemplateBackend>iex> Nx.template({2,3},{:f,32},names:[:rows,:columns])#Nx.Tensor<f32[rows:2][columns:3]Nx.TemplateBackend>Although note it is impossible to perform any operation on a tensor template:iex> t=Nx.template({2,3},{:f,32},names:[:rows,:columns])iex> Nx.abs(t)** (RuntimeError) cannot perform operations on a Nx.TemplateBackend tensorTo convert existing tensors to templates, use to_template/1.Link to this functiontensor(arg, opts \\ [])View SourceBuilds a tensor.The argument must be one of:a tensora number (which means the tensor is scalar/zero-dimensional)a boolean (also scalar/zero-dimensional)an arbitrarily nested list of numbers and booleansIf a new tensor has to be allocated, it will be allocated in
Nx.default_backend/0, unless the :backend option is given,
which overrides the default one.ExamplesA number returns a tensor of zero dimensions:iex> Nx.tensor(0)#Nx.Tensor<s320>iex> Nx.tensor(1.0)#Nx.Tensor<f321.0>Giving a list returns a vector (a one-dimensional tensor):iex> Nx.tensor([1,2,3])#Nx.Tensor<s32[3][1,2,3]>iex> Nx.tensor([1.2,2.3,3.4,4.5])#Nx.Tensor<f32[4][1.2000000476837158,2.299999952316284,3.4000000953674316,4.5]>The type can be explicitly given. Integers and floats
bigger than the given size overflow:iex> Nx.tensor([300,301,302],type::s8)#Nx.Tensor<s8[3][44,45,46]>Mixed types give higher priority to floats:iex> Nx.tensor([1,2,3.0])#Nx.Tensor<f32[3][1.0,2.0,3.0]>Boolean values are also accepted, where true is
converted to 1 and false to 0, with the type
being inferred as {:u, 8}iex> Nx.tensor(true)#Nx.Tensor<u81>iex> Nx.tensor(false)#Nx.Tensor<u80>iex> Nx.tensor([true,false])#Nx.Tensor<u8[2][1,0]>Multi-dimensional tensors are also possible:iex> Nx.tensor([[1,2,3],[4,5,6]])#Nx.Tensor<s32[2][3][[1,2,3],[4,5,6]]>iex> Nx.tensor([[1,2],[3,4],[5,6]])#Nx.Tensor<s32[3][2][[1,2],[3,4],[5,6]]>iex> Nx.tensor([[[1,2],[3,4],[5,6]],[[-1,-2],[-3,-4],[-5,-6]]])#Nx.Tensor<s32[2][3][2][[[1,2],[3,4],[5,6]],[[-1,-2],[-3,-4],[-5,-6]]]>Floats and complex numbersBesides single-precision (32 bits), floats can also have
half-precision (16) or double-precision (64):iex> Nx.tensor([1,2,3],type::f16)#Nx.Tensor<f16[3][1.0,2.0,3.0]>iex> Nx.tensor([1,2,3],type::f64)#Nx.Tensor<f64[3][1.0,2.0,3.0]>Brain-floating points are also supported:iex> Nx.tensor([1,2,3],type::bf16)#Nx.Tensor<bf16[3][1.0,2.0,3.0]>Certain backends and compilers support 8-bit floats. The precision
iomplementation of 8-bit floats may change per backend, so you must
be careful when transferring data across. The binary backend implements
F8E5M2:iex> Nx.tensor([1,2,3],type::f8)#Nx.Tensor<f8[3][1.0,2.0,3.0]>In all cases, the non-finite values negative infinity (-Inf),
infinity (Inf), and "not a number" (NaN) can be represented by
the atoms :neg_infinity, :infinity, and :nan respectively:iex> Nx.tensor([:neg_infinity,:nan,:infinity])#Nx.Tensor<f32[3][-Inf,NaN,Inf]>Finally, complex numbers are also supported in tensors:iex> Nx.tensor(Complex.new(1,-1))#Nx.Tensor<c641.0-1.0i>Naming dimensionsYou can provide names for tensor dimensions. Names are atoms:iex> Nx.tensor([[1,2,3],[4,5,6]],names:[:x,:y])#Nx.Tensor<s32[x:2][y:3][[1,2,3],[4,5,6]]>Names make your code more expressive:iex> Nx.tensor([[[1,2,3],[4,5,6],[7,8,9]]],names:[:batch,:height,:width])#Nx.Tensor<s32[batch:1][height:3][width:3][[[1,2,3],[4,5,6],[7,8,9]]]>You can also leave dimension names as nil:iex> Nx.tensor([[[1,2,3],[4,5,6],[7,8,9]]],names:[:batch,nil,nil])#Nx.Tensor<s32[batch:1][3][3][[[1,2,3],[4,5,6],[7,8,9]]]>However, you must provide a name for every dimension in the tensor:iex> Nx.tensor([[[1,2,3],[4,5,6],[7,8,9]]],names:[:batch])** (ArgumentError) invalid names for tensor of rank 3, when specifying names every dimension must have a name or be nilTensorsTensors can also be given as inputs:iex> Nx.tensor(Nx.tensor([1,2,3]))#Nx.Tensor<s32[3][1,2,3]>If the :backend and :type options are given, the tensor will
compared against those values and raise in case of mismatch:iex> Nx.tensor(Nx.tensor([1,2,3]),type::f32)** (ArgumentError) Nx.tensor/2 expects a tensor with type :f32 but it was given a tensor of type {:s, 32}The :backend option will check only against the backend name
and not specific backend configuration such as device and client.
In case the backend differs, it will also raise.The names in the given tensor are always discarded but Nx will raise
in case the tensor already has names that conflict with the assigned ones:iex> Nx.tensor(Nx.tensor([1,2,3]),names:[:row])#Nx.Tensor<s32[row:3][1,2,3]>iex> Nx.tensor(Nx.tensor([1,2,3],names:[:column]))#Nx.Tensor<s32[3][1,2,3]>iex> Nx.tensor(Nx.tensor([1,2,3],names:[:column]),names:[:row])** (ArgumentError)  cannot merge name :column on axis 0 with name :row on axis 0Options:type - sets the type of the tensor. If one is not given,
one is automatically inferred based on the input.:names - dimension names. If you wish to specify dimension
names you must specify a name for every dimension in the tensor.
Only nil and atoms are supported as dimension names.:backend - the backend to allocate the tensor on. It is either
an atom or a tuple in the shape {backend, options}. It defaults
to Nx.default_backend/0 for new tensorsLink to this functionto_pointer(tensor, opts \\ [])View SourceReturns an Nx.Pointer that represents either a local pointer or an IPC handle for the given tensor.Can be used in conjunction with from_pointer/5 to share the same memory
for multiple tensors, as well as for interoperability with other programming
languages.Options:kind - one of :local, :ipc. :local means the returned value
represents a pointer internal to the current process. :ipc means
the returned value represents an IPC handle that can be shared between
processes. Defaults to :local.Other options are relayed to the backend.Examplest=Nx.u8([10,20,30])Nx.to_pointer(t,kind::local)%Nx.Pointer{kind::local,address:1234,data_size:3,handle:nil}t=Nx.s32([1,2,3])Nx.to_pointer(t,kind::ipc)%Nx.Pointer{kind::ipc,address:nil,data_size:32,handle:"some-ipc-handle"}Link to this functiontri(n, m, opts \\ [])View SourceAn array with ones at and below the given diagonal and zeros elsewhere.Optionsk - The diagonal above which to zero elements. Default: 0.Examplesiex> tensor=Nx.tensor([[1,2,3],[4,5,6],[7,8,9]])iex> {num_rows,num_cols}=Nx.shape(tensor)iex> Nx.tri(num_rows,num_cols)#Nx.Tensor<u8[3][3][[1,0,0],[1,1,0],[1,1,1]]>iex> tensor=Nx.tensor([[1,2,3],[4,5,6],[7,8,9]])iex> {num_rows,num_cols}=Nx.shape(tensor)iex> Nx.tri(num_rows,num_cols,k:1)#Nx.Tensor<u8[3][3][[1,1,0],[1,1,1],[1,1,1]]>Link to this functiontril(tensor, opts \\ [])View SourceLower triangle of a matrix.Optionsk - The diagonal above which to zero elements. Default: 0.Examplesiex> Nx.tril(Nx.tensor([[1,2,3],[4,5,6],[7,8,9]]))#Nx.Tensor<s32[3][3][[1,0,0],[4,5,0],[7,8,9]]>iex> Nx.tril(Nx.tensor([[1,2,3],[4,5,6],[7,8,9]]),k:1)#Nx.Tensor<s32[3][3][[1,2,0],[4,5,6],[7,8,9]]>iex> Nx.tril(Nx.iota({2,3,4}))#Nx.Tensor<s32[2][3][4][[[0,0,0,0],[4,5,0,0],[8,9,10,0]],[[12,0,0,0],[16,17,0,0],[20,21,22,0]]]>iex> Nx.tril(Nx.iota({6}))** (ArgumentError) tril/2 expects a tensor with at least 2 dimensions, got: #Nx.Tensor<s32[6][0,1,2,3,4,5]>Link to this functiontriu(tensor, opts \\ [])View SourceUpper triangle of an array.Optionsk - The diagonal below which to zero elements. Default: 0.Examplesiex> Nx.triu(Nx.tensor([[1,2,3],[4,5,6],[7,8,9]]))#Nx.Tensor<s32[3][3][[1,2,3],[0,5,6],[0,0,9]]>iex> Nx.triu(Nx.tensor([[1,2,3],[4,5,6],[7,8,9]]),k:1)#Nx.Tensor<s32[3][3][[0,2,3],[0,0,6],[0,0,0]]>iex> Nx.triu(Nx.iota({2,3,4}))#Nx.Tensor<s32[2][3][4][[[0,1,2,3],[0,5,6,7],[0,0,10,11]],[[12,13,14,15],[0,17,18,19],[0,0,22,23]]]>iex> Nx.triu(Nx.iota({6}))** (ArgumentError) triu/2 expects a tensor with at least 2 dimensions, got: #Nx.Tensor<s32[6][0,1,2,3,4,5]>Link to this functionu2(tensor)View SourceShort-hand function for creating tensor of type u2.This is just an alias for Nx.tensor(tensor, type: u2).Link to this functionu4(tensor)View SourceShort-hand function for creating tensor of type u4.This is just an alias for Nx.tensor(tensor, type: u4).Link to this functionu8(tensor)View SourceShort-hand function for creating tensor of type u8.This is just an alias for Nx.tensor(tensor, type: u8).Link to this functionu16(tensor)View SourceShort-hand function for creating tensor of type u16.This is just an alias for Nx.tensor(tensor, type: u16).Link to this functionu32(tensor)View SourceShort-hand function for creating tensor of type u32.This is just an alias for Nx.tensor(tensor, type: u32).Link to this functionu64(tensor)View SourceShort-hand function for creating tensor of type u64.This is just an alias for Nx.tensor(tensor, type: u64).Functions: CumulativeLink to this functioncumulative_max(tensor, opts \\ [])View SourceReturns the cumulative maximum of elements along an axis.Options:axis - the axis to compare elements along. Defaults to 0:reverse - whether to perform accumulation in the opposite direction. Defaults to falseExamplesiex> Nx.cumulative_max(Nx.tensor([3,4,2,1]))#Nx.Tensor<s32[4][3,4,4,4]>iex> Nx.cumulative_max(Nx.tensor([[2,3,1],[1,3,2],[2,1,3]]),axis:0)#Nx.Tensor<s32[3][3][[2,3,1],[2,3,2],[2,3,3]]>iex> Nx.cumulative_max(Nx.tensor([[2,3,1],[1,3,2],[2,1,3]]),axis:1)#Nx.Tensor<s32[3][3][[2,3,3],[1,3,3],[2,2,3]]>iex> Nx.cumulative_max(Nx.tensor([[2,3,1],[1,3,2],[2,1,3]]),axis:0,reverse:true)#Nx.Tensor<s32[3][3][[2,3,3],[2,3,3],[2,1,3]]>iex> Nx.cumulative_max(Nx.tensor([[2,3,1],[1,3,2],[2,1,3]]),axis:1,reverse:true)#Nx.Tensor<s32[3][3][[3,3,1],[3,3,2],[3,3,3]]>Vectorized axesWorks the same as if the accumulation was to happen over a list of tensors.
:axis refers to the non-vectorized shape.iex> Nx.cumulative_max(Nx.tensor([[2,3,1],[1,3,2],[2,1,3]])|>Nx.vectorize(:x),axis:0)#Nx.Tensor<vectorized[x:3]s32[3][[2,3,3],[1,3,3],[2,2,3]]>Link to this functioncumulative_min(tensor, opts \\ [])View SourceReturns the cumulative minimum of elements along an axis.Options:axis - the axis to compare elements along. Defaults to 0:reverse - whether to perform accumulation in the opposite direction. Defaults to falseExamplesiex> Nx.cumulative_min(Nx.tensor([3,4,2,1]))#Nx.Tensor<s32[4][3,3,2,1]>iex> Nx.cumulative_min(Nx.tensor([[2,3,1],[1,3,2],[2,1,3]]),axis:0)#Nx.Tensor<s32[3][3][[2,3,1],[1,3,1],[1,1,1]]>iex> Nx.cumulative_min(Nx.tensor([[2,3,1],[1,3,2],[2,1,3]]),axis:1)#Nx.Tensor<s32[3][3][[2,2,1],[1,1,1],[2,1,1]]>iex> Nx.cumulative_min(Nx.tensor([[2,3,1],[1,3,2],[2,1,3]]),axis:0,reverse:true)#Nx.Tensor<s32[3][3][[1,1,1],[1,1,2],[2,1,3]]>iex> Nx.cumulative_min(Nx.tensor([[2,3,1],[1,3,2],[2,1,3]]),axis:1,reverse:true)#Nx.Tensor<s32[3][3][[1,1,1],[1,2,2],[1,1,3]]>Vectorized axesWorks the same as if the accumulation was to happen over a list of tensors.
:axis refers to the non-vectorized shape.iex> Nx.cumulative_min(Nx.tensor([[2,3,1],[1,3,2],[2,1,3]])|>Nx.vectorize(:x),axis:0)#Nx.Tensor<vectorized[x:3]s32[3][[2,2,1],[1,1,1],[2,1,1]]>Link to this functioncumulative_product(tensor, opts \\ [])View SourceReturns the cumulative product of elements along an axis.Options:axis - the axis to multiply elements along. Defaults to 0:reverse - whether to perform accumulation in the opposite direction. Defaults to falseExamplesiex> Nx.cumulative_product(Nx.tensor([1,2,3,4]))#Nx.Tensor<s32[4][1,2,6,24]>iex> Nx.cumulative_product(Nx.iota({3,3}),axis:0)#Nx.Tensor<s32[3][3][[0,1,2],[0,4,10],[0,28,80]]>iex> Nx.cumulative_product(Nx.iota({3,3}),axis:1)#Nx.Tensor<s32[3][3][[0,0,0],[3,12,60],[6,42,336]]>iex> Nx.cumulative_product(Nx.iota({3,3}),axis:0,reverse:true)#Nx.Tensor<s32[3][3][[0,28,80],[18,28,40],[6,7,8]]>iex> Nx.cumulative_product(Nx.iota({3,3}),axis:1,reverse:true)#Nx.Tensor<s32[3][3][[0,2,2],[60,20,5],[336,56,8]]>Vectorized axesWorks the same as if the accumulation was to happen over a list of tensors.
:axis refers to the non-vectorized shape.iex> Nx.cumulative_product(Nx.tensor([[2,3,0],[1,3,2],[2,1,3]])|>Nx.vectorize(:x),axis:0)#Nx.Tensor<vectorized[x:3]s32[3][[2,6,0],[1,3,6],[2,2,6]]>Link to this functioncumulative_sum(tensor, opts \\ [])View SourceReturns the cumulative sum of elements along an axis.Options:axis - the axis to sum elements along. Defaults to 0:reverse - whether to perform accumulation in the opposite direction. Defaults to falseExamplesiex> Nx.cumulative_sum(Nx.tensor([1,2,3,4]))#Nx.Tensor<s32[4][1,3,6,10]>iex> Nx.cumulative_sum(Nx.iota({3,3}),axis:0)#Nx.Tensor<s32[3][3][[0,1,2],[3,5,7],[9,12,15]]>iex> Nx.cumulative_sum(Nx.iota({3,3}),axis:1)#Nx.Tensor<s32[3][3][[0,1,3],[3,7,12],[6,13,21]]>iex> Nx.cumulative_sum(Nx.iota({3,3}),axis:0,reverse:true)#Nx.Tensor<s32[3][3][[9,12,15],[9,11,13],[6,7,8]]>iex> Nx.cumulative_sum(Nx.iota({3,3}),axis:1,reverse:true)#Nx.Tensor<s32[3][3][[3,3,2],[12,9,5],[21,15,8]]>Vectorized axesWorks the same as if the accumulation was to happen over a list of tensors.
:axis refers to the non-vectorized shape.iex> Nx.cumulative_sum(Nx.tensor([[2,3,1],[1,3,2],[2,1,3]])|>Nx.vectorize(:x),axis:0)#Nx.Tensor<vectorized[x:3]s32[3][[2,5,6],[1,4,6],[2,3,6]]>Functions: Element-wiseLink to this functionabs(tensor)View SourceComputes the absolute value of each element in the tensor.Examplesiex> Nx.abs(Nx.tensor([-2,-1,0,1,2],names:[:x]))#Nx.Tensor<s32[x:5][2,1,0,1,2]>Link to this functionacos(tensor)View SourceCalculates the inverse cosine of each element in the tensor.It is equivalent to:$$
acos(cos(z)) = z
$$Examplesiex> Nx.acos(0.10000000149011612)#Nx.Tensor<f321.4706288576126099>iex> Nx.acos(Nx.tensor([0.10000000149011612,0.5,0.8999999761581421],names:[:x]))#Nx.Tensor<f32[x:3][1.4706288576126099,1.0471975803375244,0.4510268568992615]>Link to this functionacosh(tensor)View SourceCalculates the inverse hyperbolic cosine of each element in the tensor.It is equivalent to:$$
acosh(cosh(z)) = z
$$Examplesiex> Nx.acosh(1)#Nx.Tensor<f320.0>iex> Nx.acosh(Nx.tensor([1,2,3],names:[:x]))#Nx.Tensor<f32[x:3][0.0,1.316957950592041,1.7627471685409546]>Link to this functionadd(left, right)View SourceElement-wise addition of two tensors.If a number is given, it is converted to a tensor.It will broadcast tensors whenever the dimensions do
not match and broadcasting is possible.If you're using Nx.Defn.defn/2, you can use the + operator
in place of this function: left + right.ExamplesAdding scalarsiex> Nx.add(1,2)#Nx.Tensor<s323>iex> Nx.add(1,2.2)#Nx.Tensor<f323.200000047683716>Adding a scalar to a tensoriex> Nx.add(Nx.tensor([1,2,3],names:[:data]),1)#Nx.Tensor<s32[data:3][2,3,4]>iex> Nx.add(1,Nx.tensor([1,2,3],names:[:data]))#Nx.Tensor<s32[data:3][2,3,4]>Given a float scalar converts the tensor to a float:iex> Nx.add(Nx.tensor([1,2,3],names:[:data]),1.0)#Nx.Tensor<f32[data:3][2.0,3.0,4.0]>iex> Nx.add(Nx.tensor([1.0,2.0,3.0],names:[:data]),1)#Nx.Tensor<f32[data:3][2.0,3.0,4.0]>iex> Nx.add(Nx.tensor([1.0,2.0,3.0],type::f32,names:[:data]),1)#Nx.Tensor<f32[data:3][2.0,3.0,4.0]>Unsigned tensors become signed and double their size if a
negative number is given:iex> Nx.add(Nx.tensor([0,1,2],type::u8,names:[:data]),-1)#Nx.Tensor<s16[data:3][-1,0,1]>Adding tensors of the same shapeiex> left=Nx.tensor([[1,2],[3,4]],names:[:x,:y])iex> right=Nx.tensor([[10,20],[30,40]],names:[nil,:y])iex> Nx.add(left,right)#Nx.Tensor<s32[x:2][y:2][[11,22],[33,44]]>Adding tensors with broadcastingiex> left=Nx.tensor([[1],[2]],names:[nil,:y])iex> right=Nx.tensor([[10,20]],names:[:x,nil])iex> Nx.add(left,right)#Nx.Tensor<s32[x:2][y:2][[11,21],[12,22]]>iex> left=Nx.tensor([[10,20]],names:[:x,nil])iex> right=Nx.tensor([[1],[2]],names:[nil,:y])iex> Nx.add(left,right)#Nx.Tensor<s32[x:2][y:2][[11,21],[12,22]]>iex> left=Nx.tensor([[1],[2]],names:[:x,nil])iex> right=Nx.tensor([[10,20],[30,40]])iex> Nx.add(left,right)#Nx.Tensor<s32[x:2][2][[11,21],[32,42]]>iex> left=Nx.tensor([[1,2]])iex> right=Nx.tensor([[10,20],[30,40]])iex> Nx.add(left,right)#Nx.Tensor<s32[2][2][[11,22],[31,42]]>Link to this functionasin(tensor)View SourceCalculates the inverse sine of each element in the tensor.It is equivalent to:$$
asin(sin(z)) = z
$$Examplesiex> Nx.asin(0.10000000149011612)#Nx.Tensor<f320.1001674234867096>iex> Nx.asin(Nx.tensor([0.10000000149011612,0.5,0.8999999761581421],names:[:x]))#Nx.Tensor<f32[x:3][0.1001674234867096,0.5235987901687622,1.1197694540023804]>Link to this functionasinh(tensor)View SourceCalculates the inverse hyperbolic sine of each element in the tensor.It is equivalent to:$$
asinh(sinh(z)) = z
$$Examplesiex> Nx.asinh(1)#Nx.Tensor<f320.8813735842704773>iex> Nx.asinh(Nx.tensor([1,2,3],names:[:x]))#Nx.Tensor<f32[x:3][0.8813735842704773,1.4436354637145996,1.8184465169906616]>Link to this functionatan2(left, right)View SourceElement-wise arc tangent of two tensors.If a number is given, it is converted to a tensor.It always returns a float tensor. If any of the input
tensors are not float, they are converted to f32.It will broadcast tensors whenever the dimensions do
not match and broadcasting is possible.ExamplesArc tangent between scalarsiex> Nx.atan2(1,2)#Nx.Tensor<f320.46364760398864746>Arc tangent between tensors and scalarsiex> Nx.atan2(Nx.tensor([1,2,3],names:[:data]),1)#Nx.Tensor<f32[data:3][0.7853981852531433,1.1071487665176392,1.249045729637146]>iex> Nx.atan2(1,Nx.tensor([1.0,2.0,3.0],names:[:data]))#Nx.Tensor<f32[data:3][0.7853981852531433,0.46364760398864746,0.32175055146217346]>Arc tangent between tensorsiex> neg_and_pos_zero_columns=Nx.tensor([[-0.0],[0.0]],type::f64)iex> neg_and_pos_zero_rows=Nx.tensor([-0.0,0.0],type::f64)iex> Nx.atan2(neg_and_pos_zero_columns,neg_and_pos_zero_rows)#Nx.Tensor<f64[2][2][[-3.141592653589793,-0.0],[3.141592653589793,0.0]]>Link to this functionatan(tensor)View SourceCalculates the inverse tangent of each element in the tensor.It is equivalent to:$$
atan(tan(z)) = z
$$Examplesiex> Nx.atan(0.10000000149011612)#Nx.Tensor<f320.09966865181922913>iex> Nx.atan(Nx.tensor([0.10000000149011612,0.5,0.8999999761581421],names:[:x]))#Nx.Tensor<f32[x:3][0.09966865181922913,0.46364760398864746,0.7328150868415833]>Link to this functionatanh(tensor)View SourceCalculates the inverse hyperbolic tangent of each element in the tensor.It is equivalent to:$$
atanh(tanh(z)) = z
$$Examplesiex> Nx.atanh(0.10000000149011612)#Nx.Tensor<f320.10033535212278366>iex> Nx.atanh(Nx.tensor([0.10000000149011612,0.5,0.8999999761581421],names:[:x]))#Nx.Tensor<f32[x:3][0.10033535212278366,0.5493061542510986,1.4722193479537964]>Link to this functionbitwise_and(left, right)View SourceElement-wise bitwise AND of two tensors.Only integer tensors are supported. If a float or
complex tensor is given, an error is raised.It will broadcast tensors whenever the dimensions do
not match and broadcasting is possible.If you're using Nx.Defn.defn/2, you can use the &&& operator
in place of this function: left &&& right.It does not support short-circuiting.Examplesbitwise and between scalarsiex> Nx.bitwise_and(1,0)#Nx.Tensor<s320>bitwise and between tensors and scalarsiex> Nx.bitwise_and(Nx.tensor([0,1,2],names:[:data]),1)#Nx.Tensor<s32[data:3][0,1,0]>iex> Nx.bitwise_and(Nx.tensor([0,-1,-2],names:[:data]),-1)#Nx.Tensor<s32[data:3][0,-1,-2]>bitwise and between tensorsiex> Nx.bitwise_and(Nx.tensor([0,0,1,1],names:[:data]),Nx.tensor([0,1,0,1]))#Nx.Tensor<s32[data:4][0,0,0,1]>Error casesiex> Nx.bitwise_and(Nx.tensor([0,0,1,1]),1.0)** (ArgumentError) bitwise operators expect integer tensors as inputs and outputs an integer tensor, got: {:f, 32}Link to this functionbitwise_not(tensor)View SourceApplies bitwise not to each element in the tensor.If you're using Nx.Defn.defn/2, you can use the ~~~ operator
in place of this function: ~~~tensor.Examplesiex> Nx.bitwise_not(1)#Nx.Tensor<s32-2>iex> Nx.bitwise_not(Nx.tensor([-1,0,1],type::s8,names:[:x]))#Nx.Tensor<s8[x:3][0,-1,-2]>iex> Nx.bitwise_not(Nx.tensor([0,1,254,255],type::u8,names:[:x]))#Nx.Tensor<u8[x:4][255,254,1,0]>Error casesiex> Nx.bitwise_not(Nx.tensor([0.0,1.0]))** (ArgumentError) bitwise operators expect integer tensors as inputs and outputs an integer tensor, got: {:f, 32}Link to this functionbitwise_or(left, right)View SourceElement-wise bitwise OR of two tensors.Only integer tensors are supported. If a float or
complex tensor is given, an error is raised.It will broadcast tensors whenever the dimensions do
not match and broadcasting is possible.If you're using Nx.Defn.defn/2, you can use the ||| operator
in place of this function: left ||| right.It does not support short-circuiting.Examplesbitwise or between scalarsiex> Nx.bitwise_or(1,0)#Nx.Tensor<s321>bitwise or between tensors and scalarsiex> Nx.bitwise_or(Nx.tensor([0,1,2],names:[:data]),1)#Nx.Tensor<s32[data:3][1,1,3]>iex> Nx.bitwise_or(Nx.tensor([0,-1,-2],names:[:data]),-1)#Nx.Tensor<s32[data:3][-1,-1,-1]>bitwise or between tensorsiex> Nx.bitwise_or(Nx.tensor([0,0,1,1],names:[:data]),Nx.tensor([0,1,0,1],names:[:data]))#Nx.Tensor<s32[data:4][0,1,1,1]>Error casesiex> Nx.bitwise_or(Nx.tensor([0,0,1,1]),1.0)** (ArgumentError) bitwise operators expect integer tensors as inputs and outputs an integer tensor, got: {:f, 32}Link to this functionbitwise_xor(left, right)View SourceElement-wise bitwise XOR of two tensors.Only integer tensors are supported. If a float or complex
tensor is given, an error is raised.It will broadcast tensors whenever the dimensions do
not match and broadcasting is possible.ExamplesBitwise xor between scalarsiex> Nx.bitwise_xor(1,0)#Nx.Tensor<s321>Bitwise xor and between tensors and scalarsiex> Nx.bitwise_xor(Nx.tensor([1,2,3],names:[:data]),2)#Nx.Tensor<s32[data:3][3,0,1]>iex> Nx.bitwise_xor(Nx.tensor([-1,-2,-3],names:[:data]),2)#Nx.Tensor<s32[data:3][-3,-4,-1]>Bitwise xor between tensorsiex> Nx.bitwise_xor(Nx.tensor([0,0,1,1]),Nx.tensor([0,1,0,1],names:[:data]))#Nx.Tensor<s32[data:4][0,1,1,0]>Error casesiex> Nx.bitwise_xor(Nx.tensor([0,0,1,1]),1.0)** (ArgumentError) bitwise operators expect integer tensors as inputs and outputs an integer tensor, got: {:f, 32}Link to this functioncbrt(tensor)View SourceCalculates the cube root of each element in the tensor.It is equivalent to:$$
cbrt(z) = z^{\frac{1}{3}}
$$Examplesiex> Nx.cbrt(1)#Nx.Tensor<f321.0>iex> Nx.cbrt(Nx.tensor([1,2,3],names:[:x]))#Nx.Tensor<f32[x:3][1.0,1.2599210739135742,1.4422495365142822]>Link to this functionceil(tensor)View SourceCalculates the ceil of each element in the tensor.If a non-floating tensor is given, it is returned as is.
If a floating tensor is given, then we apply the operation,
but keep its type.Examplesiex> Nx.ceil(Nx.tensor([-1,0,1],names:[:x]))#Nx.Tensor<s32[x:3][-1,0,1]>iex> Nx.ceil(Nx.tensor([-1.5,-0.5,0.5,1.5],names:[:x]))#Nx.Tensor<f32[x:4][-1.0,-0.0,1.0,2.0]>Link to this functionclip(tensor, min, max)View SourceClips the values of the tensor on the closed
interval [min, max].You can pass a tensor to min or max as long
as the tensor has a scalar shape.Examplesiex> t=Nx.tensor([[1,2,3],[4,5,6]],names:[:x,:y])iex> Nx.clip(t,2,4)#Nx.Tensor<s32[x:2][y:3][[2,2,3],[4,4,4]]>iex> t=Nx.tensor([[1,2,3],[4,5,6]],names:[:x,:y])iex> Nx.clip(t,2.0,3)#Nx.Tensor<f32[x:2][y:3][[2.0,2.0,3.0],[3.0,3.0,3.0]]>iex> t=Nx.tensor([[1,2,3],[4,5,6]],names:[:x,:y])iex> Nx.clip(t,Nx.tensor(2.0),Nx.max(1.0,3.0))#Nx.Tensor<f32[x:2][y:3][[2.0,2.0,3.0],[3.0,3.0,3.0]]>iex> t=Nx.tensor([[1.0,2.0,3.0],[4.0,5.0,6.0]],names:[:x,:y])iex> Nx.clip(t,2,6.0)#Nx.Tensor<f32[x:2][y:3][[2.0,2.0,3.0],[4.0,5.0,6.0]]>iex> t=Nx.tensor([[1.0,2.0,3.0],[4.0,5.0,6.0]],type::f32,names:[:x,:y])iex> Nx.clip(t,1,4)#Nx.Tensor<f32[x:2][y:3][[1.0,2.0,3.0],[4.0,4.0,4.0]]>Vectorized tensorsOnly the main input tensor is allowed to be vectorized. min and max threshold tensors
must be unvectorized scalar tensors.iex> t=Nx.tensor([[1.0,2.0,3.0],[4.0,5.0,6.0]],type::f32,names:[nil,:y])|>Nx.vectorize(:x)iex> Nx.clip(t,1,4)#Nx.Tensor<vectorized[x:2]f32[y:3][[1.0,2.0,3.0],[4.0,4.0,4.0]]>Link to this functioncomplex(real, imag)View SourceConstructs a complex tensor from two equally-shaped tensors.Does not accept complex tensors as inputs.Examplesiex> Nx.complex(Nx.tensor(1),Nx.tensor(2))#Nx.Tensor<c641.0+2.0i>iex> Nx.complex(Nx.tensor([1,2]),Nx.tensor([3,4]))#Nx.Tensor<c64[2][1.0+3.0i,2.0+4.0i]>Link to this functionconjugate(tensor)View SourceCalculates the complex conjugate of each element in the tensor.If $$
z = a + bi = r e^\theta
$$, $$
conjugate(z) = z^* = a - bi =  r e^{-\theta}
$$Examplesiex> Nx.conjugate(Complex.new(1,2))#Nx.Tensor<c641.0-2.0i>iex> Nx.conjugate(1)#Nx.Tensor<c641.0-0.0i>iex> Nx.conjugate(Nx.tensor([Complex.new(1,2),Complex.new(2,-4)]))#Nx.Tensor<c64[2][1.0-2.0i,2.0+4.0i]>Link to this functioncos(tensor)View SourceCalculates the cosine of each element in the tensor.It is equivalent to:$$
cos(z) = \frac{e^{iz} + e^{-iz}}{2}
$$Examplesiex> Nx.cos(1)#Nx.Tensor<f320.5403022766113281>iex> Nx.cos(Nx.tensor([1,2,3],names:[:x]))#Nx.Tensor<f32[x:3][0.5403022766113281,-0.416146844625473,-0.9899924993515015]>Link to this functioncosh(tensor)View SourceCalculates the hyperbolic cosine of each element in the tensor.It is equivalent to:$$
cosh(z) = \frac{e^z + e^{-z}}{2}
$$Examplesiex> Nx.cosh(1)#Nx.Tensor<f321.5430806875228882>iex> Nx.cosh(Nx.tensor([1,2,3],names:[:x]))#Nx.Tensor<f32[x:3][1.5430806875228882,3.762195587158203,10.067662239074707]>Link to this functioncount_leading_zeros(tensor)View SourceCounts the number of leading zeros of each element in the tensor.Examplesiex> Nx.count_leading_zeros(1)#Nx.Tensor<s3231>iex> Nx.count_leading_zeros(-1)#Nx.Tensor<s320>iex> Nx.count_leading_zeros(Nx.tensor([0,0xF,0xFF,0xFFFF],names:[:x]))#Nx.Tensor<s32[x:4][32,28,24,16]>iex> Nx.count_leading_zeros(Nx.tensor([0xF0000000,0x0F000000],names:[:x]))#Nx.Tensor<s32[x:2][0,4]>iex> Nx.count_leading_zeros(Nx.tensor([0,0xF,0xFF,0xFFFF],type::s64,names:[:x]))#Nx.Tensor<s64[x:4][64,60,56,48]>iex> Nx.count_leading_zeros(Nx.tensor([0,0xF,0xFF,0xFFFF],type::s16,names:[:x]))#Nx.Tensor<s16[x:4][16,12,8,0]>iex> Nx.count_leading_zeros(Nx.tensor([0,1,2,4,8,16,32,64,-1,-128],type::s8,names:[:x]))#Nx.Tensor<s8[x:10][8,7,6,5,4,3,2,1,0,0]>iex> Nx.count_leading_zeros(Nx.tensor([0,1,2,4,8,16,32,64,128],type::u8,names:[:x]))#Nx.Tensor<u8[x:9][8,7,6,5,4,3,2,1,0]>Error casesiex> Nx.count_leading_zeros(Nx.tensor([0.0,1.0]))** (ArgumentError) bitwise operators expect integer tensors as inputs and outputs an integer tensor, got: {:f, 32}Link to this functiondivide(left, right)View SourceElement-wise division of two tensors.If a number is given, it is converted to a tensor.It always returns a float tensor. If any of the input
tensors are not float, they are converted to f32.
Division by zero raises, but it may trigger undefined
behaviour on some compilers.It will broadcast tensors whenever the dimensions do
not match and broadcasting is possible.If you're using Nx.Defn.defn/2, you can use the / operator
in place of this function: left / right.ExamplesDividing scalarsiex> Nx.divide(1,2)#Nx.Tensor<f320.5>Dividing tensors and scalarsiex> Nx.divide(Nx.tensor([1,2,3],names:[:data]),1)#Nx.Tensor<f32[data:3][1.0,2.0,3.0]>iex> Nx.divide(1,Nx.tensor([1.0,2.0,3.0],names:[:data]))#Nx.Tensor<f32[data:3][1.0,0.5,0.3333333432674408]>Dividing tensorsiex> left=Nx.tensor([[1],[2]],names:[:x,nil])iex> right=Nx.tensor([[10,20]],names:[nil,:y])iex> Nx.divide(left,right)#Nx.Tensor<f32[x:2][y:2][[0.10000000149011612,0.05000000074505806],[0.20000000298023224,0.10000000149011612]]>iex> left=Nx.tensor([[1],[2]],type::s8)iex> right=Nx.tensor([[10,20]],type::s8,names:[:x,:y])iex> Nx.divide(left,right)#Nx.Tensor<f32[x:2][y:2][[0.10000000149011612,0.05000000074505806],[0.20000000298023224,0.10000000149011612]]>iex> left=Nx.tensor([[1],[2]],type::f32,names:[:x,nil])iex> right=Nx.tensor([[10,20]],type::f32,names:[nil,:y])iex> Nx.divide(left,right)#Nx.Tensor<f32[x:2][y:2][[0.10000000149011612,0.05000000074505806],[0.20000000298023224,0.10000000149011612]]>Link to this functionequal(left, right)View SourceElement-wise equality comparison of two tensors.If a number is given, it is converted to a tensor.It will broadcast tensors whenever the dimensions do
not match and broadcasting is possible.If you're using Nx.Defn.defn/2, you can use the == operator
in place of this function: left == right.ExamplesComparison of scalarsiex> Nx.equal(1,2)#Nx.Tensor<u80>Comparison of tensors and scalarsiex> Nx.equal(1,Nx.tensor([1,2,3],names:[:data]))#Nx.Tensor<u8[data:3][1,0,0]>Comparison of tensorsiex> left=Nx.tensor([1,2,3],names:[:data])iex> right=Nx.tensor([1,2,5])iex> Nx.equal(left,right)#Nx.Tensor<u8[data:3][1,1,0]>iex> left=Nx.tensor([[1.0,2.0,3.0],[4.0,5.0,6.0]],names:[:x,nil])iex> right=Nx.tensor([1,2,3])iex> Nx.equal(left,right)#Nx.Tensor<u8[x:2][3][[1,1,1],[0,0,0]]>Link to this functionerf(tensor)View SourceCalculates the error function of each element in the tensor.It is equivalent to:$$
erf(z) = \frac{2}{\sqrt{\pi}} \int_{0}^{z} e^{-t^2}dt
$$Examplesiex> Nx.erf(1)#Nx.Tensor<f320.8427007794380188>iex> Nx.erf(Nx.tensor([1,2,3],names:[:x]))#Nx.Tensor<f32[x:3][0.8427007794380188,0.9953222870826721,0.9999778866767883]>Link to this functionerf_inv(tensor)View SourceCalculates the inverse error function of each element in the tensor.It is equivalent to:$$
erf\text{\textunderscore}inv(erf(z)) = z
$$Examplesiex> Nx.erf_inv(0.10000000149011612)#Nx.Tensor<f320.08885598927736282>iex> Nx.erf_inv(Nx.tensor([0.10000000149011612,0.5,0.8999999761581421],names:[:x]))#Nx.Tensor<f32[x:3][0.08885598927736282,0.4769362807273865,1.163087010383606]>Link to this functionerfc(tensor)View SourceCalculates the one minus error function of each element in the tensor.It is equivalent to:$$
erfc(z) = 1 - erf(z)
$$Examplesiex> Nx.erfc(1)#Nx.Tensor<f320.15729920566082>iex> Nx.erfc(Nx.tensor([1,2,3],names:[:x]))#Nx.Tensor<f32[x:3][0.15729920566082,0.004677734803408384,2.2090496713644825e-5]>Link to this functionexp(tensor)View SourceCalculates the exponential of each element in the tensor.It is equivalent to:$$
exp(z) = e^z
$$Examplesiex> Nx.exp(1)#Nx.Tensor<f322.7182817459106445>iex> Nx.exp(Nx.tensor([1,2,3],names:[:x]))#Nx.Tensor<f32[x:3][2.7182817459106445,7.389056205749512,20.08553695678711]>Link to this functionexpm1(tensor)View SourceCalculates the exponential minus one of each element in the tensor.It is equivalent to:$$
expm1(z) = e^z - 1
$$Examplesiex> Nx.expm1(1)#Nx.Tensor<f321.718281865119934>iex> Nx.expm1(Nx.tensor([1,2,3],names:[:x]))#Nx.Tensor<f32[x:3][1.718281865119934,6.389056205749512,19.08553695678711]>Link to this functionfill(tensor, value, opts \\ [])View SourceReplaces every value in tensor with value.The returned tensor has the same shape, names and vectorized axes
as the given one. The type will be computed based on the type of
tensor and value. You can also pass a :type option to change
this behaviour.Options:type - sets the type of the returned tensor. If one is not
given, it is automatically inferred based on the inputs, with
type promotionsExamplesiex> tensor=Nx.iota({2,2})iex> Nx.fill(tensor,5)#Nx.Tensor<s32[2][2][[5,5],[5,5]]>iex> tensor=Nx.iota({2,2})|>Nx.vectorize(:x)iex> Nx.fill(tensor,5)#Nx.Tensor<vectorized[x:2]s32[2][[5,5],[5,5]]>iex> tensor=Nx.iota({2,2})iex> Nx.fill(tensor,5,type::u8)#Nx.Tensor<u8[2][2][[5,5],[5,5]]>Type promotionsType promotions should happen automatically, with the resulting type being the combination
of the tensor type and the value type.iex> tensor=Nx.iota({2,2})iex> Nx.fill(tensor,5.0)#Nx.Tensor<f32[2][2][[5.0,5.0],[5.0,5.0]]>Link to this functionfloor(tensor)View SourceCalculates the floor of each element in the tensor.If a non-floating tensor is given, it is returned as is.
If a floating tensor is given, then we apply the operation,
but keep its type.Examplesiex> Nx.floor(Nx.tensor([-1,0,1],names:[:x]))#Nx.Tensor<s32[x:3][-1,0,1]>iex> Nx.floor(Nx.tensor([-1.5,-0.5,0.5,1.5],names:[:x]))#Nx.Tensor<f32[x:4][-2.0,-1.0,0.0,1.0]>Link to this functiongreater(left, right)View SourceElement-wise greater than comparison of two tensors.If a number is given, it is converted to a tensor.It will broadcast tensors whenever the dimensions do
not match and broadcasting is possible.If you're using Nx.Defn.defn/2, you can use the > operator
in place of this function: left > right.ExamplesComparison of scalarsiex> Nx.greater(1,2)#Nx.Tensor<u80>Comparison of tensors and scalarsiex> Nx.greater(1,Nx.tensor([1,2,3],names:[:data]))#Nx.Tensor<u8[data:3][0,0,0]>Comparison of tensorsiex> left=Nx.tensor([1,2,3],names:[:data])iex> right=Nx.tensor([1,2,2])iex> Nx.greater(left,right)#Nx.Tensor<u8[data:3][0,0,1]>iex> left=Nx.tensor([[1.0,2.0,3.0],[4.0,5.0,6.0]],names:[:x,:y])iex> right=Nx.tensor([1,2,3])iex> Nx.greater(left,right)#Nx.Tensor<u8[x:2][y:3][[0,0,0],[1,1,1]]>Link to this functiongreater_equal(left, right)View SourceElement-wise greater than or equal comparison of two tensors.If a number is given, it is converted to a tensor.It will broadcast tensors whenever the dimensions do
not match and broadcasting is possible.If you're using Nx.Defn.defn/2, you can use the >= operator
in place of this function: left >= right.ExamplesComparison of scalarsiex> Nx.greater_equal(1,2)#Nx.Tensor<u80>Comparison of tensors and scalarsiex> Nx.greater_equal(1,Nx.tensor([1,2,3],names:[:data]))#Nx.Tensor<u8[data:3][1,0,0]>Comparison of tensorsiex> left=Nx.tensor([1,2,3],names:[:data])iex> right=Nx.tensor([1,2,2])iex> Nx.greater_equal(left,right)#Nx.Tensor<u8[data:3][1,1,1]>iex> left=Nx.tensor([[1.0,2.0,3.0],[4.0,5.0,6.0]],names:[:x,:y])iex> right=Nx.tensor([1,2,3])iex> Nx.greater_equal(left,right)#Nx.Tensor<u8[x:2][y:3][[1,1,1],[1,1,1]]>Link to this functionimag(tensor)View SourceReturns the imaginary component of each entry in a complex tensor
as a floating point tensor.Examplesiex> Nx.imag(Complex.new(1,2))#Nx.Tensor<f322.0>iex> Nx.imag(Nx.tensor(1))#Nx.Tensor<f320.0>iex> Nx.imag(Nx.tensor(1,type::bf16))#Nx.Tensor<bf160.0>iex> Nx.imag(Nx.tensor([Complex.new(1,2),Complex.new(2,-4)]))#Nx.Tensor<f32[2][2.0,-4.0]>Link to this functionis_infinity(tensor)View SourceDetermines if each element in tensor is Inf or -Inf.For complex tensors, if either of the components is infinity,
the entry is deemed infinity as well.Examplesiex> Nx.is_infinity(Nx.tensor([:infinity,:nan,:neg_infinity,1,0]))#Nx.Tensor<u8[5][1,0,1,0,0]>iex> Nx.is_infinity(Nx.tensor([:infinity,1,Complex.new(0,:infinity),:neg_infinity]))#Nx.Tensor<u8[4][1,0,1,1]>iex> Nx.is_infinity(Nx.tensor([1,0]))#Nx.Tensor<u8[2][0,0]>Link to this functionis_nan(tensor)View SourceDetermines if each element in tensor is a NaN.For complex tensors, if either of the components is NaN,
the entry is deemed NaN as well.Examplesiex> Nx.is_nan(Nx.tensor([:nan,1,0]))#Nx.Tensor<u8[3][1,0,0]>iex> Nx.is_nan(Nx.tensor([:nan,:infinity,Complex.new(0,:nan)]))#Nx.Tensor<u8[3][1,0,1]>iex> Nx.is_nan(Nx.tensor([1,0]))#Nx.Tensor<u8[2][0,0]>Link to this functionleft_shift(left, right)View SourceElement-wise left shift of two tensors.Only integer tensors are supported. If a float or complex
tensor is given, an error is raised. If the right side
is negative, it will raise, but it may trigger undefined
behaviour on some compilers.It will broadcast tensors whenever the dimensions do
not match and broadcasting is possible. If the number of
shifts are negative, Nx's default backend will raise,
but it may trigger undefined behaviour in other backends.If you're using Nx.Defn.defn/2, you can use the <<< operator
in place of this function: left <<< right.ExamplesLeft shift between scalarsiex> Nx.left_shift(1,0)#Nx.Tensor<s321>Left shift between tensors and scalarsiex> Nx.left_shift(Nx.tensor([1,2,3],names:[:data]),2)#Nx.Tensor<s32[data:3][4,8,12]>Left shift between tensorsiex> left=Nx.tensor([1,1,-1,-1],names:[:data])iex> right=Nx.tensor([1,2,3,4],names:[:data])iex> Nx.left_shift(left,right)#Nx.Tensor<s32[data:4][2,4,-8,-16]>Error casesiex> Nx.left_shift(Nx.tensor([0,0,1,1]),1.0)** (ArgumentError) bitwise operators expect integer tensors as inputs and outputs an integer tensor, got: {:f, 32}Link to this functionless(left, right)View SourceElement-wise less than comparison of two tensors.If a number is given, it is converted to a tensor.It will broadcast tensors whenever the dimensions do
not match and broadcasting is possible.If you're using Nx.Defn.defn/2, you can use the < operator
in place of this function: left < right.ExamplesComparison of scalarsiex> Nx.less(1,2)#Nx.Tensor<u81>Comparison of tensors and scalarsiex> Nx.less(1,Nx.tensor([1,2,3],names:[:data]))#Nx.Tensor<u8[data:3][0,1,1]>Comparison of tensorsiex> Nx.less(Nx.tensor([1,2,1]),Nx.tensor([1,2,2],names:[:data]))#Nx.Tensor<u8[data:3][0,0,1]>iex> Nx.less(Nx.tensor([[1.0,2.0,3.0],[4.0,2.0,1.0]],names:[:x,:y]),Nx.tensor([1,2,3]))#Nx.Tensor<u8[x:2][y:3][[0,0,0],[0,0,1]]>Link to this functionless_equal(left, right)View SourceElement-wise less than or equal comparison of two tensors.If a number is given, it is converted to a tensor.It will broadcast tensors whenever the dimensions do
not match and broadcasting is possible.If you're using Nx.Defn.defn/2, you can use the <= operator
in place of this function: left <= right.ExamplesComparison of scalarsiex> Nx.less_equal(1,2)#Nx.Tensor<u81>Comparison of tensors and scalarsiex> Nx.less_equal(1,Nx.tensor([1,2,3],names:[:data]))#Nx.Tensor<u8[data:3][1,1,1]>Comparison of tensorsiex> left=Nx.tensor([1,2,3],names:[:data])iex> right=Nx.tensor([1,2,2])iex> Nx.less_equal(left,right)#Nx.Tensor<u8[data:3][1,1,0]>iex> left=Nx.tensor([[1.0,2.0,3.0],[4.0,5.0,6.0]])iex> right=Nx.tensor([1,2,3],names:[:y])iex> Nx.less_equal(left,right)#Nx.Tensor<u8[2][y:3][[1,1,1],[0,0,0]]>Link to this functionlog1p(tensor)View SourceCalculates the natural log plus one of each element in the tensor.It is equivalent to:$$
log1p(z) = log(z + 1)
$$Examplesiex> Nx.log1p(1)#Nx.Tensor<f320.6931471824645996>iex> Nx.log1p(Nx.tensor([1,2,3],names:[:x]))#Nx.Tensor<f32[x:3][0.6931471824645996,1.0986123085021973,1.3862943649291992]>Link to this functionlog2(tensor)View SourceCalculates the element-wise logarithm of a tensor with base 2.Examplesiex> Nx.log2(2)#Nx.Tensor<f321.0>iex> Nx.log2(Nx.tensor([1,2,4,8]))#Nx.Tensor<f32[4][0.0,1.0,2.0,3.0]>Link to this functionlog10(tensor)View SourceCalculates the element-wise logarithm of a tensor with base 10.Examplesiex> Nx.log10(10)#Nx.Tensor<f321.0>iex> Nx.log10(Nx.tensor([1,10,100,1000]))#Nx.Tensor<f32[4][0.0,1.0,2.0,3.0]>Link to this functionlog(tensor)View SourceCalculates the natural log of each element in the tensor.It is equivalent to:$$
log(z) = ln(z),\quad \text{if z} \in \Reals
$$$$
log(z) = ln(r) + i\theta,\quad\text{if }z = re^{i\theta} \in \Complex
$$Examplesiex> Nx.log(1)#Nx.Tensor<f320.0>iex> Nx.log(Nx.tensor([1,2,3],names:[:x]))#Nx.Tensor<f32[x:3][0.0,0.6931471824645996,1.0986123085021973]>Link to this functionlog(tensor, base)View SourceCalculates the element-wise logarithm of a tensor with given base.Examplesiex> Nx.log(2,2)#Nx.Tensor<f321.0>iex> Nx.log(Nx.tensor([3,9,27,81]),3)#Nx.Tensor<f32[4][1.0,2.0,3.0,4.0]>Link to this functionlogical_and(left, right)View SourceElement-wise logical and of two tensors.Zero is considered false, any other number is considered
true.It will broadcast tensors whenever the dimensions do
not match and broadcasting is possible.If you're using Nx.Defn.defn/2, you can use the and operator
in place of this function: left and right.Examplesiex> Nx.logical_and(1,Nx.tensor([-1,0,1],names:[:data]))#Nx.Tensor<u8[data:3][1,0,1]>iex> left=Nx.tensor([-1,0,1],names:[:data])iex> right=Nx.tensor([[-1],[0],[1]])iex> Nx.logical_and(left,right)#Nx.Tensor<u8[3][data:3][[1,0,1],[0,0,0],[1,0,1]]>iex> left=Nx.tensor([-1.0,0.0,1.0],names:[:data])iex> right=Nx.tensor([[-1],[0],[1]])iex> Nx.logical_and(left,right)#Nx.Tensor<u8[3][data:3][[1,0,1],[0,0,0],[1,0,1]]>Link to this functionlogical_not(tensor)View SourceElement-wise logical not a tensor.Zero is considered false, any other number is considered
true.If you're using Nx.Defn.defn/2, you can use the not operator
in place of this function: not tensor.Examplesiex> Nx.logical_not(Nx.tensor([-1,0,1],names:[:data]))#Nx.Tensor<u8[data:3][0,1,0]>iex> Nx.logical_not(Nx.tensor([-1.0,0.0,1.0],names:[:data]))#Nx.Tensor<u8[data:3][0,1,0]>Link to this functionlogical_or(left, right)View SourceElement-wise logical or of two tensors.Zero is considered false, any other number is considered
true.It will broadcast tensors whenever the dimensions do
not match and broadcasting is possible.If you're using Nx.Defn.defn/2, you can use the or operator
in place of this function: left or right.Examplesiex> Nx.logical_or(0,Nx.tensor([-1,0,1],names:[:data]))#Nx.Tensor<u8[data:3][1,0,1]>iex> left=Nx.tensor([-1,0,1],names:[:data])iex> right=Nx.tensor([[-1],[0],[1]])iex> Nx.logical_or(left,right)#Nx.Tensor<u8[3][data:3][[1,1,1],[1,0,1],[1,1,1]]>iex> left=Nx.tensor([-1.0,0.0,1.0],names:[:data])iex> right=Nx.tensor([[-1],[0],[1]])iex> Nx.logical_or(left,right)#Nx.Tensor<u8[3][data:3][[1,1,1],[1,0,1],[1,1,1]]>Link to this functionlogical_xor(left, right)View SourceElement-wise logical xor of two tensors.Zero is considered false, any other number is considered
true.It will broadcast tensors whenever the dimensions do
not match and broadcasting is possible.Examplesiex> Nx.logical_xor(0,Nx.tensor([-1,0,1],names:[:data]))#Nx.Tensor<u8[data:3][1,0,1]>iex> left=Nx.tensor([-1,0,1],names:[:data])iex> right=Nx.tensor([[-1],[0],[1]])iex> Nx.logical_xor(left,right)#Nx.Tensor<u8[3][data:3][[0,1,0],[1,0,1],[0,1,0]]>iex> left=Nx.tensor([-1.0,0.0,1.0],names:[:data])iex> right=Nx.tensor([[-1],[0],[1]])iex> Nx.logical_xor(left,right)#Nx.Tensor<u8[3][data:3][[0,1,0],[1,0,1],[0,1,0]]>Link to this functionmax(left, right)View SourceElement-wise maximum of two tensors.If a number is given, it is converted to a tensor.It will broadcast tensors whenever the dimensions do
not match and broadcasting is possible.If you're using Nx.Defn.defn/2, you can use the max/2 function
in place of this function: max(left, right).ExamplesMax between scalarsiex> Nx.max(1,2)#Nx.Tensor<s322>Max between tensors and scalarsiex> Nx.max(Nx.tensor([1,2,3],names:[:data]),1)#Nx.Tensor<s32[data:3][1,2,3]>iex> Nx.max(1,Nx.tensor([1.0,2.0,3.0],names:[:data]))#Nx.Tensor<f32[data:3][1.0,2.0,3.0]>Max between tensorsiex> left=Nx.tensor([[1],[2]],names:[:x,:y])iex> right=Nx.tensor([[10,20]])iex> Nx.max(left,right)#Nx.Tensor<s32[x:2][y:2][[10,20],[10,20]]>iex> left=Nx.tensor([[1],[2]],type::s8,names:[:x,nil])iex> right=Nx.tensor([[10,20]],type::s8)iex> Nx.max(left,right)#Nx.Tensor<s8[x:2][2][[10,20],[10,20]]>iex> left=Nx.tensor([[1],[2]],type::f32,names:[:x,nil])iex> right=Nx.tensor([[10,20]],type::f32,names:[nil,:y])iex> Nx.max(left,right)#Nx.Tensor<f32[x:2][y:2][[10.0,20.0],[10.0,20.0]]>Link to this functionmin(left, right)View SourceElement-wise minimum of two tensors.If a number is given, it is converted to a tensor.It will broadcast tensors whenever the dimensions do
not match and broadcasting is possible.If you're using Nx.Defn.defn/2, you can use the min/2 function
in place of this function: min(left, right).ExamplesMin between scalarsiex> Nx.min(1,2)#Nx.Tensor<s321>Min between tensors and scalarsiex> Nx.min(Nx.tensor([1,2,3],names:[:data]),1)#Nx.Tensor<s32[data:3][1,1,1]>iex> Nx.min(1,Nx.tensor([1.0,2.0,3.0],names:[:data]))#Nx.Tensor<f32[data:3][1.0,1.0,1.0]>Min between tensorsiex> left=Nx.tensor([[1],[2]],names:[:x,nil])iex> right=Nx.tensor([[10,20]])iex> Nx.min(left,right)#Nx.Tensor<s32[x:2][2][[1,1],[2,2]]>iex> left=Nx.tensor([[1],[2]],type::s8,names:[:x,:y])iex> right=Nx.tensor([[10,20]],type::s8)iex> Nx.min(left,right)#Nx.Tensor<s8[x:2][y:2][[1,1],[2,2]]>iex> left=Nx.tensor([[1],[2]],type::f32,names:[:x,nil])iex> right=Nx.tensor([[10,20]],type::f32,names:[nil,:y])iex> Nx.min(left,right)#Nx.Tensor<f32[x:2][y:2][[1.0,1.0],[2.0,2.0]]>Link to this functionmultiply(left, right)View SourceElement-wise multiplication of two tensors.If a number is given, it is converted to a tensor.It will broadcast tensors whenever the dimensions do
not match and broadcasting is possible.If you're using Nx.Defn.defn/2, you can use the * operator
operator in place of this function as left * right.ExamplesMultiplying scalarsiex> Nx.multiply(1,2)#Nx.Tensor<s322>Multiplying tensors and scalarsiex> Nx.multiply(Nx.tensor([1,2,3],names:[:data]),1)#Nx.Tensor<s32[data:3][1,2,3]>iex> Nx.multiply(1,Nx.tensor([1.0,2.0,3.0],names:[:data]))#Nx.Tensor<f32[data:3][1.0,2.0,3.0]>Multiplying tensorsiex> left=Nx.tensor([[1],[2]],names:[:x,:y])iex> right=Nx.tensor([[10,20]],names:[:x,:y])iex> Nx.multiply(left,right)#Nx.Tensor<s32[x:2][y:2][[10,20],[20,40]]>iex> left=Nx.tensor([[1],[2]],type::s8,names:[:x,nil])iex> right=Nx.tensor([[10,20]],type::s8,names:[nil,:y])iex> Nx.multiply(left,right)#Nx.Tensor<s8[x:2][y:2][[10,20],[20,40]]>iex> left=Nx.tensor([[1],[2]],type::f32,names:[nil,:y])iex> right=Nx.tensor([[10,20]],type::f32,names:[:x,nil])iex> Nx.multiply(left,right)#Nx.Tensor<f32[x:2][y:2][[10.0,20.0],[20.0,40.0]]>Link to this functionnegate(tensor)View SourceNegates each element in the tensor.If you're using Nx.Defn.defn/2, you can use the - unary operator
in place of this function: -tensor.Examplesiex> Nx.negate(1)#Nx.Tensor<s32-1>iex> Nx.negate(Nx.tensor([-1,0,1]))#Nx.Tensor<s32[3][1,0,-1]>iex> Nx.negate(Nx.tensor([1.0,2.0,3.0],type::f32))#Nx.Tensor<f32[3][-1.0,-2.0,-3.0]>If an unsigned tensor is given, it works as bitwise_not:iex> Nx.negate(Nx.tensor([0,1,2],type::u8,names:[:x]))#Nx.Tensor<u8[x:3][0,255,254]>Link to this functionnot_equal(left, right)View SourceElement-wise not-equal comparison of two tensors.If a number is given, it is converted to a tensor.It will broadcast tensors whenever the dimensions do
not match and broadcasting is possible.If you're using Nx.Defn.defn/2, you can use the != operator
in place of this function: left != right.ExamplesComparison of scalarsiex> Nx.not_equal(1,2)#Nx.Tensor<u81>Comparison of tensor and scalariex> Nx.not_equal(Nx.tensor([1,2,3],names:[:data]),Nx.tensor(1))#Nx.Tensor<u8[data:3][0,1,1]>Comparison of tensorsiex> left=Nx.tensor([1,1,2])iex> right=Nx.tensor([1,2,3],names:[:data])iex> Nx.not_equal(left,right)#Nx.Tensor<u8[data:3][0,1,1]>iex> left=Nx.tensor([[1,4,2],[4,5,6]],names:[:x,:y])iex> right=Nx.tensor([[1,3,2],[4,2,1]],names:[:x,:y])iex> Nx.not_equal(left,right)#Nx.Tensor<u8[x:2][y:3][[0,1,0],[0,1,1]]>Link to this functionphase(tensor)View SourceCalculates the complex phase angle of each element in the tensor.
$$
phase(z) = atan2(b, a), z = a + bi \in \Complex
$$Examplesiex> Nx.phase(Complex.new(1,2))#Nx.Tensor<f321.1071487665176392>iex> Nx.phase(1)#Nx.Tensor<f320.0>iex> importNx,only:[sigil_VEC:2]iex> Nx.phase(~VEC[1+2i-2+1i])#Nx.Tensor<f32[2][1.1071487665176392,2.677945137023926]>Link to this functionpopulation_count(tensor)View SourceComputes the bitwise population count of each element in the tensor.Examplesiex> Nx.population_count(1)#Nx.Tensor<s321>iex> Nx.population_count(-128)#Nx.Tensor<s3225>iex> Nx.population_count(Nx.tensor([0,1,254,255],names:[:x]))#Nx.Tensor<s32[x:4][0,1,7,8]>iex> Nx.population_count(Nx.tensor([0,1,126,127,-1,-127,-128],type::s8,names:[:x]))#Nx.Tensor<s8[x:7][0,1,6,7,8,2,1]>Error casesiex> Nx.population_count(Nx.tensor([0.0,1.0]))** (ArgumentError) bitwise operators expect integer tensors as inputs and outputs an integer tensor, got: {:f, 32}Link to this functionpow(left, right)View SourceElement-wise power of two tensors.If a number is given, it is converted to a tensor.It will broadcast tensors whenever the dimensions do
not match and broadcasting is possible.If both tensors are integers and the exponent is
negative, it will raise, but it may trigger undefined
behaviour on some compilers.ExamplesPower of scalarsiex> Nx.pow(2,4)#Nx.Tensor<s3216>Power of tensors and scalarsiex> Nx.pow(Nx.tensor([1,2,3],names:[:data]),2)#Nx.Tensor<s32[data:3][1,4,9]>iex> Nx.pow(2,Nx.tensor([1.0,2.0,3.0],names:[:data]))#Nx.Tensor<f32[data:3][2.0,4.0,8.0]>Power of tensorsiex> Nx.pow(Nx.tensor([[2],[3]],names:[:x,nil]),Nx.tensor([[4,5]],names:[nil,:y]))#Nx.Tensor<s32[x:2][y:2][[16,32],[81,243]]>Link to this functionquotient(left, right)View SourceElement-wise integer division of two tensors.If a number is given, it is converted to a tensor.It always returns an integer tensor. Input tensors and
numbers must be integer types. Division by zero raises,
but it may trigger undefined behaviour on some compilers.It will broadcast tensors whenever the dimensions do
not match and broadcasting is possible.Caveat for gradThe grad operation is not supported for quotient/2.
Since integer division is, by definition, a closed operation
for the set of integers and grad involves floating points,
grad is undefined.If you need to support gradients, you might consider using
floor division, but beware of precision errors caused by
floating points:a|>Nx.divide(b)|>Nx.floor()ExamplesInteger dividing scalarsiex> Nx.quotient(11,2)#Nx.Tensor<s325>Integer dividing tensors and scalarsiex> Nx.quotient(Nx.tensor([2,4,5],names:[:data]),2)#Nx.Tensor<s32[data:3][1,2,2]>iex> Nx.quotient(10,Nx.tensor([1,2,3],names:[:data]))#Nx.Tensor<s32[data:3][10,5,3]>Dividing tensorsiex> left=Nx.tensor([[10,20]],names:[nil,:y])iex> right=Nx.tensor([[1],[2]],names:[:x,nil])iex> Nx.quotient(left,right)#Nx.Tensor<s32[x:2][y:2][[10,20],[5,10]]>iex> left=Nx.tensor([[10,20]],type::s8,names:[:x,:y])iex> right=Nx.tensor([[1],[2]],type::s8)iex> Nx.quotient(left,right)#Nx.Tensor<s8[x:2][y:2][[10,20],[5,10]]>iex> left=Nx.tensor([[10,20]],type::u8,names:[:x,:y])iex> right=Nx.tensor([[1],[2]],type::u32)iex> Nx.quotient(left,right)#Nx.Tensor<u32[x:2][y:2][[10,20],[5,10]]>Link to this functionreal(tensor)View SourceReturns the real component of each entry in a complex tensor
as a floating point tensor.Examplesiex> Nx.real(Complex.new(1,2))#Nx.Tensor<f321.0>iex> Nx.real(Nx.tensor(1))#Nx.Tensor<f321.0>iex> Nx.real(Nx.tensor(1,type::bf16))#Nx.Tensor<bf161.0>iex> Nx.real(Nx.tensor([Complex.new(1,2),Complex.new(2,-4)]))#Nx.Tensor<f32[2][1.0,2.0]>Link to this functionremainder(left, right)View SourceElement-wise remainder of two tensors.If a number is given, it is converted to a tensor.It will broadcast tensors whenever the dimensions do
not match and broadcasting is possible.If you're using Nx.Defn.defn/2, you can use the rem/2 function
in place of this function: rem(left, right).ExamplesRemainder of scalarsiex> Nx.remainder(1,2)#Nx.Tensor<s321>Remainder of tensors and scalarsiex> Nx.remainder(Nx.tensor([1,2,3],names:[:data]),2)#Nx.Tensor<s32[data:3][1,0,1]>iex> Nx.remainder(2,Nx.tensor([1.0,2.0,3.0],names:[:data]))#Nx.Tensor<f32[data:3][0.0,0.0,2.0]>Remainder of tensorsiex> left=Nx.tensor([[10],[20]],names:[:x,:y])iex> right=Nx.tensor([[3,4]],names:[nil,:y])iex> Nx.remainder(left,right)#Nx.Tensor<s32[x:2][y:2][[1,2],[2,0]]>Remainder involving negative valuesIf given a negative value as the right operand, the operation
will return the negative image of the remainder.For the example below, note that in modulo-10, adding 20 shouldn't
change the result, but in this case it does because the sign changes.iex> left=Nx.tensor(-11,type::s8)iex> right=Nx.tensor(10,type::u8)iex> Nx.remainder(left,right)#Nx.Tensor<s16-1>iex> Nx.remainder(Nx.add(left,Nx.tensor(20,type::s8)),right)#Nx.Tensor<s169>iex> positive_left=Nx.tensor(9,type::u8)iex> Nx.remainder(positive_left,right)#Nx.Tensor<u89>iex> Nx.remainder(Nx.add(positive_left,Nx.tensor(20,type::u8)),right)#Nx.Tensor<u89>Link to this functionright_shift(left, right)View SourceElement-wise right shift of two tensors.Only integer tensors are supported. If a float or complex
tensor is given, an error is raised. If the right side
is negative, it will raise, but it may trigger undefined
behaviour on some compilers.It performs an arithmetic shift if the tensor is made of
signed integers, it performs a logical shift otherwise.
In other words, it preserves the sign for signed integers.It will broadcast tensors whenever the dimensions do
not match and broadcasting is possible. If the number of
shifts are negative, Nx's default backend will raise,
but it may trigger undefined behaviour in other backends.If you're using Nx.Defn.defn/2, you can use the >>> operator
in place of this function: left >>> right.ExamplesRight shift between scalarsiex> Nx.right_shift(1,0)#Nx.Tensor<s321>Right shift between tensors and scalarsiex> Nx.right_shift(Nx.tensor([2,4,8],names:[:data]),2)#Nx.Tensor<s32[data:3][0,1,2]>Right shift between tensorsiex> left=Nx.tensor([16,32,-64,-128],names:[:data])iex> right=Nx.tensor([1,2,3,4])iex> Nx.right_shift(left,right)#Nx.Tensor<s32[data:4][8,8,-8,-8]>Error casesiex> Nx.right_shift(Nx.tensor([0,0,1,1]),1.0)** (ArgumentError) bitwise operators expect integer tensors as inputs and outputs an integer tensor, got: {:f, 32}Link to this functionround(tensor)View SourceCalculates the round (away from zero) of each element in the tensor.If a non-floating tensor is given, it is returned as is.
If a floating tensor is given, then we apply the operation,
but keep its type.Examplesiex> Nx.round(Nx.tensor([-1,0,1],names:[:x]))#Nx.Tensor<s32[x:3][-1,0,1]>iex> Nx.round(Nx.tensor([-1.5,-0.5,0.5,1.5],names:[:x]))#Nx.Tensor<f32[x:4][-2.0,-1.0,1.0,2.0]>Link to this functionrsqrt(tensor)View SourceCalculates the reverse square root of each element in the tensor.It is equivalent to:$$
rsqrt(z) = \frac{1}{\sqrt{z}}
$$Examplesiex> Nx.rsqrt(1)#Nx.Tensor<f321.0>iex> Nx.rsqrt(Nx.tensor([1,2,3],names:[:x]))#Nx.Tensor<f32[x:3][1.0,0.7071067690849304,0.5773502588272095]>Link to this functionselect(pred, on_true, on_false)View SourceConstructs a tensor from two tensors, based on a predicate.The resulting tensor is built by evaluating each element of
pred and returning either the corresponding element from
on_true or on_false.pred must either be 1 or 0 or a tensor of predicates
with a shape that matches the largest shape between s1 or s2.If the shape of on_true or on_false do not match the shape of
pred, attempts to broadcast both so they match the shape of pred.ExamplesWhen the first argument is a scalar:iex> Nx.select(1,Nx.tensor([1,2,3],names:[:x]),Nx.tensor([4,5,6],names:[:x]))#Nx.Tensor<s32[x:3][1,2,3]>iex> Nx.select(0,Nx.tensor([1,2,3],names:[:y]),Nx.tensor([4,5,6],names:[:y]))#Nx.Tensor<s32[y:3][4,5,6]>iex> Nx.select(0,Nx.tensor([[1,2]],names:[:x,:y]),Nx.tensor([[3],[4]],names:[:x,:y]))#Nx.Tensor<s32[x:2][y:2][[3,3],[4,4]]>When the first argument is a tensor:iex> Nx.select(Nx.tensor([0,1,0],names:[:x]),Nx.tensor([1,2,3],names:[:y]),Nx.tensor([4,5,6],names:[:z]))#Nx.Tensor<s32[x:3][4,2,6]>iex> x=Nx.tensor([2,4,6],names:[:x])iex> y=Nx.tensor([3,2,1])iex> Nx.select(Nx.greater(x,y),Nx.tensor([2,4,6],names:[:i]),Nx.tensor([1,3,5],names:[:j]))#Nx.Tensor<s32[x:3][1,4,6]>iex> x=Nx.tensor([2,4,6,8,10],names:[:x])iex> y=Nx.tensor([1,6,2,11,2],names:[:x])iex> Nx.select(Nx.greater(x,y),Nx.tensor(2),Nx.tensor([1,3,5,7,9],names:[:x]))#Nx.Tensor<s32[x:5][2,3,2,7,2]>If the tensor has other values, any non-zero value is considered true:iex> Nx.select(Nx.tensor([0,1,2],type::u8),Nx.tensor([0,0,0]),Nx.tensor([1,1,1]))#Nx.Tensor<s32[3][1,0,0]>iex> Nx.select(Nx.tensor([0,1,0]),Nx.tensor([1,1,1]),Nx.tensor([2.0,2.0,2.0]))#Nx.Tensor<f32[3][2.0,1.0,2.0]>Vectorized tensorsVectorized and non-vectorized tensors can be mixed-and-matched on all three inputs.iex> pred=Nx.tensor([[0,1,0],[1,1,0]])|>Nx.vectorize(:x)iex> on_true=1iex> on_false=Nx.tensor([2,3])|>Nx.vectorize(:y)iex> Nx.select(pred,on_true,on_false)#Nx.Tensor<vectorized[x:2][y:2]s32[3][[[2,1,2],[3,1,3]],[[1,1,2],[1,1,3]]]>  In the next example, notice that even though the pred input
  is scalar, because we're dealing with vectorized inputs, some
  broadcasting still occurs.iex> pred=1iex> on_true=Nx.tensor([1,2,3])|>Nx.vectorize(:x)iex> on_false=Nx.tensor([4,5])|>Nx.vectorize(:y)iex> Nx.select(pred,on_true,on_false)#Nx.Tensor<vectorized[x:3][y:2]s32[[1,1],[2,2],[3,3]]>Finally, broadcasting will also occur if more than one input share
the same vectorized axes, but one of them presents size 1iex> pred=Nx.tensor([1,0,0])|>Nx.vectorize(:x)iex> on_true=Nx.tensor([[2]])|>Nx.vectorize(:x)|>Nx.vectorize(:y)iex> on_false=Nx.tensor([3,4])|>Nx.vectorize(:y)iex> Nx.select(pred,on_true,on_false)#Nx.Tensor<vectorized[x:3][y:2]s32[[2,2],[3,4],[3,4]]>Link to this functionsigmoid(tensor)View SourceCalculates the sigmoid of each element in the tensor.It is equivalent to:$$
sigmoid(z) = \frac{1}{1 + e^{-z}}
$$Examplesiex> Nx.sigmoid(1)#Nx.Tensor<f320.7310585975646973>iex> Nx.sigmoid(Nx.tensor([1,2,3],names:[:x]))#Nx.Tensor<f32[x:3][0.7310585975646973,0.8807970881462097,0.9525741338729858]>Link to this functionsign(tensor)View SourceComputes the sign of each element in the tensor.If a number is less than zero, it returns -1.
If a number is more than zero, it returns 1.
Otherwise it returns zero (which may either be
positive or negative for floats).Examplesiex> Nx.sign(Nx.tensor([-2,-1,0,1,2],names:[:x]))#Nx.Tensor<s32[x:5][-1,-1,0,1,1]>Link to this functionsin(tensor)View SourceCalculates the sine of each element in the tensor.It is equivalent to:$$
sin(z) = \frac{e^{iz} - e^{-iz}}{2i}
$$Examplesiex> Nx.sin(1)#Nx.Tensor<f320.8414709568023682>iex> Nx.sin(Nx.tensor([1,2,3],names:[:x]))#Nx.Tensor<f32[x:3][0.8414709568023682,0.9092974066734314,0.14112000167369843]>Link to this functionsinh(tensor)View SourceCalculates the hyperbolic sine of each element in the tensor.It is equivalent to:$$
sinh(z) = \frac{e^z - e^{-z}}{2}
$$Examplesiex> Nx.sinh(1)#Nx.Tensor<f321.175201177597046>iex> Nx.sinh(Nx.tensor([1,2,3],names:[:x]))#Nx.Tensor<f32[x:3][1.175201177597046,3.6268603801727295,10.017874717712402]>Link to this functionsqrt(tensor)View SourceCalculates the square root of each element in the tensor.It is equivalent to:$$
sqrt(z) = \sqrt{z}
$$Examplesiex> Nx.sqrt(1)#Nx.Tensor<f321.0>iex> Nx.sqrt(Nx.tensor([1,2,3],names:[:x]))#Nx.Tensor<f32[x:3][1.0,1.4142135381698608,1.7320507764816284]>Link to this functionsubtract(left, right)View SourceElement-wise subtraction of two tensors.If a number is given, it is converted to a tensor.It will broadcast tensors whenever the dimensions do
not match and broadcasting is possible.If you're using Nx.Defn.defn/2, you can use the - operator
in place of this function: left - right.ExamplesSubtracting scalarsiex> Nx.subtract(1,2)#Nx.Tensor<s32-1>Subtracting tensors and scalarsiex> Nx.subtract(Nx.tensor([1,2,3],names:[:data]),1)#Nx.Tensor<s32[data:3][0,1,2]>iex> Nx.subtract(1,Nx.tensor([1.0,2.0,3.0],names:[:data]))#Nx.Tensor<f32[data:3][0.0,-1.0,-2.0]>Subtracting tensorsiex> left=Nx.tensor([[1],[2]],names:[:x,:y])iex> right=Nx.tensor([[10,20]],names:[:x,:y])iex> Nx.subtract(left,right)#Nx.Tensor<s32[x:2][y:2][[-9,-19],[-8,-18]]>iex> left=Nx.tensor([[1],[2]],type::s8,names:[:x,nil])iex> right=Nx.tensor([[10,20]],type::s8,names:[nil,:y])iex> Nx.subtract(left,right)#Nx.Tensor<s8[x:2][y:2][[-9,-19],[-8,-18]]>iex> left=Nx.tensor([[1],[2]],type::f32,names:[nil,:y])iex> right=Nx.tensor([[10,20]],type::f32,names:[:x,nil])iex> Nx.subtract(left,right)#Nx.Tensor<f32[x:2][y:2][[-9.0,-19.0],[-8.0,-18.0]]>Link to this functiontan(tensor)View SourceCalculates the tangent of each element in the tensor.It is equivalent to:$$
tan(z) = \frac{sin(z)}{cos(z)}
$$Examplesiex> Nx.tan(1)#Nx.Tensor<f321.5574077367782593>iex> Nx.tan(Nx.tensor([1,2,3],names:[:x]))#Nx.Tensor<f32[x:3][1.5574077367782593,-2.185039758682251,-0.14254654943943024]>Link to this functiontanh(tensor)View SourceCalculates the hyperbolic tangent of each element in the tensor.It is equivalent to:$$
sinh(z) = \frac{e^z - e^{-z}}{e^z + e^{-z}}
$$Examplesiex> Nx.tanh(1)#Nx.Tensor<f320.7615941762924194>iex> Nx.tanh(Nx.tensor([1,2,3],names:[:x]))#Nx.Tensor<f32[x:3][0.7615941762924194,0.9640275835990906,0.9950547814369202]>Functions: IndexedLink to this functiongather(tensor, indices, opts \\ [])View SourceBuilds a new tensor by taking individual values from the original
tensor at the given indices.Indices must be a tensor where the last dimension is usually of the
same size as the tensor rank. Each entry in indices will be
part of the results. If the last dimension of indices is less than
the tensor rank, then a multidimensional tensor is gathered and
spliced into the result.Options:axes - controls to which dimensions of tensor
each element in the last dimension of indexes applies to.
It defaults so the first element in indexes apply to the first
axis, the second to the second, and so on. It must be a sorted
list of axes and be of the same size as the last dimension of
the indexes tensor.ExamplesGathering scalarsiex> t=Nx.tensor([[1,2],[3,4]])iex> Nx.gather(t,Nx.tensor([[1,1],[0,1],[1,0]]))#Nx.Tensor<s32[3][4,2,3]>iex> t=Nx.tensor([[1,2],[3,4]])iex> Nx.gather(t,Nx.tensor([[[1,1],[0,0]],[[1,0],[0,1]]]))#Nx.Tensor<s32[2][2][[4,1],[3,2]]>iex> t=Nx.tensor([[[1,2],[11,12]],[[101,102],[111,112]]])iex> Nx.gather(t,Nx.tensor([[0,0,0],[0,1,1],[1,1,1]]))#Nx.Tensor<s32[3][1,12,112]>Gathering subsetsiex> t=Nx.tensor([[1,2,3],[3,4,5]])iex> Nx.gather(t,Nx.tensor([[1],[0]]))#Nx.Tensor<s32[2][3][[3,4,5],[1,2,3]]>The :axes option controls which dimensions the indexes point to,
this can be useful, for example, to access columns instead of rows.
Note can also access the same index several times:iex> t=Nx.tensor([[1,2,3],[4,5,6]])iex> Nx.gather(t,Nx.tensor([[1],[0],[2],[1]]),axes:[1])#Nx.Tensor<s32[4][2][[2,5],[1,4],[3,6],[2,5]]>The overall output shape will have the format of the indices shape
(except the last element) followed by all non-indexed dimensions of
the tensor. Here is a more complex example:iex> t=Nx.iota({2,1,3})iex> Nx.gather(t,Nx.tensor([[[1],[0],[2]]]),axes:[2])#Nx.Tensor<s32[1][3][2][1][[[[1],[4]],[[0],[3]],[[2],[5]]]]>Vectorized tensorstensor and indices have their vectorized axes broadcast together,
and then the operation takes place normally, with :axis and indices
having their values in reference to the input shape.iex> t=Nx.tensor([[[1,2],[11,12]],[[101,102],[111,112]]])|>Nx.vectorize(:x)iex> idx=Nx.tensor([[[0,0],[0,1]],[[1,0],[1,1]]])|>Nx.vectorize(:x)iex> Nx.gather(t,idx)#Nx.Tensor<vectorized[x:2]s32[2][[1,2],[111,112]]>And with vectorized broadcasting:iex> t=Nx.tensor([[[1,2],[11,12]],[[101,102],[111,112]]])|>Nx.vectorize(:x)iex> idx=Nx.tensor([[[0,0],[0,1]],[[1,0],[1,1]]])|>Nx.vectorize(:y)iex> Nx.gather(t,idx)#Nx.Tensor<vectorized[x:2][y:2]s32[2][[[1,2],[11,12]],[[101,102],[111,112]]]>Error casesiex> Nx.gather(Nx.tensor([[1,2],[3,4]]),Nx.tensor([[0,0]],type::f32))** (ArgumentError) indices must be an integer tensor, got {:f, 32}Link to this functionindexed_add(target, indices, updates, opts \\ [])View SourcePerforms an indexed add operation on the target tensor,
adding the updates into the corresponding indices positions.This operation is the grad for gather/2 and gather-like operations such as
take/3 and take_along_axis/3.indices must be a fully qualified tensor of shape {n, Nx.rank(target)}, with n
being an arbitrary number of indices, while updates must have a compatible {n} shape.See also: indexed_add/3, gather/2, take/3, take_along_axis/3Options:axes - controls which dimensions the indexes apply to.
It must be a sorted list of axes and be of the same size
as the second (last) dimension of the indexes tensor.
It defaults to the leading axes of the tensor.ExamplesAdding a single entry as a scalarAs a shorthand notation, rank-1 indices can be used for updating a single entry:iex> Nx.indexed_add(Nx.tensor([[1],[2]]),Nx.tensor([1,0]),8)#Nx.Tensor<s32[2][1][[1],[10]]>Adding multiple scalar entriesiex> t=Nx.iota({1,2,3})#Nx.Tensor<s32[1][2][3][[[0,1,2],[3,4,5]]]>iex> indices=Nx.tensor([[0,0,0],[0,1,1],[0,0,0],[0,0,2],[0,1,2]])iex> updates=Nx.tensor([1,3,1,-2,5])iex> Nx.indexed_add(t,indices,updates)#Nx.Tensor<s32[1][2][3][[[2,1,0],[3,7,10]]]>Type promotionsType promotions should happen automatically, with the resulting type being the combination
of the target type and the updates type.iex> Nx.indexed_add(Nx.tensor([1.0]),Nx.tensor([[0],[0]]),Nx.tensor([1,1]))#Nx.Tensor<f32[1][3.0]>iex> Nx.indexed_add(Nx.tensor([1]),Nx.tensor([[0],[0]]),Nx.tensor([1.0,1.0]))#Nx.Tensor<f32[1][3.0]>iex> Nx.indexed_add(Nx.tensor([1],type::s32),Nx.tensor([[0],[0]]),Nx.tensor([1,1],type::s64))#Nx.Tensor<s64[1][3]>Vectorized tensorsAll of the inputs can be vectorized. The function will broadcast along the vectorized axes
before calculating the results.iex> x=Nx.tensor([[0,10],[10,20]])|>Nx.vectorize(:x)iex> idx=Nx.tensor([[[0],[0]],[[0],[1]],[[1],[1]]])|>Nx.vectorize(:y)iex> Nx.indexed_add(x,idx,Nx.tensor([1,1]))#Nx.Tensor<vectorized[x:2][y:3]s32[2][[[2,10],[1,11],[0,12]],[[12,20],[11,21],[10,22]]]>Error casesiex> Nx.indexed_add(Nx.tensor([[1],[2]]),Nx.tensor([[[1,2,3]]]),Nx.tensor([0]))** (ArgumentError) indices must be a rank 1 or 2 tensor, got: 3iex> Nx.indexed_add(Nx.tensor([[1],[2]]),Nx.tensor([[1,2]]),Nx.tensor([0,1]))** (ArgumentError) expected the leading axis of indices ({1, 2}) and leading axis of updates ({2}) to matchiex> Nx.indexed_add(Nx.tensor([[1,2,3]]),Nx.tensor([[0]]),Nx.tensor([[1,2,3,4,5]]))** (ArgumentError) axis (1) of updates ({1, 5}) must be less than or equal to the axis (1) of {1, 3})Link to this functionindexed_put(target, indices, updates, opts \\ [])View SourcePuts individual values from updates into the given tensor at the corresponding indices.indices must be a fully qualified tensor of shape {n, i}, with n being an arbitrary
number of indices, while updates must have a compatible {n, ...j} shape, such that
i + j = rank(tensor).In case of repeating indices, the result is non-determinstic, since the operation happens
in parallel when running on devices such as the GPU.See also: indexed_add/3, put_slice/3.Options:axes - controls which dimensions the indexes apply to.
It must be a sorted list of axes and be of the same size
as the second (last) dimension of the indexes tensor.
It defaults to the leading axes of the tensor.ExamplesStoring a single entry as a scalarAs a shorthand notation, rank-1 indices can be used for updating a single entry:iex> Nx.indexed_put(Nx.tensor([[1],[2]]),Nx.tensor([1,0]),10)#Nx.Tensor<s32[2][1][[1],[10]]>Storing multiple scalar entries scalarsiex> Nx.indexed_put(Nx.tensor([0,0,0]),Nx.tensor([[1],[2]]),Nx.tensor([2,4]))#Nx.Tensor<s32[3][0,2,4]>iex> Nx.indexed_put(Nx.tensor([0,0,0]),Nx.tensor([[1],[2]]),Nx.tensor([3,4]))#Nx.Tensor<s32[3][0,3,4]>iex> t=Nx.iota({1,2,3})#Nx.Tensor<s32[1][2][3][[[0,1,2],[3,4,5]]]>iex> indices=Nx.tensor([[0,0,0],[0,1,1],[0,0,2]])iex> updates=Nx.tensor([1,3,-2])iex> Nx.indexed_put(t,indices,updates)#Nx.Tensor<s32[1][2][3][[[1,1,-2],[3,3,5]]]>Storing non-scalars on a given dimensioniex> t=Nx.iota({1,3,2})#Nx.Tensor<s32[1][3][2][[[0,1],[2,3],[4,5]]]>iex> indices=Nx.tensor([[0,0],[0,2]])iex> updates=Nx.tensor([[0,10],[40,50]])iex> Nx.indexed_put(t,indices,updates)#Nx.Tensor<s32[1][3][2][[[0,10],[2,3],[40,50]]]>The :axes option controls which dimensions the indexes apply to.
It must be a sorted list of axes. All non-listed axes are taken as
the update dimensions:iex> t=Nx.iota({1,2,3})#Nx.Tensor<s32[1][2][3][[[0,1,2],[3,4,5]]]>iex> indices=Nx.tensor([[0,0],[0,2]])iex> updates=Nx.tensor([[0,30],[20,50]])iex> Nx.indexed_put(t,indices,updates,axes:[0,2])#Nx.Tensor<s32[1][2][3][[[0,1,20],[30,4,50]]]>Type promotionsType promotions should happen automatically, with the resulting type being the combination
of the target type and the updates type.iex> Nx.indexed_put(Nx.tensor([1.0]),Nx.tensor([[0]]),Nx.tensor([3]))#Nx.Tensor<f32[1][3.0]>iex> Nx.indexed_put(Nx.tensor([1]),Nx.tensor([[0]]),Nx.tensor([3.0]))#Nx.Tensor<f32[1][3.0]>iex> Nx.indexed_put(Nx.tensor([1],type::s32),Nx.tensor([[0]]),Nx.tensor([3],type::s64))#Nx.Tensor<s64[1][3]>Vectorized tensorsAll of the inputs can be vectorized. The function will broadcast along the vectorized axes
before calculating the results.iex> x=Nx.tensor([[0,10],[10,20]])|>Nx.vectorize(:x)iex> idx=Nx.tensor([[[0],[0]],[[0],[1]],[[1],[1]]])|>Nx.vectorize(:y)iex> Nx.indexed_put(x,idx,Nx.tensor([1,1]))#Nx.Tensor<vectorized[x:2][y:3]s32[2][[[1,10],[1,1],[0,1]],[[1,20],[1,1],[10,1]]]>Error casesiex> Nx.indexed_put(Nx.tensor([[1],[2]]),Nx.tensor([[[1,2,3]]]),Nx.tensor([0]))** (ArgumentError) indices must be a rank 1 or 2 tensor, got: 3iex> Nx.indexed_put(Nx.tensor([[1],[2]]),Nx.tensor([[1,2]]),Nx.tensor([0,1]))** (ArgumentError) expected the leading axis of indices ({1, 2}) and leading axis of updates ({2}) to matchiex> Nx.indexed_put(Nx.iota({1,2,3}),Nx.tensor([[0,1,2]]),10,axes:[1,0,2])** (ArgumentError) :axes must be an ordered listLink to this functionput_slice(tensor, start_indices, slice)View SourcePuts the given slice into the given tensor at the given
start_indices.The given slice must be of the same rank as tensor. Each axis
must be less than or equal to the size to the equivalent axis
in the tensor.The number of elements in start_indices should match the
rank of the tensor.See also: indexed_add/3, indexed_put/3.Examplesiex> t=Nx.tensor([0,1,2,3,4])iex> Nx.put_slice(t,[2],Nx.tensor([5,6]))#Nx.Tensor<s32[5][0,1,5,6,4]>iex> t=Nx.tensor([[1,2,3],[4,5,6]])iex> Nx.put_slice(t,[0,1],Nx.tensor([[7,8],[9,10]]))#Nx.Tensor<s32[2][3][[1,7,8],[4,9,10]]>Similar to slice/3, dynamic start indexes are also supported:iex> t=Nx.tensor([[1,2,3],[4,5,6]])iex> Nx.put_slice(t,[Nx.tensor(0),Nx.tensor(1)],Nx.tensor([[10.0,11.0]]))#Nx.Tensor<f32[2][3][[1.0,10.0,11.0],[4.0,5.0,6.0]]>Also similar to slice/3, if start_index + slice_dimension > dimension,
the start index will be clipped in order to put the whole slice:iex> t=Nx.tensor([[1,2,3],[4,5,6]])iex> Nx.put_slice(t,[1,1],Nx.tensor([[7,8],[9,10]]))#Nx.Tensor<s32[2][3][[1,7,8],[4,9,10]]>Vectorized tensorsThe both tensor to be sliced and the slices can be vectorized,
but indices must be non-vectorized.iex> t=Nx.tensor([[1,2,3,4],[5,6,7,8]])|>Nx.vectorize(:x)iex> slice=Nx.tensor([[10,20],[30,40]])|>Nx.vectorize(:y)iex> Nx.put_slice(t,[2],slice)#Nx.Tensor<vectorized[x:2][y:2]s32[4][[[1,2,10,20],[1,2,30,40]],[[5,6,10,20],[5,6,30,40]]]>Link to this functionslice(tensor, start_indices, lengths, opts \\ [])View SourceSlices a tensor from start_indices with lengths.You can optionally provide a stride to specify the amount
of stride in each dimension.Both start indices and lengths must match the rank of the
input tensor shape. All start indexes must be greater than
or equal to zero. All lengths must be strictly greater than
zero. If start_index + length exceeds the tensor dimension,
the start_index will be clipped in order to guarantee the
length is the requested one. See the "Clipping" section below.It is possible for start_indices to be a list of tensors.
However, lengths must always be a list of integers. If you
want to specify a tensor as the list of indices, see take/3.If the :strides is given, it must be strictly greater than zero.
The resulting tensor will have the shape of length unless
:strides are given.It is not possible to slice in reverse. See gather/2,
slice_along_axis/4, take/3, and take_along_axis/3 for other ways
to retrieve values from a tensor.Examplesiex> Nx.slice(Nx.tensor([1,2,3,4,5,6]),[0],[3])#Nx.Tensor<s32[3][1,2,3]>iex> Nx.slice(Nx.tensor([1,2,3,4,5,6]),[0],[6],strides:[2])#Nx.Tensor<s32[3][1,3,5]>iex> Nx.slice(Nx.tensor([[1,2],[3,4],[5,6]]),[0,0],[3,2],strides:[2,1])#Nx.Tensor<s32[2][2][[1,2],[5,6]]>Strides can also be a number that applies to all dimensions:iex> t=Nx.tensor([[1,2],[3,4],[5,6]])iex> Nx.slice(t,[0,0],[3,2],strides:2)#Nx.Tensor<s32[2][1][[1],[5]]>A more complex example:iex> t=Nx.iota({2,15,30})iex> Nx.slice(t,[0,4,11],[2,3,9],strides:[2,1,3])#Nx.Tensor<s32[1][3][3][[[131,134,137],[161,164,167],[191,194,197]]]>Tensors as start_indicesThe start_indices list can be made of scalar tensors:iex> Nx.slice(Nx.tensor([[1,2,3],[4,5,6]]),[Nx.tensor(1),Nx.tensor(2)],[1,1])#Nx.Tensor<s32[1][1][[6]]>iex> t=Nx.tensor([...> [0.0,0.0,0.0,0.0,0.0,0.0,0.0],...> [0.0,0.0,0.0,0.0,0.0,0.0,0.0],...> [1.0,1.0,1.0,1.0,1.0,1.0,1.0],...> [1.0,1.0,1.0,1.0,1.0,1.0,1.0],...> [1.0,1.0,1.0,1.0,1.0,1.0,1.0],...> [1.0,1.0,1.0,1.0,1.0,1.0,1.0]...> ])iex> Nx.slice(t,[Nx.tensor(0),Nx.tensor(0)],[6,7],strides:[5,3])#Nx.Tensor<f32[2][3][[0.0,0.0,0.0],[1.0,1.0,1.0]]>Clippingslice/3 will always guarantee the return tensor has the
given lengths. See the following example:iex> Nx.slice(Nx.iota({3,3}),[2,2],[1,1])#Nx.Tensor<s32[1][1][[8]]>In the example above, start_index + length <= dimension,
so there is no clipping. However, if the start_index + length
is to exceed the dimension, the index will be clipped in order
to guarantee the given lengths:iex> Nx.slice(Nx.iota({3,3}),[2,2],[2,2])#Nx.Tensor<s32[2][2][[4,5],[7,8]]>This also applies when the start index is given by tensors:iex> Nx.slice(Nx.iota({3,3}),[Nx.tensor(2),Nx.tensor(2)],[2,2])#Nx.Tensor<s32[2][2][[4,5],[7,8]]>Vectorized tensorsBoth the tensor to be sliced and the indices can be vectorized.iex> Nx.slice(Nx.iota({3,3},vectorized_axes:[x:2]),[0,Nx.tensor(1)],[2,2])#Nx.Tensor<vectorized[x:2]s32[2][2][[[1,2],[4,5]],[[1,2],[4,5]]]>iex> idx=Nx.tensor([0,1,10])|>Nx.vectorize(:i)iex> Nx.slice(Nx.iota({3,3}),[0,idx],[2,2])#Nx.Tensor<vectorized[i:3]s32[2][2][[[0,1],[3,4]],[[1,2],[4,5]],[[1,2],[4,5]]]>Error casesiex> Nx.slice(Nx.tensor([[1,2,3],[4,5,6]]),[Nx.tensor([1,2]),Nx.tensor(1)],[1,1])** (ArgumentError) index must be scalar, got shape {2} for axis 0iex> Nx.slice(Nx.tensor([[1,2,3],[4,5,6]]),[Nx.tensor(1.0),Nx.tensor(0)],[1,1])** (ArgumentError) index must be integer type, got {:f, 32} for axis 0Link to this functionslice_along_axis(tensor, start_index, len, opts \\ [])View SourceSlices a tensor along the given axis.You can optionally provide a stride to specify the amount
of stride in along the given dimension.Start index must be greater than or equal to zero. It can be an
integer or a scalar tensor. Length must be strictly greater than
zero. start_index + length must not exceed the respective tensor
dimension.The axis will be normalized with the dimensions and names of the
given tensor.If the :strides is given, it must be strictly greater than zero.It is not possible to slice in reverse. See gather/2, slice/3,
take/3, and take_along_axis/3 for other ways to retrieve values
from a tensor.Options:axis - The axis along which to take the values from. Defaults to 0.:strides - The stride to slice the axis along of. Defaults to 1.Examplesiex> Nx.slice_along_axis(Nx.iota({5,2}),1,2,axis:0)#Nx.Tensor<s32[2][2][[2,3],[4,5]]>iex> Nx.slice_along_axis(Nx.iota({2,5}),1,2,axis:1)#Nx.Tensor<s32[2][2][[1,2],[6,7]]>iex> Nx.slice_along_axis(Nx.iota({2,5},names:[:x,:y]),0,1,axis::x)#Nx.Tensor<s32[x:1][y:5][[0,1,2,3,4]]>iex> Nx.slice_along_axis(Nx.iota({2,5},names:[:x,:y]),Nx.tensor(0),1,axis::x)#Nx.Tensor<s32[x:1][y:5][[0,1,2,3,4]]>iex> Nx.slice_along_axis(Nx.iota({2,5}),0,3,axis:-1,strides:2)#Nx.Tensor<s32[2][2][[0,2],[5,7]]>Vectorized tensorsSlices are taken over each vectorized entry.
The start_index cannot be vectorized.iex> t=Nx.iota({2,5},vectorized_axes:[x:2])iex> Nx.slice_along_axis(t,0,3,axis:1,strides:2)#Nx.Tensor<vectorized[x:2]s32[2][2][[[0,2],[5,7]],[[0,2],[5,7]]]>Link to this functionsplit(tensor, split, opts \\ [])View SourceSplit a tensor into train and test subsets.split must be defined so that there are no empty result tensors.
This means that split must be:an integer such that 0 < split and split < axis_sizea float such that 0.0 < split and ceil(axis_size * split) < axis_sizeOptions:axis - The axis along which to split the tensor. Defaults to 0.ExamplesAll examples will operate on the same tensor so that it's easier to compare different configurations.iex> t=Nx.tensor([[0,1,2,3],[4,5,6,7],[8,9,10,11]])iex> {left,right}=Nx.split(t,2,axis:0)iex> left#Nx.Tensor<s32[2][4][[0,1,2,3],[4,5,6,7]]>iex> right#Nx.Tensor<s32[1][4][[8,9,10,11]]>iex> {left,right}=Nx.split(t,2,axis:1)iex> left#Nx.Tensor<s32[3][2][[0,1],[4,5],[8,9]]>iex> right#Nx.Tensor<s32[3][2][[2,3],[6,7],[10,11]]>iex> t=Nx.tensor([[0,1,2,3],[4,5,6,7],[8,9,10,11]])iex> {left,right}=Nx.split(t,0.5,axis:0)iex> left#Nx.Tensor<s32[2][4][[0,1,2,3],[4,5,6,7]]>iex> right#Nx.Tensor<s32[1][4][[8,9,10,11]]>iex> {left,right}=Nx.split(t,0.75,axis:1)iex> left#Nx.Tensor<s32[3][3][[0,1,2],[4,5,6],[8,9,10]]>iex> right#Nx.Tensor<s32[3][1][[3],[7],[11]]>Negative indices are also accepted, in the same fashion as Enum.split/2.iex> t=Nx.tensor([1,2,3,4])iex> {left,right}=Nx.split(t,-1)iex> left#Nx.Tensor<s32[3][1,2,3]>iex> right#Nx.Tensor<s32[1][4]>Link to this functiontake(tensor, indices, opts \\ [])View SourceTakes and concatenates slices along an axis.Intuitively speaking, take/3 reorders tensor slices along
the given axis based on the given indices, possibly duplicating
and removing slices.Passing a multi-dimensional indices tensor only affects the
resulting shape. Specifically, the given axis in the input shape
gets replaced with the indices shape.See gather/2, slice/3, slice_along_axis/4, and take_along_axis/3
for other ways to retrieve values from a tensor.Options:axis - an axis to take tensor slices over. Defaults to 0.Examplesiex> t=Nx.tensor([[1,2],[3,4]])iex> Nx.take(t,Nx.tensor([1,0,1]))#Nx.Tensor<s32[3][2][[3,4],[1,2],[3,4]]>iex> t=Nx.tensor([[1,2],[3,4]])iex> Nx.take(t,Nx.tensor([1,0,1]),axis:1)#Nx.Tensor<s32[2][3][[2,1,2],[4,3,4]]>iex> t=Nx.tensor([[1,2],[3,4]],names:[:x,:y])iex> Nx.take(t,Nx.tensor([1,0,1]),axis::y)#Nx.Tensor<s32[x:2][y:3][[2,1,2],[4,3,4]]>iex> t=Nx.tensor([[[1,2],[11,12]],[[101,102],[111,112]]])iex> Nx.take(t,Nx.tensor([1,0,1]),axis:1)#Nx.Tensor<s32[2][3][2][[[11,12],[1,2],[11,12]],[[111,112],[101,102],[111,112]]]>Multi-dimensional indices tensor:iex> t=Nx.tensor([[1,2],[11,12]])iex> Nx.take(t,Nx.tensor([[0,0],[1,1],[0,0]]),axis:1)#Nx.Tensor<s32[2][3][2][[[1,1],[2,2],[1,1]],[[11,11],[12,12],[11,11]]]>iex> t=Nx.tensor([[[1,2],[11,12]],[[101,102],[111,112]]])iex> Nx.take(t,Nx.tensor([[0,0,0],[1,1,1],[0,0,0]]),axis:1)#Nx.Tensor<s32[2][3][3][2][[[[1,2],[1,2],[1,2]],[[11,12],[11,12],[11,12]],[[1,2],[1,2],[1,2]]],[[[101,102],[101,102],[101,102]],[[111,112],[111,112],[111,112]],[[101,102],[101,102],[101,102]]]]>Vectorized tensorstensor and indices have their vectorized axes broadcast together,
and then the operation takes place normally, with :axis and indices
having their values in reference to the input shape.iex> t=Nx.tensor([[1,2],[11,12]])iex> idx=Nx.tensor([0,1,0])|>Nx.vectorize(:x)iex> Nx.take(t,idx)#Nx.Tensor<vectorized[x:3]s32[2][[1,2],[11,12],[1,2]]>iex> t=Nx.tensor([[[1,2]],[[11,12]]])|>Nx.vectorize(:x)iex> idx=Nx.tensor([0,1])iex> Nx.take(t,idx,axis:1)#Nx.Tensor<vectorized[x:2]s32[1][2][[[1,2]],[[11,12]]]>In case both inputs are vectorized, they will be broadcasted
together before calculations are performed:iex> t=Nx.tensor([[1,2],[11,12]])|>Nx.vectorize(:x)iex> idx=Nx.tensor([0,1,0])|>Nx.vectorize(:y)iex> Nx.take(t,idx)#Nx.Tensor<vectorized[x:2][y:3]s32[[1,2,1],[11,12,11]]>iex> t=Nx.tensor([[1,2],[11,12]])|>Nx.vectorize(:x)iex> idx=Nx.tensor([[0,1,0],[0,1,1]])|>Nx.vectorize(:x)iex> Nx.take(t,idx)#Nx.Tensor<vectorized[x:2]s32[3][[1,2,1],[11,12,12]]>Error casesiex> Nx.take(Nx.tensor([[1,2],[3,4]]),Nx.tensor([1,0,1],type::f32))** (ArgumentError) indices must be an integer tensor, got {:f, 32}Link to this functiontake_along_axis(tensor, indices, opts \\ [])View SourceTakes the values from a tensor given an indices tensor, along the specified axis.The indices shape must be the same as the tensor's shape, with the exception for
the axis dimension, which can have arbitrary size. The returned tensor will have the
same shape as the indices tensor.See gather/2, slice/3, slice_along_axis/4, and take/3 for other ways to retrieve
values from a tensor.Options:axis - The axis along which to take the values from. Defaults to 0.Examplesiex> t=Nx.tensor([[1,2,3],[4,5,6]])iex> Nx.take_along_axis(t,Nx.tensor([[0,0,2,2,1,1],[2,2,1,1,0,0]]),axis:1)#Nx.Tensor<s32[2][6][[1,1,3,3,2,2],[6,6,5,5,4,4]]>iex> t=Nx.tensor([[1,2,3],[4,5,6]])iex> Nx.take_along_axis(t,Nx.tensor([[0,1,1],[1,0,0],[0,1,0]]),axis:0)#Nx.Tensor<s32[3][3][[1,5,6],[4,2,3],[1,5,3]]>The indices returned from Nx.argsort/2 can be used with Nx.take_along_axis/3 to
produce the sorted tensor (or to sort more tensors according to the same criteria).iex> tensor=Nx.tensor([[[1,2],[3,4],[5,6]]])#Nx.Tensor<s32[1][3][2][[[1,2],[3,4],[5,6]]]>iex> idx1=Nx.argsort(tensor,axis:1,direction::desc)#Nx.Tensor<s32[1][3][2][[[2,2],[1,1],[0,0]]]>iex> Nx.take_along_axis(tensor,idx1,axis:1)#Nx.Tensor<s32[1][3][2][[[5,6],[3,4],[1,2]]]>iex> idx2=Nx.argsort(tensor,axis:2,direction::desc)#Nx.Tensor<s32[1][3][2][[[1,0],[1,0],[1,0]]]>iex> Nx.take_along_axis(tensor,idx2,axis:2)#Nx.Tensor<s32[1][3][2][[[2,1],[4,3],[6,5]]]>Vectorized tensorstensor and indices have their vectorized axes broadcast together,
and then the operation takes place normally, with :axis and indices
having their values in reference to the input shape.iex> t=Nx.tensor([[[1,2,3]],[[4,5,6]]])|>Nx.vectorize(:x)iex> idx=Nx.tensor([[[0,0,2,1]],[[2,1,0,0]]])|>Nx.vectorize(:x)iex> Nx.take_along_axis(t,idx,axis:1)#Nx.Tensor<vectorized[x:2]s32[1][4][[[1,1,3,2]],[[6,5,4,4]]]>In the example below, we have broadcasting throughout the vectorized axesiex> t=Nx.tensor([[1,2,3],[4,5,6]])|>Nx.vectorize(:x)iex> idx=Nx.tensor([[0,0,2,1],[2,1,0,0]])|>Nx.vectorize(:y)iex> Nx.take_along_axis(t,idx,axis:0)#Nx.Tensor<vectorized[x:2][y:2]s32[4][[[1,1,3,2],[3,2,1,1]],[[4,4,6,5],[6,5,4,4]]]>Error casesiex> tensor=Nx.iota({3,3})iex> idx=Nx.tensor([[2.0],[1.0],[2.0]],type::f32)iex> Nx.take_along_axis(tensor,idx,axis:1)** (ArgumentError) indices must be an integer tensor, got {:f, 32}Functions: N-dimLink to this functionargsort(tensor, opts \\ [])View SourceSorts the tensor along the given axis according
to the given direction and returns the corresponding indices
of the original tensor in the new sorted positions.If no axis is given, defaults to 0.See take_along_axis/3 for examples on how to apply the
resulting indices from this function.Options:axis - The name or number of the corresponding axis on which the sort
should be applied:direction - Can be :asc or :desc. Defaults to :asc:stable - If the sorting is stable. Defaults to false:type - The type of the resulting tensor. Defaults to :s32.Examplesiex> Nx.argsort(Nx.tensor([16,23,42,4,8,15]))#Nx.Tensor<s32[6][3,4,5,0,1,2]>iex> t=Nx.tensor([[3,1,7],[2,5,4]],names:[:x,:y])iex> Nx.argsort(t,axis::x)#Nx.Tensor<s32[x:2][y:3][[1,0,1],[0,1,0]]>iex> t=Nx.tensor([[3,1,7],[2,5,4]],names:[:x,:y])iex> Nx.argsort(t,axis::y)#Nx.Tensor<s32[x:2][y:3][[1,0,2],[0,2,1]]>iex> t=Nx.tensor([[3,1,7],[2,5,4]],names:[:x,:y])iex> Nx.argsort(t,axis::y,direction::asc,type::u32)#Nx.Tensor<u32[x:2][y:3][[1,0,2],[0,2,1]]>Same tensor sorted over different axes. In this case,
we pass the stable option to preserve the order in case
of duplicate elements:iex> t=Nx.tensor(...> [...> [...> [4,5,2],...> [2,5,3],...> [5,0,2]...> ],...> [...> [1,9,8],...> [2,1,3],...> [2,1,4]...> ]...> ],...> names:[:x,:y,:z]...> )iex> Nx.argsort(t,axis::x,stable:true)#Nx.Tensor<s32[x:2][y:3][z:3][[[1,0,0],[0,1,0],[1,0,0]],[[0,1,1],[1,0,1],[0,1,1]]]>iex> Nx.argsort(t,axis::y,stable:true)#Nx.Tensor<s32[x:2][y:3][z:3][[[1,2,0],[0,0,2],[2,1,1]],[[0,1,1],[1,2,2],[2,0,0]]]>When it comes to NaN and infinities, NaN always sorts higher than
everything else:iex> t=Nx.tensor([:nan,:neg_infinity,:infinity,0.0])iex> Nx.argsort(t)#Nx.Tensor<s32[4][1,3,2,0]>iex> Nx.argsort(t,direction::desc)#Nx.Tensor<s32[4][0,2,3,1]>Link to this functionconcatenate(tensors, opts \\ [])View SourceConcatenates tensors along the given axis.Tensors can be a tuple or any Nx.Container or Nx.LazyContainer.
This means you can easily concatenate all columns in a dataframe
and other data structures. For convenience, this function also allows
a list of tensors to be given, which may be common outside of defn.If no axis is provided, defaults to 0. All tensors must have the same
rank and all of their axis except the concatenated one must match.If tensors with mixed types are given, the types will
be merged to a higher type and all of the tensors will
be cast to the higher type before concatenating.
If tensors are named, the names must match.ExamplesGiving a single tensor is a no-op:iex> Nx.concatenate([Nx.tensor([1,2,3])])#Nx.Tensor<s32[3][1,2,3]>Multiple tensors are concatented:iex> Nx.concatenate([Nx.tensor([1,2,3]),Nx.tensor([4,5,6])])#Nx.Tensor<s32[6][1,2,3,4,5,6]>Types are merged and names must match:iex> t1=Nx.iota({2,2,2},names:[:x,:y,:z],type::f32)iex> t2=Nx.iota({1,2,2},names:[:x,:y,:z],type::u8)iex> t3=Nx.iota({1,2,2},names:[:x,:y,:z],type::s64)iex> Nx.concatenate([t1,t2,t3],axis::x)#Nx.Tensor<f32[x:4][y:2][z:2][[[0.0,1.0],[2.0,3.0]],[[4.0,5.0],[6.0,7.0]],[[0.0,1.0],[2.0,3.0]],[[0.0,1.0],[2.0,3.0]]]>And you can pick a different axis:iex> t1=Nx.iota({1,3,2},names:[:x,:y,:z])iex> t2=Nx.iota({1,1,2},names:[:x,:y,:z])iex> t3=Nx.iota({1,2,2},names:[:x,:y,:z])iex> Nx.concatenate([t1,t2,t3],axis::y)#Nx.Tensor<s32[x:1][y:6][z:2][[[0,1],[2,3],[4,5],[0,1],[0,1],[2,3]]]>You can also pass any container (or lazy container) as first argument
and they are recursively traversed:iex> Nx.concatenate({Nx.tensor([1,2]),{Nx.tensor([3,4]),Nx.tensor([5,6])}})#Nx.Tensor<s32[6][1,2,3,4,5,6]>Vectorized tensorsIf vectorized tensors are given, they are all broadcasted throughout the
vectorized axes before concatenation. Normal concatenation rules still apply
to the inner shapes.iex> x=Nx.tensor([[1,2]])|>Nx.vectorize(:x)iex> y=Nx.tensor([[3,4],[5,6]])|>Nx.vectorize(:y)iex> z=Nx.tensor([[10],[11]])|>Nx.vectorize(:x)iex> Nx.concatenate({x,y,z})#Nx.Tensor<vectorized[x:2][y:2]s32[5][[[1,2,3,4,10],[1,2,5,6,10]],[[1,2,3,4,11],[1,2,5,6,11]]]>Error casesShapes must have the same rank and match on the non-concatenating axis.For example, the tensors below work if we concatenate on axis 1, but not on axis 0:iex> t1=Nx.iota({1,2,3})iex> t2=Nx.iota({1,1,3})iex> result=Nx.concatenate([t1,t2],axis:1)iex> Nx.shape(result){1,3,3}iex> Nx.concatenate([t1,t2],axis:0)** (ArgumentError) expected all shapes to match {*, 2, 3}, got unmatching shape: {1, 1, 3}If the ranks are different, it doesn't work, regardless of the axis choice:iex> t1=Nx.iota({1,2,3})iex> t2=Nx.iota({1,1})iex> Nx.concatenate([t1,t2])** (ArgumentError) expected all shapes to match {*, 2, 3}, got unmatching shape: {1, 1}Link to this functionconv(tensor, kernel, opts \\ [])View SourceComputes an n-D convolution (where n >= 3) as used in neural networks.This function can be thought of as sliding an n-D
kernel across the input, producing a new tensor that
has the same number of elements as the number of valid
windows in the input tensor. Each element is the result
of summing the element-wise products in the window across
each input channel.The ranks of both input and kernel must match. By
default, both input and kernel are expected to have shapes
of the following form:input - {batch_size, input_channels, input_d0, ..., input_dn}kernel - {output_channels, input_channels, kernel_d0, ..., kernel_dn}Where input_d0...input_dn and kernel_d0...kernel_dn represent
an arbitrary number of spatial dimensions. You can alter this configuration
using one of the *_permutation configuration options. Permutations
are input, kernel, and output specifications for the layout of the
convolution. For example, if your input tensor is configured with
"channels last", you can specify the input permutation with:Nx.conv(img,kernel,input_permutation:[0,3,1,2])Permutations expect configurations that specify the location of
dimensions in the following orders:input_permutation - [batch_dim, input_channel_dim, ...spatial_dims...]kernel_permutation - [output_channel_dim, input_channel_dim, ...spatial_dims...]output_permutation - [batch_dim, output_channel_dim, ...spatial_dims...]Using named tensors, it's a bit easier to see how permutations
help you configure the convolution. Given input tensor with names
[:batch, :height, :width, :channels] (channels last) and kernel
tensor with names [:input, :output, :height, :width], you can
configure the convolution with the following permutations:Nx.conv(img,kernel,input_permutation:[:batch,:channels,:height,:width],kernel_permutation:[:output,:input,:height,:width],output_permutation:[:batch,:channels,:height,:width])Notice that output_permutation is normalized with respect to
the input permutation names. We cannot guarantee that every
permutation is supported in every backend or compiler.To configure how the window slides along the input tensor, you
can specify :strides. :strides must be a positive integer
or tuple of positive integers for each spatial dimension
in the input and kernel. For each spatial dimension, the
window will slide by the configuration specified in :strides.
As an example, for a 2-D convolution with strides: [2, 1],
the window will slide 2 positions along the first spatial
dimension until it reaches the end of the dimension and then
1 position along the second spatial dimension.You may specify a padding configuration using :padding,
which will zero-pad the input tensor. Acceptable padding
configurations are::valid - no padding:same - pad input spatial dimensions such that they
will remain unchanged in the output tensor[{d0_hi, d0_lo}, ..., {dn_hi, dn_lo}] - a general padding
configuration of edge high and edge low padding values. You
may only specify padding for the edges of spatial dimensions
of the input tensor. Padding values may be negative.You can dilate convolutions by setting :input_dilation or
:kernel_dilation. Both :input_dilation and :kernel_dilation
must either be positive integers or tuples of positive integers
for each spatial dimension in the input and kernel tensors. Dilations
can be thought of as applying dilation - 1 interior padding to the
input or kernel tensor.You can split both the input and kernel tensor into feature groups
using :feature_group_size. This will split both the input and kernel
tensor channels and compute a grouped convolution. The size of the
kernel input feature channels times the size of the feature group must
match the size of the input tensor feature channels. Additionally,
the size of the kernel output feature channels must be evenly divisible
by the group size.You can also split the input tensor along the batch dimension by
specifying :batch_group_size. This will compute a grouped convolution
in the same way as with :feature_group_size, however, the input
tensor will be split into groups along the batch dimension.Examplesiex> left=Nx.iota({1,1,3,3})iex> right=Nx.iota({4,1,1,1})iex> Nx.conv(left,right,strides:[1,1])#Nx.Tensor<f32[1][4][3][3][[[[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0]],[[0.0,1.0,2.0],[3.0,4.0,5.0],[6.0,7.0,8.0]],[[0.0,2.0,4.0],[6.0,8.0,10.0],[12.0,14.0,16.0]],[[0.0,3.0,6.0],[9.0,12.0,15.0],[18.0,21.0,24.0]]]]>iex> left=Nx.iota({1,1,3,3})iex> right=Nx.iota({4,1,2,1})iex> Nx.conv(left,right,strides:2,padding::same,kernel_dilation:[2,1])#Nx.Tensor<f32[1][4][2][2][[[[3.0,5.0],[0.0,0.0]],[[9.0,15.0],[6.0,10.0]],[[15.0,25.0],[12.0,20.0]],[[21.0,35.0],[18.0,30.0]]]]>Complex tensors are also supported:iex> left=Nx.tensor([[[Complex.new(1,1),2,Complex.new(3,-3)]]])iex> right=Nx.tensor([[[1,Complex.new(0,2),Complex.new(0,3)]]])iex> Nx.conv(left,right,padding:[{2,2}])#Nx.Tensor<c64[1][1][5][[[-3.0+3.0i,-2.0+8.0i,10.0+14.0i,8.0+6.0i,3.0-3.0i]]]>Link to this functiondiff(tensor, opts \\ [])View SourceCalculate the n-th discrete difference along the given axis.The first difference is given by $out_i = a_{i+1} - a_i$ along the given axis,
higher differences are calculated by using diff recursively.Options:order - the number of times to perform the difference. Defaults to 1:axis - the axis to perform the difference along. Defaults to -1Examplesiex> Nx.diff(Nx.tensor([1,2,4,7,0]))#Nx.Tensor<s32[4][1,2,3,-7]>iex> Nx.diff(Nx.tensor([1,2,4,7,0]),order:2)#Nx.Tensor<s32[3][1,1,-10]>iex> Nx.diff(Nx.tensor([[1,3,6,10],[0,5,6,8]]))#Nx.Tensor<s32[2][3][[2,3,4],[5,1,2]]>iex> Nx.diff(Nx.tensor([[1,3,6,10],[0,5,6,8]]),axis:0)#Nx.Tensor<s32[1][4][[-1,2,0,-2]]>iex> Nx.diff(Nx.tensor([1,2,4,7,0]),order:0)#Nx.Tensor<s32[5][1,2,4,7,0]>iex> Nx.diff(Nx.tensor([1,2,4,7,0]),order:-1)** (ArgumentError) order must be non-negative but got: -1Link to this functiondot(t1, t2)View SourceReturns the dot product of two tensors.Given a and b, computes the dot product according to
the following rules:If both a and b are scalars, it is equivalent to a * b.If a is a scalar and b is a tensor, it is equivalent to Nx.multiply(a, b).If a is a tensor and b is a scalar, it is equivalent to Nx.multiply(a, b).If both a and b are 1-D tensors (vectors), it is the sum of the element-wise
product between a and b. The lengths of a and b must be equal.If both a and b are 2-D tensors (matrices), it is equivalent to matrix-multiplication.If either a or b is a 1-D tensor, and the other is an n-D tensor, it is the
sum of the element-wise product along the last axis of a or b. The length of the
1-D tensor must match the last dimension of the n-D tensor.If a is an n-D tensor and b is an m-D tensor, it is the sum of the element-wise
product along the last axis of a and the second-to-last axis of b. The last dimension
of a must match the second-to-last dimension of b.For a more general dot function where you control which axes contract,
see dot/4.ExamplesDot product of scalarsiex> Nx.dot(5,5)#Nx.Tensor<s3225>iex> Nx.dot(-2.0,5.0)#Nx.Tensor<f32-10.0>iex> Nx.dot(2,2.0)#Nx.Tensor<f324.0>Dot product of vectorsiex> Nx.dot(Nx.tensor([1,2,3]),Nx.tensor([4,5,6]))#Nx.Tensor<s3232>iex> Nx.dot(Nx.tensor([2.0,4.0,3.0,5.0]),Nx.tensor([1.0,2.0,3.0,4.0]))#Nx.Tensor<f3239.0>iex> Nx.dot(Nx.tensor([1.0,2.0,3.0]),Nx.tensor([1,2,3]))#Nx.Tensor<f3214.0>Dot product of matricesiex> left=Nx.tensor([[1,2,3],[4,5,6]],names:[:i,:j])iex> right=Nx.tensor([[7,8],[9,10],[11,12]],names:[:x,:y])iex> Nx.dot(left,right)#Nx.Tensor<s32[i:2][y:2][[58,64],[139,154]]>iex> left=Nx.tensor([[10.0,13.0,14.0,15.0],[59.0,20.0,10.0,30.0]],names:[:i,:j])iex> right=Nx.tensor([[2.0,4.0],[5.0,1.0],[6.0,8.0],[9.0,10.0]],names:[:x,:y])iex> Nx.dot(left,right)#Nx.Tensor<f32[i:2][y:2][[304.0,315.0],[548.0,636.0]]>iex> left=Nx.tensor([[1,2,3],[4,5,6]],names:[:i,:j])iex> right=Nx.tensor([[7.0,8.0],[9.0,10.0],[11.0,12.0]],names:[:x,:y])iex> Nx.dot(left,right)#Nx.Tensor<f32[i:2][y:2][[58.0,64.0],[139.0,154.0]]>Dot product of vector and n-d tensoriex> left=Nx.tensor([[[1,2],[3,4]],[[5,6],[7,8]]],names:[:i,:j,:k])iex> right=Nx.tensor([5,10],names:[:x])iex> Nx.dot(left,right)#Nx.Tensor<s32[i:2][j:2][[25,55],[85,115]]>iex> left=Nx.tensor([5,10],names:[:x])iex> right=Nx.tensor([[1,2,3],[4,5,6]],names:[:i,:j])iex> Nx.dot(left,right)#Nx.Tensor<s32[j:3][45,60,75]>iex> left=Nx.tensor([[[[[1.0,2.0],[3.0,4.0]],[[5.0,6.0],[7.0,8.0]]]]],names:[:shard,:batch,:x,:y,:z])iex> right=Nx.tensor([2.0,2.0],names:[:data])iex> Nx.dot(left,right)#Nx.Tensor<f32[shard:1][batch:1][x:2][y:2][[[[6.0,14.0],[22.0,30.0]]]]>Dot product of n-D and m-D tensoriex> left=Nx.tensor([[[1,2,3],[4,5,6],[7,8,9]],[[1,2,3],[4,5,6],[7,8,9]]],names:[:x,:y,:z])iex> right=Nx.tensor([[[1,2,3],[3,4,5],[5,6,7]]],names:[:i,:j,:k])iex> Nx.dot(left,right)#Nx.Tensor<s32[x:2][y:3][i:1][k:3][[[[22,28,34]],[[49,64,79]],[[76,100,124]]],[[[22,28,34]],[[49,64,79]],[[76,100,124]]]]>Vectorized tensorsVectorized axes are treated as batched axes, much like
dot/6 behaves with non-vectorized tensors.iex> t1=Nx.tensor([[1,2],[3,4]])|>Nx.vectorize(:x)iex> t2=Nx.tensor([[10,20],[30,40]])|>Nx.vectorize(:x)iex> Nx.dot(t1,t2)#Nx.Tensor<vectorized[x:2]s32[50,250]>iex> t1=Nx.tensor([1,2])|>Nx.vectorize(:x)iex> t2=Nx.tensor([[10,20]])|>Nx.vectorize(:y)iex> Nx.dot(t1,t2)#Nx.Tensor<vectorized[x:2][y:1]s32[2][[[10,20]],[[20,40]]]>Error casesiex> Nx.dot(Nx.tensor([1,2,3]),Nx.tensor([1,2]))** (ArgumentError) dot/zip expects shapes to be compatible, dimension 0 of left-side (3) does not equal dimension 0 of right-side (2)Link to this functiondot(t1, contract_axes1, t2, contract_axes2)View SourceComputes the generalized dot product between two tensors, given
the contracting axes.This is equivalent to calling Nx.dot/6 with no batching dimensions:Nx.dot(t1,contract_axes1,[],t2,contract_axes2,[])Examplesiex> t1=Nx.tensor([[1,2],[3,4]],names:[:x,:y])iex> t2=Nx.tensor([[10,20],[30,40]],names:[:height,:width])iex> Nx.dot(t1,[0],t2,[0])#Nx.Tensor<s32[y:2][width:2][[100,140],[140,200]]>iex> t1=Nx.tensor([[0.0,1.0,2.0],[3.0,4.0,5.0]])iex> t2=Nx.tensor([[0.0,1.0],[2.0,3.0],[4.0,5.0]])iex> Nx.dot(t1,[0,1],t2,[1,0])#Nx.Tensor<f3250.0>Vectorized tensorsThe contracting axes refer to the tensors' shapes
and do not apply to the vectorized axes:iex> t1=Nx.tensor([[[1,1],[2,2]],[[1,1],[1,1]]])|>Nx.vectorize(:x)iex> t2=Nx.tensor([[1,2],[3,4]])iex> Nx.dot(t1,[0],t2,[0])#Nx.Tensor<vectorized[x:2]s32[2][2][[[7,10],[7,10]],[[4,6],[4,6]]]>iex> Nx.dot(t1,[1],t2,[0])#Nx.Tensor<vectorized[x:2]s32[2][2][[[4,6],[8,12]],[[4,6],[4,6]]]>Link to this functiondot(t1_in, contract_axes1, batch_axes1, t2_in, contract_axes2, batch_axes2)View SourceComputes the generalized dot product between two tensors, given
the contracting and batch axes.The dot product is computed by multiplying the values from t1
given by contract_axes1 against the values from t2 given by
contract_axes2, considering batch axes of batch_axes1 and
batch_axes2. For instance, the first axis in contract_axes1
will be matched against the first axis in contract_axes2 and
so on. The axes given by contract_axes1 and contract_axes2
are effectively removed from the final tensor, which is why they
are often called the contraction axes.If no contracting axes are given, the final product works like
Nx.outer/2.Specifying batch axes will compute a vectorized dot product
along the given batch dimensions. The length of batch_axes1
and batch_axes2 must match. Additionally, batch_axes1 and
batch_axes2 must be a list of successive dimension numbers,
where each batch axis matches the dimension of the corresponding
batch axis in the other input.The contracting axes must be dot-product compatible and the
batch dimensions must always have the same number of elements.ExamplesContracting along axesiex> t1=Nx.tensor([[1,2],[3,4]],names:[:x,:y])iex> t2=Nx.tensor([[10,20],[30,40]],names:[:height,:width])iex> Nx.dot(t1,[0],[],t2,[0],[])#Nx.Tensor<s32[y:2][width:2][[100,140],[140,200]]>iex> Nx.dot(t1,[0],[],t2,[1],[])#Nx.Tensor<s32[y:2][height:2][[70,150],[100,220]]>iex> Nx.dot(t1,[1],[],t2,[0],[])#Nx.Tensor<s32[x:2][width:2][[70,100],[150,220]]>iex> Nx.dot(t1,[1],[],t2,[1],[])#Nx.Tensor<s32[x:2][height:2][[50,110],[110,250]]>iex> Nx.dot(t1,[0,1],[],t2,[0,1],[])#Nx.Tensor<s32300>If no axes are given, it works like outer/2:iex> t1=Nx.tensor([[1,2],[3,4]])iex> t2=Nx.tensor([[10,20],[30,40]])iex> Nx.dot(t1,[],[],t2,[],[])#Nx.Tensor<s32[2][2][2][2][[[[10,20],[30,40]],[[20,40],[60,80]]],[[[30,60],[90,120]],[[40,80],[120,160]]]]>Dot product between two batched tensorsiex> u=Nx.tensor([[[1]],[[2]]])iex> v=Nx.tensor([[[3]],[[4]]])iex> Nx.dot(u,[2],[0],v,[2],[0])#Nx.Tensor<s32[2][1][1][[[3]],[[8]]]>iex> u=Nx.tensor([[[1,1]],[[2,2]]])iex> v=Nx.tensor([[[3],[3]],[[4],[4]]])iex> Nx.dot(u,[2],[0],v,[1],[0])#Nx.Tensor<s32[2][1][1][[[6]],[[16]]]>Vectorized tensorsIf you already have vectorized axes, they will be automatically
added to the batched axes of dot/6. Input axes must refer to
the tensor shape, and offsets due to vectorized axes are
handled internally.Rewriting the previous example with vectorization:iex> u=Nx.tensor([[[1,1]],[[2,2]]])|>Nx.vectorize(:x)iex> v=Nx.tensor([[[3],[3]],[[4],[4]]])|>Nx.vectorize(:x)iex> Nx.dot(u,[1],[],v,[0],[])# note that axes refer to the inner shapes#Nx.Tensor<vectorized[x:2]s32[1][1][[[6]],[[16]]]>  Because the batch axes are now empty, we can use dot/4 to be more concise.Nx.dot(u,[1],v,[0])  However, we can go even further. Since we are contracting the last axis of
  u with the first axis of v, we can rely on dot/2 to achieve the same
  result.Nx.dot(u,v)Error casesiex> u=Nx.tensor([[[1,1]],[[2,2]]])iex> v=Nx.tensor([[[3],[3]],[[4],[4]]])iex> Nx.dot(u,[2],[0],v,[1],[])** (ArgumentError) right tensor must be batched if left tensor is batchediex> u=Nx.tensor([[[1,1]],[[2,2]]])iex> v=Nx.tensor([[[3],[3]],[[4],[4]]])iex> Nx.dot(u,[2],[],v,[1],[0])** (ArgumentError) left tensor must be batched if right tensor is batchediex> u=Nx.tensor([[[1,1]],[[2,2]]])iex> v=Nx.tensor([[[3],[3]],[[4],[4]]])iex> Nx.dot(u,[2],[1],v,[1],[0])** (ArgumentError) invalid dot batch axis for the left tensor, batch axes must be successive dimensions starting from 0, got [1]iex> u=Nx.tensor([[[1,1]],[[2,2]]])iex> v=Nx.tensor([[[3],[3]],[[4],[4]]])iex> Nx.dot(u,[2],[0],v,[1],[1])** (ArgumentError) invalid dot batch axis for the right tensor, batch axes must be successive dimensions starting from 0, got [1]iex> u=Nx.tensor([[[1,1]],[[2,2]]])iex> v=Nx.tensor([[[3],[3]],[[4],[4]]])iex> Nx.dot(u,[0],[0],v,[1],[0])** (ArgumentError) dot batch axes for left tensor ([0]) cannot be in contract axes ([0])iex> u=Nx.tensor([[[1,1]],[[2,2]]])iex> v=Nx.tensor([[[3],[3]],[[4],[4]]])iex> Nx.dot(u,[2],[0],v,[0],[0])** (ArgumentError) dot batch axes for right tensor ([0]) cannot be in contract axes ([0])Link to this functionfft2(tensor, opts \\ [])View SourceCalculates the 2D DFT of the given tensor.Options:eps - Threshold which backends can use for cleaning-up results. Defaults to 1.0e-10.:lengths - A 2 element list where each element is either a positive integer or :power_of_two.
Will pad or slice the tensor accordingly. :power_of_two will automatically pad to the next power of two.:axes - the 2 axes upon which the Inverse 2D DFT will be calculated. Defaults to the last 2 axes.Examplesiex> Nx.fft2(Nx.tensor([[1,0,1,0],[1,1,1,1]]))#Nx.Tensor<c64[2][4][[6.0+0.0i,0.0+0.0i,2.0+0.0i,0.0+0.0i],[-2.0+0.0i,0.0+0.0i,2.0+0.0i,0.0+0.0i]]>The calculation can happen on a specific pair of axes:iex> tensor=Nx.tensor([[[1,0,1,0]],[[1,1,1,1]]])iex> Nx.fft2(tensor,axes:[0,-1])#Nx.Tensor<c64[2][1][4][[[6.0+0.0i,0.0+0.0i,2.0+0.0i,0.0+0.0i]],[[-2.0+0.0i,0.0+0.0i,2.0+0.0i,0.0+0.0i]]]>iex> Nx.fft2(tensor,axes:[-2,-1])#Nx.Tensor<c64[2][1][4][[[2.0+0.0i,0.0+0.0i,2.0+0.0i,0.0+0.0i]],[[4.0+0.0i,0.0+0.0i,0.0+0.0i,0.0+0.0i]]]>Padding and slicing can be introduced through :lengths:iex> tensor=Nx.tensor([[1,1],[1,0]])iex> Nx.fft2(tensor,lengths:[2,4])#Nx.Tensor<c64[2][4][[3.0+0.0i,2.0-1.0i,1.0+0.0i,2.0+1.0i],[1.0+0.0i,0.0-1.0i,-1.0+0.0i,0.0+1.0i]]>iex> Nx.fft2(tensor,lengths:[4,2])#Nx.Tensor<c64[4][2][[3.0+0.0i,1.0+0.0i],[2.0-1.0i,0.0-1.0i],[1.0+0.0i,-1.0+0.0i],[2.0+1.0i,0.0+1.0i]]>iex> Nx.fft2(Nx.tensor([[1,1,0],[1,1,0],[1,1,-1]]),lengths:[:power_of_two,:power_of_two])#Nx.Tensor<c64[4][4][[5.0+0.0i,4.0-3.0i,-1.0+0.0i,4.0+3.0i],[1.0-2.0i,-2.0-1.0i,1.0+0.0i,0.0-1.0i],[1.0+0.0i,2.0-1.0i,-1.0+0.0i,2.0+1.0i],[1.0+2.0i,0.0+1.0i,1.0+0.0i,-2.0+1.0i]]>iex> Nx.fft2(Nx.tensor([[[1,1,0,0,2,3]]]),axes:[0,2],lengths:[2,4])#Nx.Tensor<c64[2][1][4][[[2.0+0.0i,1.0-1.0i,0.0+0.0i,1.0+1.0i]],[[2.0+0.0i,1.0-1.0i,0.0+0.0i,1.0+1.0i]]]>If an N-dimensional tensor is passed, the DFT is, by default, applied to the last axes:iex> tensor=Nx.tensor([...> [[[1,0,1,0,10,10],[1,1,1,1,10,10]]],...> [[[-2,-2,-2,-2,20,20],[0,0,0,1,-20,-20]]]])iex> Nx.fft2(tensor,lengths:[2,4])#Nx.Tensor<c64[2][1][2][4][[[[6.0+0.0i,0.0+0.0i,2.0+0.0i,0.0+0.0i],[-2.0+0.0i,0.0+0.0i,2.0+0.0i,0.0+0.0i]]],[[[-7.0+0.0i,0.0+1.0i,-1.0+0.0i,0.0-1.0i],[-9.0+0.0i,0.0-1.0i,1.0+0.0i,0.0+1.0i]]]]>Vectorized tensorsVectorized tensors work the same as N-dimensional tensorsiex> tensor=Nx.tensor([...> [[[1,0,1,0,10,10],[1,1,1,1,10,10]]],...> [[[-2,-2,-2,-2,20,20],[0,0,0,1,-20,-20]]]...> ])|>Nx.vectorize(:x)iex> Nx.fft2(tensor,lengths:[2,4])#Nx.Tensor<vectorized[x:2]c64[1][2][4][[[[6.0+0.0i,0.0+0.0i,2.0+0.0i,0.0+0.0i],[-2.0+0.0i,0.0+0.0i,2.0+0.0i,0.0+0.0i]]],[[[-7.0+0.0i,0.0+1.0i,-1.0+0.0i,0.0-1.0i],[-9.0+0.0i,0.0-1.0i,1.0+0.0i,0.0+1.0i]]]]>Error Casesiex> Nx.fft2(Nx.tensor([[1,1]]),lengths:[:invalid,2])** (ArgumentError) expected :lengths to be a list of lengths or :power_of_two, got: [:invalid, 2]iex> Nx.fft2(Nx.tensor([1,1]),length::invalid)** (ArgumentError) expected a tensor with rank > 1, got tensor with rank 1Link to this functionfft(tensor, opts \\ [])View SourceCalculates the DFT of the given tensor.Options:eps - Threshold which backends can use for cleaning-up results. Defaults to 1.0e-10.:length - Either a positive integer or :power_of_two. Will pad or slice the tensor
accordingly. :power_of_two will automatically pad to the next power of two.:axis - the axis upon which the DFT will be calculated. Defaults to the last axis.Examplesiex> Nx.fft(Nx.tensor([1,1,0,0]))#Nx.Tensor<c64[4][2.0+0.0i,1.0-1.0i,0.0+0.0i,1.0+1.0i]>iex> Nx.fft(Nx.tensor([1,1,1,0,1,1]))#Nx.Tensor<c64[6][5.0+0.0i,1.0+0.0i,-1.0+0.0i,1.0+0.0i,-1.0+0.0i,1.0+0.0i]>The calculation can happen on a specific axis:iex> tensor=Nx.tensor([[1,1,1,0,1,1],[1,1,1,0,1,1]])iex> Nx.fft(tensor,axis:-1)#Nx.Tensor<c64[2][6][[5.0+0.0i,1.0+0.0i,-1.0+0.0i,1.0+0.0i,-1.0+0.0i,1.0+0.0i],[5.0+0.0i,1.0+0.0i,-1.0+0.0i,1.0+0.0i,-1.0+0.0i,1.0+0.0i]]>iex> Nx.fft(tensor,axis:-2)#Nx.Tensor<c64[2][6][[2.0+0.0i,2.0+0.0i,2.0+0.0i,0.0+0.0i,2.0+0.0i,2.0+0.0i],[0.0+0.0i,0.0+0.0i,0.0+0.0i,0.0+0.0i,0.0+0.0i,0.0+0.0i]]>Padding and slicing can be introduced through :length:iex> Nx.fft(Nx.tensor([1,1]),length:4)#Nx.Tensor<c64[4][2.0+0.0i,1.0-1.0i,0.0+0.0i,1.0+1.0i]>iex> Nx.fft(Nx.tensor([1,1,0]),length::power_of_two)#Nx.Tensor<c64[4][2.0+0.0i,1.0-1.0i,0.0+0.0i,1.0+1.0i]>iex> Nx.fft(Nx.tensor([1,1,0,0,2,3]),length:4)#Nx.Tensor<c64[4][2.0+0.0i,1.0-1.0i,0.0+0.0i,1.0+1.0i]>If an N-dimensional tensor is passed, the DFT is applied, by default, to its last axis:iex> Nx.fft(Nx.tensor([[1,1,0,0,2,3],[1,0,0,0,2,3]]),length:4)#Nx.Tensor<c64[2][4][[2.0+0.0i,1.0-1.0i,0.0+0.0i,1.0+1.0i],[1.0+0.0i,1.0+0.0i,1.0+0.0i,1.0+0.0i]]>Vectorized tensorsVectorized tensors work the same as N-dimensional tensorsiex> tensor=Nx.tensor([[1,1,0,0,2,3],[1,0,0,0,2,3]])|>Nx.vectorize(:x)iex> Nx.fft(tensor,length:4)#Nx.Tensor<vectorized[x:2]c64[4][[2.0+0.0i,1.0-1.0i,0.0+0.0i,1.0+1.0i],[1.0+0.0i,1.0+0.0i,1.0+0.0i,1.0+0.0i]]>Error Casesiex> Nx.fft(Nx.tensor([1,1]),length::invalid)** (RuntimeError) expected an integer or :power_of_two as length, got: :invalidLink to this functionifft2(tensor, opts \\ [])View SourceCalculates the Inverse 2D DFT of the given tensor.Options:eps - Threshold which backends can use for cleaning-up results. Defaults to 1.0e-10.:lengths - A 2 element list where each element is either a positive integer or :power_of_two.
Will pad or slice the tensor accordingly. :power_of_two will automatically pad to the next power of two.:axes - the 2 axes upon which the Inverse 2D DFT will be calculated. Defaults to the last 2 axes.Examplesiex> Nx.ifft2(Nx.tensor([[6,0,2,0],[-2,0,2,0]]))#Nx.Tensor<c64[2][4][[1.0+0.0i,0.0+0.0i,1.0+0.0i,0.0+0.0i],[1.0+0.0i,1.0+0.0i,1.0+0.0i,1.0+0.0i]]>The calculation can happen on a specific pair of axes:iex> tensor=Nx.tensor([[[6,0,2,0]],[[-2,0,2,0]]])iex> Nx.ifft2(tensor,axes:[0,-1])#Nx.Tensor<c64[2][1][4][[[1.0+0.0i,0.0+0.0i,1.0+0.0i,0.0+0.0i]],[[1.0+0.0i,1.0+0.0i,1.0+0.0i,1.0+0.0i]]]>iex> Nx.ifft2(tensor,axes:[-2,-1])#Nx.Tensor<c64[2][1][4][[[2.0+0.0i,1.0+0.0i,2.0+0.0i,1.0+0.0i]],[[0.0+0.0i,-1.0+0.0i,0.0+0.0i,-1.0+0.0i]]]>Padding and slicing can be introduced through :lengths:iex> tensor=Nx.tensor([[8,8],[8,0]])iex> Nx.ifft2(tensor,lengths:[2,4])#Nx.Tensor<c64[2][4][[3.0+0.0i,2.0+1.0i,1.0+0.0i,2.0-1.0i],[1.0+0.0i,0.0+1.0i,-1.0+0.0i,0.0-1.0i]]>iex> Nx.ifft2(tensor,lengths:[4,2])#Nx.Tensor<c64[4][2][[3.0+0.0i,1.0+0.0i],[2.0+1.0i,0.0+1.0i],[1.0+0.0i,-1.0+0.0i],[2.0-1.0i,0.0-1.0i]]>iex> Nx.ifft2(Nx.tensor([[16,16,0],[16,16,0],[16,16,-16]]),lengths:[:power_of_two,:power_of_two])#Nx.Tensor<c64[4][4][[5.0+0.0i,4.0+3.0i,-1.0+0.0i,4.0-3.0i],[1.0+2.0i,-2.0+1.0i,1.0+0.0i,0.0+1.0i],[1.0+0.0i,2.0+1.0i,-1.0+0.0i,2.0-1.0i],[1.0-2.0i,0.0-1.0i,1.0+0.0i,-2.0-1.0i]]>iex> Nx.ifft2(Nx.tensor([[[8,8,0,0,2,3]]]),axes:[0,2],lengths:[2,4])#Nx.Tensor<c64[2][1][4][[[2.0+0.0i,1.0+1.0i,0.0+0.0i,1.0-1.0i]],[[2.0+0.0i,1.0+1.0i,0.0+0.0i,1.0-1.0i]]]>If an N-dimensional tensor is passed, the Inverse DFT is, by default, applied to the last axes:iex> tensor=Nx.tensor([...> [[[8,0,8,0,10,10],[8,8,8,8,10,10]]],...> [[[-16,-16,-16,-16,20,20],[0,0,0,8,-20,-20]]]])iex> Nx.ifft2(tensor,lengths:[2,4])#Nx.Tensor<c64[2][1][2][4][[[[6.0+0.0i,0.0+0.0i,2.0+0.0i,0.0+0.0i],[-2.0+0.0i,0.0+0.0i,2.0+0.0i,0.0+0.0i]]],[[[-7.0+0.0i,0.0-1.0i,-1.0+0.0i,0.0+1.0i],[-9.0+0.0i,0.0+1.0i,1.0+0.0i,0.0-1.0i]]]]>Vectorized tensorsVectorized tensors work the same as N-dimensional tensorsiex> tensor=Nx.tensor([...> [[[8,0,8,0,10,10],[8,8,8,8,10,10]]],...> [[[-16,-16,-16,-16,20,20],[0,0,0,8,-20,-20]]]...> ])|>Nx.vectorize(:x)iex> Nx.ifft2(tensor,lengths:[2,4])#Nx.Tensor<vectorized[x:2]c64[1][2][4][[[[6.0+0.0i,0.0+0.0i,2.0+0.0i,0.0+0.0i],[-2.0+0.0i,0.0+0.0i,2.0+0.0i,0.0+0.0i]]],[[[-7.0+0.0i,0.0-1.0i,-1.0+0.0i,0.0+1.0i],[-9.0+0.0i,0.0+1.0i,1.0+0.0i,0.0-1.0i]]]]>Error Casesiex> Nx.ifft2(Nx.tensor([[1,1]]),lengths:[:invalid,2])** (ArgumentError) expected :lengths to be a list of lengths or :power_of_two, got: [:invalid, 2]iex> Nx.ifft2(Nx.tensor([1,1]),length::invalid)** (ArgumentError) expected a tensor with rank > 1, got tensor with rank 1Link to this functionifft(tensor, opts \\ [])View SourceCalculates the Inverse DFT of the given tensor.Options:eps - Threshold which backends can use for cleaning-up results. Defaults to 1.0e-10.:length - Either a positive integer or :power_of_two. Will pad or slice the tensor
accordingly. :power_of_two will automatically pad to the next power of two.:axis - the axis upon which the Inverse DFT will be calculated. Defaults to the last axis.Examplesiex> Nx.ifft(Nx.tensor([2,Complex.new(1,-1),0,Complex.new(1,1)]))#Nx.Tensor<c64[4][1.0+0.0i,1.0+0.0i,0.0+0.0i,0.0+0.0i]>iex> Nx.ifft(Nx.tensor([5,1,-1,1,-1,1]))#Nx.Tensor<c64[6][1.0+0.0i,1.0+0.0i,1.0+0.0i,0.0+0.0i,1.0+0.0i,1.0+0.0i]>The calculation can happen on a specific axis:iex> tensor=Nx.tensor([[5,1,-1,1,-1,1],[5,1,-1,1,-1,1]])iex> Nx.ifft(tensor,axis:-1)#Nx.Tensor<c64[2][6][[1.0+0.0i,1.0+0.0i,1.0+0.0i,0.0+0.0i,1.0+0.0i,1.0+0.0i],[1.0+0.0i,1.0+0.0i,1.0+0.0i,0.0+0.0i,1.0+0.0i,1.0+0.0i]]>iex> Nx.ifft(tensor,axis:-2)#Nx.Tensor<c64[2][6][[5.0+0.0i,1.0+0.0i,-1.0+0.0i,1.0+0.0i,-1.0+0.0i,1.0+0.0i],[0.0+0.0i,0.0+0.0i,0.0+0.0i,0.0+0.0i,0.0+0.0i,0.0+0.0i]]>Padding and slicing can be introduced through :length:iex> Nx.ifft(Nx.tensor([1,1]),length:4)#Nx.Tensor<c64[4][0.5+0.0i,0.25+0.25i,0.0+0.0i,0.25-0.25i]>iex> Nx.ifft(Nx.tensor([1,1,0]),length::power_of_two)#Nx.Tensor<c64[4][0.5+0.0i,0.25+0.25i,0.0+0.0i,0.25-0.25i]>iex> Nx.ifft(Nx.tensor([1,1,0,0,2,3]),length:4)#Nx.Tensor<c64[4][0.5+0.0i,0.25+0.25i,0.0+0.0i,0.25-0.25i]>If an N-dimensional tensor is passed, the Inverse DFT is applied, by default,to its last axis:iex> Nx.ifft(Nx.tensor([[1,1,0,0,2,3],[1,0,0,0,2,3]]),length:4)#Nx.Tensor<c64[2][4][[0.5+0.0i,0.25+0.25i,0.0+0.0i,0.25-0.25i],[0.25+0.0i,0.25+0.0i,0.25+0.0i,0.25+0.0i]]>Vectorized tensorsVectorized tensors work the same as N-dimensional tensorsiex> tensor=Nx.tensor([[1,1,0,0,2,3],[1,0,0,0,2,3]])|>Nx.vectorize(:x)iex> Nx.ifft(tensor,length:4)#Nx.Tensor<vectorized[x:2]c64[4][[0.5+0.0i,0.25+0.25i,0.0+0.0i,0.25-0.25i],[0.25+0.0i,0.25+0.0i,0.25+0.0i,0.25+0.0i]]>Error Casesiex> Nx.ifft(Nx.tensor([1,1]),length::invalid)** (RuntimeError) expected an integer or :power_of_two as length, got: :invalidLink to this functionouter(t1, t2)View SourceComputes the outer product of two tensors.The output is always a two-dimensional tensor.Examplesiex> Nx.outer(Nx.tensor([1,2,3],names:[:x]),100)#Nx.Tensor<s32[x:3][1][[100],[200],[300]]>iex> Nx.outer(Nx.tensor([1,2,3],names:[:x]),Nx.tensor([10,20],names:[:y]))#Nx.Tensor<s32[x:3][y:2][[10,20],[20,40],[30,60]]>iex> Nx.outer(Nx.tensor([[1,2],[3,4]],names:[:x,:y]),Nx.tensor([10,20,30],names:[:z]))#Nx.Tensor<s32[x:4][z:3][[10,20,30],[20,40,60],[30,60,90],[40,80,120]]>Vectorized tensorsBecause outer/2 is built on top of otheriex> x=Nx.tensor([[1,2,3],[0,-1,-2]],names:[nil,:a])|>Nx.vectorize(:x)iex> y=Nx.tensor([[10,20],[-10,-20]],names:[nil,:b])|>Nx.vectorize(:y)iex> Nx.outer(x,y)#Nx.Tensor<vectorized[x:2][y:2]s32[a:3][b:2][[[[10,20],[20,40],[30,60]],[[-10,-20],[-20,-40],[-30,-60]]],[[[0,0],[-10,-20],[-20,-40]],[[0,0],[10,20],[20,40]]]]>Link to this functionreverse(tensor, opts \\ [])View SourceReverses the tensor in the given dimensions.If no axes are provided, reverses every axis.You can pass either names or numbers for the reverse
dimensions. Dimensions must be unique, but they do not
have to be successive.Examplesiex> Nx.reverse(Nx.tensor([1,2,3]))#Nx.Tensor<s32[3][3,2,1]>iex> Nx.reverse(Nx.tensor([[1,2,3],[4,5,6]]))#Nx.Tensor<s32[2][3][[6,5,4],[3,2,1]]>iex> Nx.reverse(Nx.tensor([1,2,3],names:[:x]),axes:[:x])#Nx.Tensor<s32[x:3][3,2,1]>iex> Nx.reverse(Nx.tensor([[1,2,3],[4,5,6]],names:[:x,:y]),axes:[:x])#Nx.Tensor<s32[x:2][y:3][[4,5,6],[1,2,3]]>iex> Nx.reverse(Nx.tensor([[1,2,3],[4,5,6]],names:[:x,:y]),axes:[:y])#Nx.Tensor<s32[x:2][y:3][[3,2,1],[6,5,4]]>iex> Nx.reverse(Nx.iota({2,2,2},type::f32,names:[:x,:y,:z]),axes:[:x,:z])#Nx.Tensor<f32[x:2][y:2][z:2][[[5.0,4.0],[7.0,6.0]],[[1.0,0.0],[3.0,2.0]]]>Vectorized tensorsFor vectorized tensors, the :axes refer to the non-vectorized part.
Vectorized axes will always remain unchanged.iex> v=Nx.vectorize(Nx.iota({1,2,3}),:x)#Nx.Tensor<vectorized[x:1]s32[2][3][[[0,1,2],[3,4,5]]]>iex> Nx.reverse(v)#Nx.Tensor<vectorized[x:1]s32[2][3][[[5,4,3],[2,1,0]]]>iex> Nx.reverse(v,axes:[1])#Nx.Tensor<vectorized[x:1]s32[2][3][[[2,1,0],[5,4,3]]]>Link to this functionsort(tensor, opts \\ [])View SourceSorts the tensor along the given axis according
to the given direction.If no axis is given, defaults to 0.Options:axis - The name or number of the corresponding axis on which the sort
should be applied:direction - Can be :asc or :desc. Defaults to :asc:stable - If the sorting is stable. Defaults to falseExamplesiex> Nx.sort(Nx.tensor([16,23,42,4,8,15]))#Nx.Tensor<s32[6][4,8,15,16,23,42]>iex> t=Nx.tensor([[3,1,7],[2,5,4]],names:[:x,:y])iex> Nx.sort(t,axis::x)#Nx.Tensor<s32[x:2][y:3][[2,1,4],[3,5,7]]>iex> t=Nx.tensor([[3,1,7],[2,5,4]],names:[:x,:y])iex> Nx.sort(t,axis::y)#Nx.Tensor<s32[x:2][y:3][[1,3,7],[2,4,5]]>iex> t=Nx.tensor([[3,1,7],[2,5,4]],names:[:x,:y])iex> Nx.sort(t,axis::y,direction::asc)#Nx.Tensor<s32[x:2][y:3][[1,3,7],[2,4,5]]>iex> t=Nx.tensor(...> [...> [[4,5],[2,5],[5,0]],...> [[1,9],[2,1],[2,1]],...> [[0,-1],[-1,0],[0,-1]],...> [[-1,0],[0,-1],[-1,0]]...> ],...> names:[:x,:y,:z]...> )iex> Nx.sort(t,axis::x)#Nx.Tensor<s32[x:4][y:3][z:2][[[-1,-1],[-1,-1],[-1,-1]],[[0,0],[0,0],[0,0]],[[1,5],[2,1],[2,0]],[[4,9],[2,5],[5,1]]]>Same tensor sorted over different axes:iex> t=Nx.tensor(...> [...> [...> [4,5,2],...> [2,5,3],...> [5,0,2]...> ],...> [...> [1,9,8],...> [2,1,3],...> [2,1,4]...> ]...> ],...> names:[:x,:y,:z]...> )iex> Nx.sort(t,axis::x)#Nx.Tensor<s32[x:2][y:3][z:3][[[1,5,2],[2,1,3],[2,0,2]],[[4,9,8],[2,5,3],[5,1,4]]]>iex> Nx.sort(t,axis::y)#Nx.Tensor<s32[x:2][y:3][z:3][[[2,0,2],[4,5,2],[5,5,3]],[[1,1,3],[2,1,4],[2,9,8]]]>iex> Nx.sort(t,axis::z)#Nx.Tensor<s32[x:2][y:3][z:3][[[2,4,5],[2,3,5],[0,2,5]],[[1,8,9],[1,2,3],[1,2,4]]]>When it comes to NaN and infinities, NaN always sorts higher than
everything else:iex> t=Nx.tensor([:nan,:neg_infinity,0.0,:infinity])iex> Nx.sort(t)#Nx.Tensor<f32[4][-Inf,0.0,Inf,NaN]>iex> Nx.sort(t,direction::desc)#Nx.Tensor<f32[4][NaN,Inf,0.0,-Inf]>Link to this functionstack(tensors, opts \\ [])View SourceStacks a list of tensors with the same shape along a new axis.Tensors can be a tuple or any Nx.Container or Nx.LazyContainer.
This means you can easily concatenate all columns in a dataframe
and other data structures. For convenience, this function also allows
a list of tensors to be given, which may be common outside of defn.If no axis is provided, defaults to 0. All tensors must have the same
shape.If tensors with mixed types are given, the types will
be merged to a higher type and all of the tensors will
be cast to the higher type before concatenating.
If tensors are named, the names must match.Options:axis - optional index of the axis along which the tensors are stacked. Defaults to 0.:name - optional name for the added dimension. Defaults to an unnamed axis.ExamplesStacking always creates a new dimension:iex> Nx.stack([1,2,3])#Nx.Tensor<s32[3][1,2,3]>iex> Nx.stack([Nx.tensor([1,2,3]),Nx.tensor([4,5,6])])#Nx.Tensor<s32[2][3][[1,2,3],[4,5,6]]>The axis option can be given:iex> t1=Nx.iota({2,1,4})iex> t2=Nx.iota({2,1,4})iex> t3=Nx.iota({2,1,4})iex> Nx.stack([t1,t2,t3],axis:-1)#Nx.Tensor<s32[2][1][4][3][[[[0,0,0],[1,1,1],[2,2,2],[3,3,3]]],[[[4,4,4],[5,5,5],[6,6,6],[7,7,7]]]]>And a name can be given for the new dimension:iex> Nx.stack([Nx.tensor(1),Nx.tensor(2)],name::x)#Nx.Tensor<s32[x:2][1,2]>You can also pass any container (or lazy container) as first argument
and they are recursively traversed:iex> Nx.stack({Nx.tensor([1,2]),{Nx.tensor([3,4]),Nx.tensor([5,6])}})#Nx.Tensor<s32[3][2][[1,2],[3,4],[5,6]]>Link to this functiontop_k(tensor, opts \\ [])View SourceReturns a tuple of {values, indices} for the top k
values in last dimension of the tensor.:k is an option and must be at least 1, and less than
or equal to the size of the last dimension of the tensor.
It defaults to 1.Examplesiex> a=Nx.tensor([1,2,3,4,5])iex> {values,indices}=Nx.top_k(a,k:2)iex> values#Nx.Tensor<s32[2][5,4]>iex> indices#Nx.Tensor<s32[2][4,3]>:k defaults to 1:iex> a=Nx.tensor([[1.0,2.0,3.0],[4.0,5.0,6.0]])iex> {values,indices}=Nx.top_k(a)iex> values#Nx.Tensor<f32[2][1][[3.0],[6.0]]>iex> indices#Nx.Tensor<s32[2][1][[2],[2]]>When it comes to NaN and infinities, NaN always sorts higher than
everything else:iex> t=Nx.tensor([:nan,:neg_infinity,:infinity,0.0])iex> {values,indices}=Nx.top_k(t,k:3)iex> values#Nx.Tensor<f32[3][NaN,Inf,0.0]>iex> indices#Nx.Tensor<s32[3][0,2,3]>Error casesiex> a=Nx.tensor([1,2,3,4,5])iex> Nx.top_k(a,k:6)** (ArgumentError) top_k input last axis size must be greater than or equal to k, got size=5 and k=6iex> a=Nx.tensor(1)iex> Nx.top_k(a,k:1)** (ArgumentError) top_k input must have at least rank 1Functions: ShapeLink to this functionaxes(shape)View SourceReturns all of the axes in a tensor.If a shape is given, it returns the axes for the given shape.Examplesiex> Nx.axes(Nx.tensor([[1,2,3],[4,5,6]]))[0,1]iex> Nx.axes(1)[]iex> Nx.axes({1,2,3})[0,1,2]Link to this functionaxis_index(tensor, axis)View SourceReturns the index of the given axis in the tensor.Examplesiex> Nx.axis_index(Nx.iota({100,10,20}),0)0iex> Nx.axis_index(Nx.iota({100,10,20}),-1)2iex> Nx.axis_index(Nx.iota({100,10,20},names:[:batch,:x,:y]),:x)1Error casesiex> Nx.axis_index(Nx.iota({100,10,20}),3)** (ArgumentError) given axis (3) invalid for shape with rank 3iex> Nx.axis_index(Nx.iota({100,10,20},names:[:batch,:x,:y]),:z)** (ArgumentError) name :z not found in tensor with names [:batch, :x, :y]Link to this functionaxis_size(tensor, axis)View SourceReturns the size of a given axis of a tensor.It accepts either an atom as the name or an integer as the axis.
It raises if the axis/name does not exist.Examplesiex> Nx.axis_size(Nx.iota({100,10,20}),0)100iex> Nx.axis_size(Nx.iota({100,10,20},names:[:batch,:x,:y]),:y)20Link to this functionbit_size(tensor)View SourceReturns the bit size of the data in the tensor
computed from its shape and type.Examplesiex> Nx.bit_size(Nx.tensor([[1,2,3],[4,5,6]]))192iex> Nx.bit_size(Nx.tensor([[1,2,3],[4,5,6]],type::u8))48iex> Nx.bit_size(Nx.tensor([[1,2,3],[3,2,1]],type::u2))12iex> Nx.bit_size(1)32Vectorized tensors account for all elementsiex> Nx.bit_size(Nx.tensor([[1,2],[3,4]])|>Nx.vectorize(:x))128Link to this functionbroadcast(tensor, shape, opts \\ [])View SourceBroadcasts tensor to the given broadcast_shape.The new shape is either a tuple or a tensor which we will
retrieve the current shape from. The broadcast shape must
be of equal or higher rank than the current shape.An optional :axes can be given to customize how broadcasting
happens. axes must be a list with the same length as the
tensor shape. Each axis in the list maps to the dimension
in the broadcast shape that must match. For example, an axis
of [1, 2] says the 0 dimension of the tensor matches to
the 1 dimension of the broadcast shape and the 1 dimension
of the tensor matches the 2 dimension of the broadcast shape.
Each matching dimension must either be 1, for implicit
broadcasting, or match the dimension in the broadcast shape.Broadcasting is destructive with respect to names. You can
optionally provide new :names for the new tensor. If you
pass a tensor with named dimensions, the new tensor will
inherit names from that tensor.ExamplesWithout axesiex> Nx.broadcast(1,{1,2,3})#Nx.Tensor<s32[1][2][3][[[1,1,1],[1,1,1]]]>iex> Nx.broadcast(Nx.tensor([[1],[2]],names:[:x,:y]),Nx.tensor([[10,20],[30,40]],names:[:i,:j]))#Nx.Tensor<s32[i:2][j:2][[1,1],[2,2]]>iex> Nx.broadcast(Nx.tensor([[1,2]],names:[:x,:y]),Nx.tensor([[10,20],[30,40]],names:[:i,:j]))#Nx.Tensor<s32[i:2][j:2][[1,2],[1,2]]>Note that, even if there is no broadcasting because the
shape is the same, names are discarded if none are given:iex> Nx.broadcast(Nx.iota({2,2},names:[:x,:y]),{2,2})#Nx.Tensor<s32[2][2][[0,1],[2,3]]>iex> Nx.broadcast(Nx.iota({2,2},names:[:x,:y]),{2,2},names:[:i,:j])#Nx.Tensor<s32[i:2][j:2][[0,1],[2,3]]>With axesUsing the default broadcast rules, we cannot broadcast a
tensor of shape (3) to the shape (3, 2), because the lower
dimensions must match. But with Nx.broadcast/3 we can
configure how the dimensions match:iex> t=Nx.tensor([1,2,3])iex> Nx.broadcast(t,{3,2},axes:[0],names:[:x,:y])#Nx.Tensor<s32[x:3][y:2][[1,1],[2,2],[3,3]]>Or a more complex example:iex> t=Nx.tensor([1,2,3])iex> Nx.broadcast(t,{2,3,2},axes:[1],names:[:x,:y,:z])#Nx.Tensor<s32[x:2][y:3][z:2][[[1,1],[2,2],[3,3]],[[1,1],[2,2],[3,3]]]>Vectorized tensorsVectorized axes remain unchanged, and normal broadcast rules apply otherwise.iex> a=Nx.tensor([[[1,2,3]],[[4,5,6]]])|>Nx.vectorize(:x)iex> Nx.broadcast(a,{2,3})#Nx.Tensor<vectorized[x:2]s32[2][3][[[1,2,3],[1,2,3]],[[4,5,6],[4,5,6]]]>For tensors as shapes, the broadcast will only take the shape in consideration.iex> a=Nx.tensor([[1,2,3],[4,5,6]])|>Nx.vectorize(:x)#Nx.Tensor<vectorized[x:2]s32[3][[1,2,3],[4,5,6]]>iex> b=Nx.tensor([[[1,2,3],[4,5,6]]],names:[nil,nil,:y])|>Nx.vectorize(:a)#Nx.Tensor<vectorized[a:1]s32[2][y:3][[[1,2,3],[4,5,6]]]>iex> Nx.broadcast(a,b,axes:[1],names:[:i,:j])#Nx.Tensor<vectorized[x:2]s32[i:2][j:3][[[1,2,3],[1,2,3]],[[4,5,6],[4,5,6]]]>Link to this functionbyte_size(tensor)View SourceReturns the byte size of the data in the tensor
computed from its shape and type.If the tensor has s2/s4/u2/u4 types, the value
will be rounded down. Consider using bit_size/1
instead.Examplesiex> Nx.byte_size(Nx.tensor([[1,2,3],[4,5,6]]))24iex> Nx.byte_size(Nx.tensor([[1.0,2.0,3.0],[4.0,5.0,6.0]]))24iex> Nx.byte_size(Nx.tensor([[1,2,3],[4,5,6]],type::u8))6iex> Nx.byte_size(1)4Vectorized tensors account for all elementsiex> Nx.byte_size(Nx.tensor([[1,2],[3,4]])|>Nx.vectorize(:x))16Link to this functioncompatible?(left, right)View SourceChecks if two tensors have the same shape, type, and compatible names.The data in the tensor is ignored.Note: This function cannot be used in defn.Examplesiex> Nx.compatible?(Nx.iota({3,2}),Nx.iota({3,2}))trueiex> Nx.compatible?(Nx.iota({3,2}),Nx.iota({3,2},names:[:rows,:columns]))trueiex> Nx.compatible?(...> Nx.iota({3,2},names:[:rows,nil]),...> Nx.iota({3,2},names:[nil,:columns])...> )trueiex> Nx.compatible?(...> Nx.iota({3,2},names:[:foo,:bar]),...> Nx.iota({3,2},names:[:rows,:columns])...> )falseiex> Nx.compatible?(Nx.iota({3,2}),Nx.iota({2,3}))falseiex> Nx.compatible?(Nx.iota({2,2}),Nx.iota({2,2},type::f32))falseUsing collections:iex> Nx.compatible?({Nx.iota({3,2}),{1,2}},{Nx.iota({3,2}),{3,4}})trueiex> Nx.compatible?(%{foo:Nx.iota({3,2})},%{foo:Nx.iota({3,2})})trueiex> Nx.compatible?(%{foo:Nx.iota({3,2})},%{bar:Nx.iota({3,2})})falseVectorized tensorsSame compatibility criteria applies to vectorized tensors, but there's
the additional requirement that vectorized axes must be the same in both
tensors.iex> Nx.compatible?(Nx.tensor([1,2])|>Nx.vectorize(:x),Nx.tensor([3,4])|>Nx.vectorize(:x))trueiex> Nx.compatible?(Nx.tensor([1,2,3])|>Nx.vectorize(:x),Nx.tensor([1,2])|>Nx.vectorize(:x))falseiex> Nx.compatible?(Nx.tensor([1])|>Nx.vectorize(:x),Nx.tensor([1,2])|>Nx.vectorize(:y))falseLink to this functionflat_size(tensor)View SourceReturns the number of elements in the tensor (including vectorized axes).See also: size/1Examplesiex> Nx.flat_size(Nx.tensor([[1,2,3],[4,5,6]]))6iex> Nx.flat_size(10)1iex> t=Nx.iota({4,3,2})iex> v1=Nx.vectorize(t,:x)iex> Nx.flat_size(v1)24iex> Nx.flat_size(Nx.vectorize(v1,:y))24Link to this functionflatten(tensor, opts \\ [])View SourceFlattens a n-dimensional tensor to a 1-dimensional tensor.Flattening only changes the tensor metadata, it doesn't
copy the underlying structure.Flatten is a destructive operation with respect to names.Examplesiex> t=Nx.iota({2,2,2,2})#Nx.Tensor<s32[2][2][2][2][[[[0,1],[2,3]],[[4,5],[6,7]]],[[[8,9],[10,11]],[[12,13],[14,15]]]]>iex> Nx.flatten(t)#Nx.Tensor<s32[16][0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15]>And if the tensor is already 1-dimensional:iex> t=Nx.iota({16})#Nx.Tensor<s32[16][0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15]>iex> Nx.flatten(t)#Nx.Tensor<s32[16][0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15]>You may also pass :axes to Nx.flatten/2, to specify which consecutive
axes to flatten:iex> t=Nx.iota({1,2,3})#Nx.Tensor<s32[1][2][3][[[0,1,2],[3,4,5]]]>iex> Nx.flatten(t,axes:[1,2])#Nx.Tensor<s32[1][6][[0,1,2,3,4,5]]>:axes must be consecutive, otherwise it will raise:iex> t=Nx.iota({1,2,3})#Nx.Tensor<s32[1][2][3][[[0,1,2],[3,4,5]]]>iex> Nx.flatten(t,axes:[0,2])** (ArgumentError) flatten axes must be consecutiveVectorized tensorsOnly the inner shape is flattened, leaving vectorized axes untouched.iex> t=Nx.iota({1,3,2,2})|>Nx.vectorize(:x)|>Nx.vectorize(:y)iex> Nx.flatten(t)#Nx.Tensor<vectorized[x:1][y:3]s32[4][[[0,1,2,3],[4,5,6,7],[8,9,10,11]]]>Link to this functionnames(a)View SourceReturns all of the names in a tensor.Examplesiex> Nx.names(Nx.tensor([[1,2,3],[4,5,6]],names:[:batch,:data]))[:batch,:data]iex> Nx.names(Nx.tensor([1,2,3]))[nil]iex> Nx.names(5)[]Link to this functionnew_axis(tensor, axis, name \\ nil)View SourceAdds a new axis of size 1 with optional name.Examplesiex> t=Nx.tensor([[1,2,3],[4,5,6]])iex> Nx.new_axis(t,0,:new)#Nx.Tensor<s32[new:1][2][3][[[1,2,3],[4,5,6]]]>iex> Nx.new_axis(t,1,:new)#Nx.Tensor<s32[2][new:1][3][[[1,2,3]],[[4,5,6]]]>iex> Nx.new_axis(t,2,:new)#Nx.Tensor<s32[2][3][new:1][[[1],[2],[3]],[[4],[5],[6]]]>Axis can also be negative, which will start from the back:iex> t=Nx.tensor([[1,2,3],[4,5,6]])iex> Nx.new_axis(t,-1,:new)#Nx.Tensor<s32[2][3][new:1][[[1],[2],[3]],[[4],[5],[6]]]>Vectorized tensorsSimilarly to reshape/2, vectorized tensors will have their
vectors unchanged. The examples below show that the new axes
only affect the tensor shape.iex> t=Nx.tensor([1])|>Nx.vectorize(:x)#Nx.Tensor<vectorized[x:1]s32[1]>iex> t=Nx.new_axis(t,-1,:new)#Nx.Tensor<vectorized[x:1]s32[new:1][[1]]>iex> Nx.new_axis(t,0)#Nx.Tensor<vectorized[x:1]s32[1][new:1][[[1]]]>Link to this functionpad(tensor, pad_value, padding_config)View SourcePads a tensor with a given value.You must specify a padding configuration. A padding
configuration is a list of tuples consisting of
{pad_width_low, pad_width_high, pad_width_interior}
for each dimension in the input tensor. The padding
configuration must be of the same length as the tensor shape.Padding widths can be negative. If they are negative,
the tensor is clipped on either end according to the
padding width. Interior padding widths cannot be negative.See also: reflect/2Examplesiex> Nx.pad(Nx.tensor(1),0,[])#Nx.Tensor<s321>iex> Nx.pad(Nx.tensor([1,2,3],names:[:data]),0,[{1,1,0}])#Nx.Tensor<s32[data:5][0,1,2,3,0]>iex> Nx.pad(Nx.tensor([[1,2,3],[4,5,6]]),0,[{0,0,1},{0,0,1}])#Nx.Tensor<s32[3][5][[1,0,2,0,3],[0,0,0,0,0],[4,0,5,0,6]]>iex> Nx.pad(Nx.tensor([[1,2,3],[4,5,6]]),0,[{1,1,0},{1,1,0}])#Nx.Tensor<s32[4][5][[0,0,0,0,0],[0,1,2,3,0],[0,4,5,6,0],[0,0,0,0,0]]>iex> tensor=Nx.tensor([[[1,2],[3,4]],[[5,6],[7,8]]])iex> Nx.pad(tensor,0,[{0,2,0},{1,1,0},{1,0,0}])#Nx.Tensor<s32[4][4][3][[[0,0,0],[0,1,2],[0,3,4],[0,0,0]],[[0,0,0],[0,5,6],[0,7,8],[0,0,0]],[[0,0,0],[0,0,0],[0,0,0],[0,0,0]],[[0,0,0],[0,0,0],[0,0,0],[0,0,0]]]>iex> tensor=Nx.tensor([[[1,2],[3,4]],[[5,6],[7,8]]])iex> Nx.pad(tensor,0,[{1,0,0},{1,1,0},{0,1,0}])#Nx.Tensor<s32[3][4][3][[[0,0,0],[0,0,0],[0,0,0],[0,0,0]],[[0,0,0],[1,2,0],[3,4,0],[0,0,0]],[[0,0,0],[5,6,0],[7,8,0],[0,0,0]]]>iex> tensor=Nx.tensor([[[1.0,2.0],[3.0,4.0]],[[5.0,6.0],[7.0,8.0]]])iex> Nx.pad(tensor,0.0,[{1,2,0},{1,0,0},{0,1,0}])#Nx.Tensor<f32[5][3][3][[[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0]],[[0.0,0.0,0.0],[1.0,2.0,0.0],[3.0,4.0,0.0]],[[0.0,0.0,0.0],[5.0,6.0,0.0],[7.0,8.0,0.0]],[[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0]],[[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0]]]>iex> Nx.pad(Nx.tensor([0,1,2,3,0]),0,[{-1,-1,0}])#Nx.Tensor<s32[3][1,2,3]>iex> tensor=Nx.tensor([...> [[0,0,0],[0,0,0],[0,0,0],[0,0,0]],...> [[0,0,0],[1,2,0],[3,4,0],[0,0,0]],...> [[0,0,0],[5,6,0],[7,8,0],[0,0,0]]...> ])iex> Nx.pad(tensor,0,[{-1,0,0},{-1,-1,0},{0,-1,0}])#Nx.Tensor<s32[2][2][2][[[1,2],[3,4]],[[5,6],[7,8]]]>iex> tensor=Nx.tensor([[0,1,2,3],[0,4,5,6]])iex> Nx.pad(tensor,0,[{0,0,0},{-1,1,0}])#Nx.Tensor<s32[2][4][[1,2,3,0],[4,5,6,0]]>iex> tensor=Nx.tensor([[0,1,2],[3,4,5]],type::f32)iex> Nx.pad(tensor,0,[{-1,2,0},{1,-1,0}])#Nx.Tensor<f32[3][3][[0.0,3.0,4.0],[0.0,0.0,0.0],[0.0,0.0,0.0]]>Vectorized tensorsLike with the non-vectorized case, pad_value must be a non-vectorized scalar tensor.
Vectorized axes remain unchanged.iex> t=Nx.tensor([[1],[2],[3]],names:[nil,:data])|>Nx.vectorize(:x)iex> Nx.pad(t,0,[{1,1,0}])#Nx.Tensor<vectorized[x:3]s32[data:3][[0,1,0],[0,2,0],[0,3,0]]>Link to this functionrank(shape)View SourceReturns the rank of a tensor.If a tuple is given as a shape, it computes the rank
of the given tuple.Examplesiex> Nx.rank(Nx.tensor(1))0iex> Nx.rank(Nx.tensor([[1,2,3],[4,5,6]]))2iex> Nx.rank(1)0iex> Nx.rank({1,2,3})3Link to this functionreflect(tensor, opts \\ [])View SourcePads a tensor of rank 1 or greater along the given axes through periodic reflections.Options:padding_config - A list of tuples in the format {pre, post},
which specify the length (0 or greater) of the reflection before and
after the tensor along a each axis.See also: pad/3Examplesiex> Nx.reflect(Nx.tensor([0,1,2]),padding_config:[{3,1}])#Nx.Tensor<s32[7][1,2,1,0,1,2,1]>iex> Nx.reflect(Nx.tensor([[0,1,2],[3,4,5]],names:[:x,:y]),padding_config:[{2,0},{2,1}])#Nx.Tensor<s32[x:4][y:6][[2,1,0,1,2,1],[5,4,3,4,5,4],[2,1,0,1,2,1],[5,4,3,4,5,4]]>Link to this functionrename(tensor, names)View SourceAdds (or overrides) the given names to the tensor.Examplesiex> Nx.rename(Nx.iota({2,3}),[:foo,:bar])#Nx.Tensor<s32[foo:2][bar:3][[0,1,2],[3,4,5]]>Vectorized tensorsOnly the inner axis names are renamed. New names must not overlap with
vectorized names.iex> t=Nx.tensor([[1],[2],[3]],names:[nil,:y])|>Nx.vectorize(:x)iex> Nx.rename(t,[:a])#Nx.Tensor<vectorized[x:3]s32[a:1][[1],[2],[3]]>iex> Nx.rename(t,[:x])** (ArgumentError) name :x is already a name for a vectorized axisLink to this functionreshape(tensor, new_shape, opts \\ [])View SourceChanges the shape of a tensor.The new shape is either a tuple or a tensor which we will
retrieve the current shape from. The shapes must be compatible:
the product of each dimension in the shape must be equal.You may specify one of the dimensions as :auto. Nx will compute
the size of the dimension based on the original shape and new shape.Reshaping only changes the tensor metadata, it doesn't copy
the underlying structure.Reshape is a destructive operation with respect to names. You
can optionally provide :names for each of the dimensions
in the reshaped tensor. If you do not provide :names, they
will be taken from the tensor the shape is taken from or
all of the dimension names will be set to nil.Examplesiex> t=Nx.tensor([1,2,3,4],names:[:x])iex> Nx.reshape(t,{2,2},names:[:x,:y])#Nx.Tensor<s32[x:2][y:2][[1,2],[3,4]]>The shape can also be an existing tensor:iex> shape=Nx.tensor([[0],[0],[0],[0]],names:[:x,:y])iex> Nx.reshape(Nx.tensor([1,2,3,4]),shape)#Nx.Tensor<s32[x:4][y:1][[1],[2],[3],[4]]>Even a scalar can be transformed into a 3-dimensional tensor:iex> t=Nx.tensor(1)iex> Nx.reshape(t,{1,1,1},names:[:x,:y,:z])#Nx.Tensor<s32[x:1][y:1][z:1][[[1]]]>You can use :auto to infer dimension sizes. This is useful when you
don't know the size some dimension should be ahead of time:iex> t=Nx.tensor([[1,2,3],[4,5,6]])iex> Nx.reshape(t,{:auto,2},names:[:x,:y])#Nx.Tensor<s32[x:3][y:2][[1,2],[3,4],[5,6]]>Vectorized tensorsVectorized tensors have their inner shapes changed, keeping vectors unchanged.iex> t=Nx.tensor([[[1,2,3],[4,5,6]]])|>Nx.vectorize(:x)iex> t.shape{2,3}iex> Nx.reshape(t,{3,2})#Nx.Tensor<vectorized[x:1]s32[3][2][[[1,2],[3,4],[5,6]]]>Link to this functionshape(number)View SourceReturns the shape of the tensor as a tuple.The size of this tuple gives the rank of the tensor.If a shape as a tuple is given, it returns the shape itself.Examplesiex> Nx.shape(Nx.tensor(1)){}iex> Nx.shape(Nx.tensor([[1,2,3],[4,5,6]])){2,3}iex> Nx.shape(1){}iex> Nx.shape({1,2,3}){1,2,3}Link to this functionsize(shape)View SourceReturns the number of elements in the tensor.If a tuple is given, it returns the number of elements in a tensor with that shape.
Vectorized tensors will not include vectorized axes sizes. See flat_size/1.Examplesiex> Nx.size(Nx.tensor([[1,2,3],[4,5,6]]))6iex> Nx.size(1)1iex> Nx.size({1,2,3,2})12iex> Nx.size(Nx.vectorize(Nx.iota({4,3,2}),:x))6Link to this functionsqueeze(tensor, opts \\ [])View SourceSqueezes the given size 1 dimensions out of the tensor.If no axes are given, squeezes all size 1 dimensions
from the tensor.While this is equivalent to a reshape which eliminates
the size 1 axes, squeeze preserves important information
about which axes were squeezed out which can then be used
later on in transformations.Examplesiex> Nx.squeeze(Nx.tensor([[[[[1]]]]]))#Nx.Tensor<s321>iex> Nx.squeeze(Nx.tensor([[[[1]]],[[[2]]]],names:[:x,:y,:z,:i]))#Nx.Tensor<s32[x:2][1,2]>iex> Nx.squeeze(Nx.tensor([[1,2,3]],names:[:x,:y]),axes:[:x])#Nx.Tensor<s32[y:3][1,2,3]>iex> Nx.squeeze(Nx.tensor([[1],[2]],names:[:x,:y]),axes:[:y])#Nx.Tensor<s32[x:2][1,2]>Vectorized tensorssqueeze/2 operates on the tensor's shape, leaving vectorized axes untouched.iex> t=Nx.tensor([[[[[1],[2],[3]]]]])|>Nx.vectorize(:x)#Nx.Tensor<vectorized[x:1]s32[1][1][3][1][[[[[1],[2],[3]]]]]>iex> Nx.squeeze(t)#Nx.Tensor<vectorized[x:1]s32[3][[1,2,3]]>iex> Nx.squeeze(t,axes:[0,1])#Nx.Tensor<vectorized[x:1]s32[3][1][[[1],[2],[3]]]>Error casesiex> Nx.squeeze(Nx.tensor([[1,2,3],[4,5,6]]),axes:[1])** (ArgumentError) cannot squeeze dimensions whose sizes are not 1, got 3 for dimension 1iex> Nx.squeeze(Nx.tensor([[[[[1]]]]]),axes:[0,0])** (ArgumentError) axes [0, 0] must be unique integers between 0 and 4Link to this functiontile(tensor, repetitions)View SourceCreates a new tensor by repeating the input tensor
along the given axes.If the tensor has less dimensions than the repetitions given,
the tensor will grow in dimensionality.If the tensor has more dimensions than the repetitions given,
tiling is done from the rightmost dimensions (i.e. if the input
shape is {1,2,3} and repetitions = [2], the result is the same
as if repetitions = [1,1,2]).Examplesiex> a=Nx.tensor([0,1,2])iex> Nx.tile(a,[2])#Nx.Tensor<s32[6][0,1,2,0,1,2]>iex> Nx.tile(a,[1,2])#Nx.Tensor<s32[1][6][[0,1,2,0,1,2]]>iex> Nx.tile(a,[2,2])#Nx.Tensor<s32[2][6][[0,1,2,0,1,2],[0,1,2,0,1,2]]>iex> Nx.tile(a,[2,1])#Nx.Tensor<s32[2][3][[0,1,2],[0,1,2]]>iex> Nx.tile(a,[2,1,2])#Nx.Tensor<s32[2][1][6][[[0,1,2,0,1,2]],[[0,1,2,0,1,2]]]>iex> b=Nx.tensor([[1,2],[3,4]])iex> Nx.tile(b,[2])#Nx.Tensor<s32[2][4][[1,2,1,2],[3,4,3,4]]>iex> Nx.tile(b,[2,1])#Nx.Tensor<s32[4][2][[1,2],[3,4],[1,2],[3,4]]>iex> Nx.tile(b,[1,2])#Nx.Tensor<s32[2][4][[1,2,1,2],[3,4,3,4]]>iex> c=Nx.tensor([1,2,3,4])iex> Nx.tile(c,[4,1])#Nx.Tensor<s32[4][4][[1,2,3,4],[1,2,3,4],[1,2,3,4],[1,2,3,4]]>Vectorized tensorsLike reshape/2, tile/2 works on the shape, leaving vectors untouched.iex> t=Nx.vectorize(Nx.tensor([[1,2,3],[4,5,6]]),:x)iex> Nx.tile(t,[1,3,1])#Nx.Tensor<vectorized[x:2]s32[1][3][3][[[[1,2,3],[1,2,3],[1,2,3]]],[[[4,5,6],[4,5,6],[4,5,6]]]]>Error casesiex> Nx.tile(Nx.tensor([1,2]),1.0)** (ArgumentError) repetitions must be a list of integers, got: 1.0iex> Nx.tile(Nx.tensor([1,2]),[1,1.0])** (ArgumentError) repetitions must be a list of integers, got: [1, 1.0]iex> Nx.tile(Nx.tensor([1,2]),nil)** (ArgumentError) repetitions must be a list of integers, got: nilLink to this functiontranspose(tensor, opts \\ [])View SourceTransposes a tensor to the given axes.If no axes are given, the default behavior is to
reverse the order of the original tensor's axes.The axes is a list of integers or dimension names
containing how the new dimensions must be ordered.
The highest dimension is zero.Examplesiex> Nx.transpose(Nx.tensor(1))#Nx.Tensor<s321>iex> Nx.transpose(Nx.iota({2,3,4},names:[:x,:y,:z]))#Nx.Tensor<s32[z:4][y:3][x:2][[[0,12],[4,16],[8,20]],[[1,13],[5,17],[9,21]],[[2,14],[6,18],[10,22]],[[3,15],[7,19],[11,23]]]>iex> Nx.transpose(Nx.tensor(1),axes:[])#Nx.Tensor<s321>iex> Nx.transpose(Nx.iota({2,3,4},names:[:batch,:x,:y]),axes:[2,1,:batch])#Nx.Tensor<s32[y:4][x:3][batch:2][[[0,12],[4,16],[8,20]],[[1,13],[5,17],[9,21]],[[2,14],[6,18],[10,22]],[[3,15],[7,19],[11,23]]]>Vectorized tensorsFor vectorized tensors, transpose will manipulate the inner shape only,
keeping the order of vectorized axes the same.iex> v=Nx.vectorize(Nx.iota({1,2,3}),:x)#Nx.Tensor<vectorized[x:1]s32[2][3][[[0,1,2],[3,4,5]]]>iex> Nx.transpose(v)#Nx.Tensor<vectorized[x:1]s32[3][2][[[0,3],[1,4],[2,5]]]>iex> Nx.transpose(v,axes:[1,0])#Nx.Tensor<vectorized[x:1]s32[3][2][[[0,3],[1,4],[2,5]]]>Errorsiex> Nx.transpose(Nx.iota({2,2},names:[:batch,:x]),axes:[:batch])** (ArgumentError) expected length of permutation (1) to match rank of shape (2)iex> Nx.transpose(Nx.iota({2,2}),axes:[1,2])** (ArgumentError) given axis (2) invalid for shape with rank 2Functions: VectorizationLink to this functionbroadcast_vectors(tensors_or_containers, opts \\ [])View SourceBroadcasts vectorized axes, ensuring they end up with the same final size.The inner shape is unchanged for each tensor.
The order of the vectorized axes is determined by order of appearance in the input list.Options:align_ranks - boolean that indicates whether the inner
shapes should be aligned to the maximum rank of the inputs.
That is, 1-sized leading dimensions are added so
that all tensors have the same rank in the output.
This only applies in case one of the inputs is vectorized.Examplesiex> x=Nx.tensor([1,2])|>Nx.vectorize(:x)#Nx.Tensor<vectorized[x:2]s32[1,2]>iex> xy=Nx.tensor([[[5]],[[6]]])|>Nx.vectorize(:y)|>Nx.vectorize(:x)#Nx.Tensor<vectorized[y:2][x:1]s32[1][[[5]],[[6]]]>iex> [broadcast_x,broadcast_xy]=Nx.broadcast_vectors([x,xy],align_ranks:true)iex> broadcast_x#Nx.Tensor<vectorized[x:2][y:2]s32[1][[[1],[1]],[[2],[2]]]>iex> broadcast_xy#Nx.Tensor<vectorized[x:2][y:2]s32[1][[[5],[6]],[[5],[6]]]>iex> [broadcast_xy,broadcast_x]=Nx.broadcast_vectors([xy,x])iex> broadcast_x#Nx.Tensor<vectorized[y:2][x:2]s32[[1,2],[1,2]]>iex> broadcast_xy#Nx.Tensor<vectorized[y:2][x:2]s32[1][[[5],[5]],[[6],[6]]]>Link to this functiondevectorize(tensor_or_container, opts \\ [])View SourceTransforms a vectorized tensor back into a regular tensor.Options:keep_names - a boolean indicating whether
vectorized axes' names should be turned into the new
axes' names. Defaults to true.Examplesiex> t=Nx.iota({1,2,3})|>Nx.vectorize(:x)|>Nx.vectorize(:y)#Nx.Tensor<vectorized[x:1][y:2]s32[3][[[0,1,2],[3,4,5]]]>iex> Nx.devectorize(t)#Nx.Tensor<s32[x:1][y:2][3][[[0,1,2],[3,4,5]]]>iex> Nx.devectorize(t,keep_names:false)#Nx.Tensor<s32[1][2][3][[[0,1,2],[3,4,5]]]>ContainersContainers are also supported:iex> input={1,%{a:Nx.iota({3},vectorized_axes:[x:1])}}iex> {t1,%{a:t2}}=Nx.devectorize(input)iex> t1#Nx.Tensor<s321>iex> t2#Nx.Tensor<s32[x:1][3][[0,1,2]]>Link to this functionreshape_vectors(tensors_or_containers, opts \\ [])View SourceReshapes input tensors so that they are all vectorized with the same vectors.For vectors with the same name to be compatible, they need to either
have the same size or one must be of size 1.Options:align_ranks - boolean that indicates whether the inner
shapes should be aligned to the maximum rank of the inputs.
That is, 1-sized leading dimensions are added so
that all tensors have the same rank in the output.
This only applies in case one of the inputs is vectorized.ExamplesTwo vectors of the same name are compatible if they have the same sizes or if either has size 1.iex> x=Nx.tensor([1,2,3])|>Nx.vectorize(:x)iex> xy=Nx.tensor([[[5]],[[6]]])|>Nx.vectorize(:y)|>Nx.vectorize(:x)iex> [x,xy]=Nx.reshape_vectors([x,xy])iex> x.vectorized_axes[x:3,y:1]iex> xy.vectorized_axes[x:1,y:2]The resulting tensors will all present the combined vectors in the
same order in which each unique vector appears in the input.
The example below shows how this behaves for a pair of tensors.iex> x=Nx.tensor([1,2,3])|>Nx.vectorize(:x)iex> y=Nx.tensor([4])|>Nx.vectorize(:y)iex> [xv,yv]=Nx.reshape_vectors([x,y])iex> xv.vectorized_axes[x:3,y:1]iex> yv.vectorized_axes[x:1,y:1]iex> [yv,xv]=Nx.reshape_vectors([y,x])iex> xv.vectorized_axes[y:1,x:3]iex> yv.vectorized_axes[y:1,x:1]The :align_ranks option controls whether the resulting tensors should end up
with the same rank, which helps with broadcasting in some cases.iex> x=1iex> y=Nx.tensor([[[1],[1]],[[2],[2]],[[3],[3]]])|>Nx.vectorize(:y)iex> [xv,yv]=Nx.reshape_vectors([x,y])iex> xv#Nx.Tensor<vectorized[y:1]s32[1]>iex> yv#Nx.Tensor<vectorized[y:3]s32[2][1][[[1],[1]],[[2],[2]],[[3],[3]]]>iex> [xv,_yv]=Nx.reshape_vectors([x,y],align_ranks:true)iex> xv#Nx.Tensor<vectorized[y:1]s32[1][1][[[1]]]>Link to this functionrevectorize(tensor, target_axes, opts \\ [])View SourceChanges the disposition of the vectorized axes of a tensor or Nx.Container.This function is basically a short-hand for:tensor|>Nx.devectorize(keep_names:false)|>Nx.reshape(vectorized_sizes++target_shape,names:target_names)|>Nx.vectorize(vectorized_names)Accepts the target_axes keyword list where the total size must match the current total
size of the vectorized axes.Between target_axes and the :target_shape option, there can be at most one :auto entry.Options:target_shape - the (non-vectorized) output shape.:target_names - the names for the output shape.Examplesiex> t=Nx.iota({1},vectorized_axes:[x:2,y:3,z:4])iex> t2=Nx.revectorize(t,x:12,y::auto)iex> t2.vectorized_axes[x:12,y:2]iex> t3=Nx.revectorize(t,a::auto)iex> t3.vectorized_axes[a:24]Also works on containers. Note that the revectorization happens on a per-entry basis.iex> t1=Nx.iota({1},vectorized_axes:[x:2,y:3])iex> t2=Nx.iota({1},vectorized_axes:[x:2,y:1])iex> {r1,r2}=Nx.revectorize({t1,t2},a::auto)iex> r1.vectorized_axes[a:6]iex> r2.vectorized_axes[a:2]This function is useful for when you need to introduce a temporary custom axis to ease calculations.
The example below shows how to manipulate your vectorized tensor for that objective.iex> t=Nx.iota({2,2,2})|>Nx.vectorize(x:2,y:2)#Nx.Tensor<vectorized[x:2][y:2]s32[2][[[0,1],[2,3]],[[4,5],[6,7]]]>iex> Nx.revectorize(t,temp::auto,x:2)# Note that if we don't pass `:target_shape`, `:auto` will only act upon the vectorized axes#Nx.Tensor<vectorized[temp:2][x:2]s32[2][[[0,1],[2,3]],[[4,5],[6,7]]]>iex> revec=Nx.revectorize(t,[temp::auto,x:2],target_shape:{})#Nx.Tensor<vectorized[temp:4][x:2]s32[[0,1],[2,3],[4,5],[6,7]]>iex> Nx.revectorize(revec,[new_vec:2],target_shape:{1,4},target_names:[:x,:last])#Nx.Tensor<vectorized[new_vec:2]s32[x:1][last:4][[[0,1,2,3]],[[4,5,6,7]]]>Note how in the last example the :x name could be reused in various positions
(both vectorized and non-vectorized), because revectorize/2 ensures that the
names are rewritten at each call.Link to this functionvectorize(tensor, name_or_axes)View Source@spec vectorize(
  tensor :: Nx.Tensor.t(),
  name_or_axes :: atom() | [atom() | {atom(), pos_integer()}]
) :: Nx.Tensor.t()Transforms a tensor into a vectorized tensor.Each vectorization removes the leading axes from the shape and appends them to
the :vectorized_axes list for the tensor.The vectorization specification can be a list of atoms or {atom, pos_integer}
pairs. If a single atom is given, it behaves as a single-element list.
The atom names the vectorized axes. If a pair is given, we also verify
that the given size matches the size of the to-be-vectorized axis.In the examples below, we discuss in more detail how a vectorized tensor works.ExamplesIn this first example, we turn a {2, 3}-shaped tensor into a vectorized tensor
with 1 vectorized axes and rank 1 shape, {3}, and then into a vectorized tensor
with 2 vectorized axes and rank 0 shape.iex> t=Nx.iota({2,3})iex> vectorized=Nx.vectorize(t,:first)#Nx.Tensor<vectorized[first:2]s32[3][[0,1,2],[3,4,5]]>iex> Nx.vectorize(vectorized,:second)#Nx.Tensor<vectorized[first:2][second:3]s32[[0,1,2],[3,4,5]]>You can also vectorize multiple axes at once by passing a list,
as seen in the examples below. The first example doesn't validate
sizes. The second ensures the second axis has size 3.iex> t=Nx.iota({2,3})iex> v1=Nx.vectorize(t,[:first,:second])#Nx.Tensor<vectorized[first:2][second:3]s32[[0,1,2],[3,4,5]]>iex> v2=Nx.vectorize(t,[:first,second:3])iex> v1==v2trueA vectorized tensor can be thought of as a tensor that signals
to Nx that any operation applied on it must instead be applied
to each individual entry for the vectorized axis.
Nested vectorizations just apply this idea recursively, ultimately
applying the operation to each non-vectorized entry.In the following example, notice that you don't need to have the
second argument shaped in a way that can be broadcasted, because
vectorization handles that automatically.In the example below, shape {4} isn't broadcast-compatible with {2}:iex> Nx.add(Nx.tensor([4,3,2,1]),Nx.tensor([0,1]))** (ArgumentError) cannot broadcast tensor of dimensions {4} to {2}If we want to add the two tensors, normally we would need to reshape
to signal which axis are broadcasted together:iex> left=Nx.tensor([4,3,2,1])|>Nx.reshape({4,1})iex> right=Nx.tensor([0,1])|>Nx.reshape({1,2})iex> Nx.add(left,right)#Nx.Tensor<s32[4][2][[4,5],[3,4],[2,3],[1,2]]>However, it vectorize/1 simplifies this process. We can instead
signal that each entry on the left tensor will be treated as an
individual tensor, effectively forcing the same broadcast to happen.
In fact, you can think of the following code as a series of
additions between tensors of shapes {} and {2} respectively.iex> vectorized=Nx.vectorize(Nx.tensor([4,3,2,1]),:x)#Nx.Tensor<vectorized[x:4]s32[4,3,2,1]>iex> Nx.add(vectorized,Nx.tensor([0,1]))#Nx.Tensor<vectorized[x:4]s32[2][[4,5],[3,4],[2,3],[1,2]]>ContainersContainers are also supported:iex> input={Nx.tensor([1]),%{a:Nx.tensor([2])}}iex> {t1,%{a:t2}}=Nx.vectorize(input,x:1)iex> t1#Nx.Tensor<vectorized[x:1]s32[1]>iex> t2#Nx.Tensor<vectorized[x:1]s32[2]>Error casesiex> Nx.vectorize(Nx.tensor(1),:x)** (ArgumentError) cannot vectorize tensor of rank 0iex> Nx.vectorize(Nx.tensor([1]),[:x,:y])** (ArgumentError) number of vectorized axes must not be greater than the shape sizeiex> Nx.vectorize(Nx.tensor([1]),[x:2])** (ArgumentError) expected vectorized axis :x to have size 2, got 1iex> Nx.vectorize(Nx.tensor([[1]]),[:x,"y"])** (ArgumentError) expected vectorized axis specification to be an atom or a tuple of {atom, pos_integer}, got: "y"iex> Nx.vectorize(Nx.tensor([[1]],names:[:x,:y]),[:y])** (ArgumentError) cannot use name :y for new vectorized axes because there's already an axis with the same nameiex> t=Nx.vectorize(Nx.tensor([[1]]),:x)iex> Nx.vectorize(t,:x)** (ArgumentError) cannot use name :x for new vectorized axes because there's already a vectorized axis with the same nameFunctions: TypeLink to this functionas_type(tensor, type)View SourceChanges the type of a tensor.Note conversion between float and integers truncates the
result. Consider using round/1, floor/1, or ceil/1
before casting from float to integer to guarantee consistent
behavior.Casting from a higher precision may lead to an overflow
or underflow, which is platform and compiler dependent
behaviour.Casting of non-finite types to integer types are handled
such as:negative infinity becomes the minimum value for said typepositive infinity becomes the maximum value for said typenan becomes zeroExamplesiex> Nx.as_type(Nx.tensor([0,1,2],names:[:data]),:f32)#Nx.Tensor<f32[data:3][0.0,1.0,2.0]>iex> Nx.as_type(Nx.tensor([0.0,1.0,2.0],names:[:data]),:bf16)#Nx.Tensor<bf16[data:3][0.0,1.0,2.0]>iex> Nx.as_type(Nx.tensor([0.0,1.0,2.0],names:[:data]),:s64)#Nx.Tensor<s64[data:3][0,1,2]>Casting numbers as complex will return the corresponding complex with 0 imaginary component:iex> Nx.as_type(Nx.tensor([1,-2]),:c64)#Nx.Tensor<c64[2][1.0+0.0i,-2.0+0.0i]>Casting complex numbers will return their real parts as the target type:iex> Nx.as_type(Nx.tensor([Complex.new(1,2),Complex.new(0,3),Complex.new(4,5)]),:f64)#Nx.Tensor<f64[3][1.0,0.0,4.0]>iex> Nx.as_type(Nx.tensor([Complex.new(-1,2),Complex.new(-2,3),Complex.new(3,-4)]),:s64)#Nx.Tensor<s64[3][-1,-2,3]>Casting of non-finite values to integer types convert to pre-determined
integer values:iex> non_finite=Nx.tensor([:infinity,:nan,:neg_infinity])iex> Nx.as_type(non_finite,:u8)#Nx.Tensor<u8[3][255,0,0]>iex> Nx.as_type(non_finite,:s32)#Nx.Tensor<s32[3][2147483647,0,-2147483648]>Non-finite values between float types are preserved:iex> non_finite=Nx.tensor([:infinity,:nan])iex> Nx.as_type(non_finite,:f64)#Nx.Tensor<f64[2][Inf,NaN]>iex> Nx.as_type(non_finite,:f16)#Nx.Tensor<f16[2][Inf,NaN]>If the input is a numerical constant instead of a tensor, this is an
alias to Nx.tensor(number, type: type). In the example below,
notice how precision is only lost if we pass a type which can't
represent the numerical input:iex> Nx.as_type(1.0e-128,:f32)#Nx.Tensor<f320.0>iex> Nx.as_type(1.0e-128,:f64)#Nx.Tensor<f641.0e-128>Link to this functionbitcast(tensor, type)View SourceChanges the type of a tensor, using a bitcast.The width of input tensor's type must match the width
of the output type. bitcast/1 does not change the
underlying tensor data, but instead changes how
the tensor data is viewed.Machines with different floating-point representations
will give different results.For complex numbers, the last axis will change in size
depending on whether you are upcasting or downcasting.Examplesiex> t=Nx.bitcast(Nx.tensor([0,0,0],names:[:data],type::s32),:f32)#Nx.Tensor<f32[data:3][0.0,0.0,0.0]>iex> Nx.bitcast(t,:s32)#Nx.Tensor<s32[data:3][0,0,0]>iex> t=Nx.vectorize(Nx.tensor([[0,-1],[1,-2],[2,-3]],type::s8),:x)#Nx.Tensor<vectorized[x:3]s8[2][[0,-1],[1,-2],[2,-3]]>iex> Nx.bitcast(t,:u8)#Nx.Tensor<vectorized[x:3]u8[2][[0,255],[1,254],[2,253]]>Error casesiex> Nx.bitcast(Nx.tensor([0,1,2],names:[:data],type::s16),:f32)** (ArgumentError) input type width must match new type width, got input type {:s, 16} and output type {:f, 32}iex> Nx.bitcast(Nx.tensor([0],type::c64),:s64)** (ArgumentError) Nx.bitcast/2 does not support complex inputsiex> Nx.bitcast(Nx.tensor([0],type::s64),:c64)** (ArgumentError) Nx.bitcast/2 does not support complex inputsLink to this functiontype(tensor)View SourceReturns the type of the tensor.See Nx.Type for more information.Examplesiex> Nx.type(Nx.tensor([1,2,3])){:s,32}iex> Nx.type(Nx.tensor([1,2,3],type::f32)){:f,32}iex> Nx.type(1){:s,32}iex> Nx.type(1.0){:f,32}Functions: WindowLink to this functionwindow_max(tensor, window_dimensions, opts \\ [])View SourceReturns the maximum over each window of size window_dimensions
in the given tensor, producing a tensor that contains the same
number of elements as valid positions of the window.You may optionally specify :strides which is a tuple
of non-zero steps to take along each axis between
each window.You may also optionally specify :padding which is either
one of :valid (no padding) or :same (pad so output shape
is the same as input shape) or a general padding configuration
for each dimension in the input tensor. Your padding configuration
cannot include any negative pad values. You may only specify
padding for the high and low edges of the given dimension. Pads
with the minimum value for the type of the given tensor.Examplesiex> Nx.window_max(Nx.tensor([[[1,2,3],[4,5,6]],[[1,2,3],[4,5,6]]]),{1,2,1})#Nx.Tensor<s32[2][1][3][[[4,5,6]],[[4,5,6]]]>iex> t=Nx.tensor([[[1,2,3],[4,5,6]],[[1,2,3],[4,5,6]]])iex> Nx.window_max(t,{2,2,1},strides:[1,2,3],padding:[{0,1},{2,0},{1,1}])#Nx.Tensor<s32[2][2][2][[[-2147483648,-2147483648],[-2147483648,6]],[[-2147483648,-2147483648],[-2147483648,6]]]>iex> t=Nx.tensor([[[4.0,2.0,3.0],[2.0,5.0,6.5]],[[1.2,2.2,3.2],[4.0,5.0,6.2]]])iex> Nx.window_max(t,{2,1,1},strides:[2,1,1],padding:[{1,1},{0,0},{1,1}])#Nx.Tensor<f32[2][2][5][[[-Inf,4.0,2.0,3.0,-Inf],[-Inf,2.0,5.0,6.5,-Inf]],[[-Inf,1.2000000476837158,2.200000047683716,3.200000047683716,-Inf],[-Inf,4.0,5.0,6.199999809265137,-Inf]]]>iex> t=Nx.tensor([[[4,2,1,3],[4,2,1,7]],[[1,2,5,7],[1,8,9,2]]])iex> opts=[strides:[2,1,1],padding::valid,window_dilations:[1,2,2]]iex> Nx.window_max(t,{1,1,2},opts)#Nx.Tensor<s32[1][2][2][[[4,3],[4,7]]]>Vectorized tensorsFor vectorized tensors, the windows will slide throughout all vectorized axes,
and all options refer to the inner shape only.iex> t=Nx.iota({2,1,2,5})|>Nx.vectorize(:x)|>Nx.vectorize(:y)#Nx.Tensor<vectorized[x:2][y:1]s32[2][5][[[[0,1,2,3,4],[5,6,7,8,9]]],[[[10,11,12,13,14],[15,16,17,18,19]]]]>iex> Nx.window_max(t,{2,2},strides:[1,2],window_dilations:[1,2])#Nx.Tensor<vectorized[x:2][y:1]s32[1][2][[[[7,9]]],[[[17,19]]]]>Link to this functionwindow_mean(tensor, window_dimensions, opts \\ [])View SourceAverages over each window of size window_dimensions in the
given tensor, producing a tensor that contains the same
number of elements as valid positions of the window.You may optionally specify :strides which is a tuple
of non-zero steps to take along each axis between
each window.You may also optionally specify :padding which is either
one of :valid (no padding) or :same (pad so output shape
is the same as input shape) or a general padding configuration
for each dimension in the input tensor. Your padding configuration
cannot include any negative pad values. You may only specify
padding for the high and low edges of the given dimension. Pads
with 0.Examplesiex> t=Nx.tensor([[[1,2,3],[4,5,6]],[[1,2,3],[4,5,6]]])iex> Nx.window_mean(t,{1,2,1})#Nx.Tensor<f32[2][1][3][[[2.5,3.5,4.5]],[[2.5,3.5,4.5]]]>iex> t=Nx.tensor([[[1,2,3],[4,5,6]],[[1,2,3],[4,5,6]]])iex> Nx.window_mean(t,{2,2,1},strides:[1,2,3],padding:[{0,1},{2,0},{1,1}])#Nx.Tensor<f32[2][2][2][[[0.0,0.0],[0.0,4.5]],[[0.0,0.0],[0.0,2.25]]]>iex> t=Nx.tensor([[[4.0,2.0,3.0],[2.0,5.0,6.5]],[[1.2,2.2,3.2],[4.0,5.0,6.2]]])iex> Nx.window_mean(t,{2,1,1},strides:[2,1,1],padding:[{1,1},{0,0},{1,1}])#Nx.Tensor<f32[2][2][5][[[0.0,2.0,1.0,1.5,0.0],[0.0,1.0,2.5,3.25,0.0]],[[0.0,0.6000000238418579,1.100000023841858,1.600000023841858,0.0],[0.0,2.0,2.5,3.0999999046325684,0.0]]]>iex> t=Nx.tensor([[[4,2,1,3],[4,2,1,7]],[[1,2,5,7],[1,8,9,2]]])iex> opts=[strides:[2,1,1],padding::valid,window_dilations:[1,2,1]]iex> Nx.window_mean(t,{1,1,2},opts)#Nx.Tensor<f32[1][2][3][[[3.0,1.5,2.0],[3.0,1.5,4.0]]]>iex> t=Nx.tensor([[[4,2,1,3],[4,2,1,7]],[[1,2,5,7],[1,8,9,2]]])iex> opts=[strides:[2,1,1],padding::valid,window_dilations:[1,2,2]]iex> Nx.window_mean(t,{1,1,2},opts)#Nx.Tensor<f32[1][2][2][[[2.5,2.5],[2.5,4.5]]]>Vectorized tensorsFor vectorized tensors, the windows will slide throughout all vectorized axes,
and all options refer to the inner shape only.iex> t=Nx.iota({2,1,2,5})|>Nx.vectorize(:x)|>Nx.vectorize(:y)#Nx.Tensor<vectorized[x:2][y:1]s32[2][5][[[[0,1,2,3,4],[5,6,7,8,9]]],[[[10,11,12,13,14],[15,16,17,18,19]]]]>iex> Nx.window_mean(t,{2,2},strides:[1,2],window_dilations:[1,2])#Nx.Tensor<vectorized[x:2][y:1]f32[1][2][[[[3.5,5.5]]],[[[13.5,15.5]]]]>Link to this functionwindow_min(tensor, window_dimensions, opts \\ [])View SourceReturns the minimum over each window of size window_dimensions
in the given tensor, producing a tensor that contains the same
number of elements as valid positions of the window.You may optionally specify :strides which is a tuple
of non-zero steps to take along each axis between
each window.You may also optionally specify :padding which is either
one of :valid (no padding) or :same (pad so output shape
is the same as input shape) or a general padding configuration
for each dimension in the input tensor. Your padding configuration
cannot include any negative pad values. You may only specify
padding for the high and low edges of the given dimension. Pads
with the maximum value for the type of the given tensor.Examplesiex> Nx.window_min(Nx.tensor([[[1,2,3],[4,5,6]],[[1,2,3],[4,5,6]]]),{1,2,1})#Nx.Tensor<s32[2][1][3][[[1,2,3]],[[1,2,3]]]>iex> t=Nx.tensor([[[1,2,3],[4,5,6]],[[1,2,3],[4,5,6]]])iex> Nx.window_min(t,{2,2,1},strides:[1,2,3],padding:[{0,1},{2,0},{1,1}])#Nx.Tensor<s32[2][2][2][[[2147483647,2147483647],[2147483647,3]],[[2147483647,2147483647],[2147483647,3]]]>iex> t=Nx.tensor([[[4.0,2.0,3.0],[2.0,5.0,6.5]],[[1.2,2.2,3.2],[4.0,5.0,6.2]]])iex> Nx.window_min(t,{2,1,1},strides:[2,1,1],padding:[{1,1},{0,0},{1,1}])#Nx.Tensor<f32[2][2][5][[[Inf,4.0,2.0,3.0,Inf],[Inf,2.0,5.0,6.5,Inf]],[[Inf,1.2000000476837158,2.200000047683716,3.200000047683716,Inf],[Inf,4.0,5.0,6.199999809265137,Inf]]]>iex> t=Nx.tensor([[[4,2,1,3],[4,2,1,7]],[[1,2,5,7],[1,8,9,2]]])iex> opts=[strides:[2,1,1],padding::valid,window_dilations:[1,2,2]]iex> Nx.window_min(t,{1,1,2},opts)#Nx.Tensor<s32[1][2][2][[[1,2],[1,2]]]>Vectorized tensorsFor vectorized tensors, the windows will slide throughout all vectorized axes,
and all options refer to the inner shape only.iex> t=Nx.iota({2,1,2,5})|>Nx.vectorize(:x)|>Nx.vectorize(:y)#Nx.Tensor<vectorized[x:2][y:1]s32[2][5][[[[0,1,2,3,4],[5,6,7,8,9]]],[[[10,11,12,13,14],[15,16,17,18,19]]]]>iex> Nx.window_min(t,{2,2},strides:[1,2],window_dilations:[1,2])#Nx.Tensor<vectorized[x:2][y:1]s32[1][2][[[[0,2]]],[[[10,12]]]]>Link to this functionwindow_product(tensor, window_dimensions, opts \\ [])View SourceReturns the product over each window of size window_dimensions
in the given tensor, producing a tensor that contains the same
number of elements as valid positions of the window.The rank of the input tensor and the window dimensions must
match.You may optionally specify :strides which is a tuple
of non-zero steps to take along each axis between
each window.You may also optionally specify :padding which is either
one of :valid (no padding) or :same (pad so output shape
is the same as input shape) or a general padding configuration
for each dimension in the input tensor. Your padding configuration
cannot include any negative pad values. You may only specify
padding for the high and low edges of the given dimension. Pads
with 1.Examplesiex> Nx.window_product(Nx.tensor([[[1,2,3],[4,5,6]],[[1,2,3],[4,5,6]]]),{1,2,1})#Nx.Tensor<s32[2][1][3][[[4,10,18]],[[4,10,18]]]>iex> t=Nx.tensor([[[1,2,3],[4,5,6]],[[1,2,3],[4,5,6]]])iex> Nx.window_product(t,{2,2,1},strides:[1,2,3],padding:[{0,1},{2,0},{1,1}])#Nx.Tensor<s32[2][2][2][[[1,1],[1,324]],[[1,1],[1,18]]]>iex> t=Nx.tensor([[[4.0,2.0,3.0],[2.0,5.0,6.5]],[[1.2,2.2,3.2],[4.0,5.0,6.2]]])iex> Nx.window_product(t,{2,1,1},strides:[2,1,1],padding:[{1,1},{0,0},{1,1}])#Nx.Tensor<f32[2][2][5][[[1.0,4.0,2.0,3.0,1.0],[1.0,2.0,5.0,6.5,1.0]],[[1.0,1.2000000476837158,2.200000047683716,3.200000047683716,1.0],[1.0,4.0,5.0,6.199999809265137,1.0]]]>iex> t=Nx.tensor([[[4,2,1,3],[4,2,1,7]],[[1,2,5,7],[1,8,9,2]]])iex> opts=[strides:[2,1,1],padding::valid,window_dilations:[1,2,2]]iex> Nx.window_product(t,{1,1,2},opts)#Nx.Tensor<s32[1][2][2][[[4,6],[4,14]]]>Vectorized tensorsFor vectorized tensors, the windows will slide throughout all vectorized axes,
and all options refer to the inner shape only.iex> t=Nx.iota({2,1,2,5})|>Nx.vectorize(:x)|>Nx.vectorize(:y)#Nx.Tensor<vectorized[x:2][y:1]s32[2][5][[[[0,1,2,3,4],[5,6,7,8,9]]],[[[10,11,12,13,14],[15,16,17,18,19]]]]>iex> Nx.window_product(t,{2,2},strides:[1,2],window_dilations:[1,2])#Nx.Tensor<vectorized[x:2][y:1]s32[1][2][[[[0,504]]],[[[30600,54264]]]]>Link to this functionwindow_reduce(tensor, acc, window_dimensions, opts \\ [], fun)View SourceReduces over each window of size dimensions
in the given tensor, producing a tensor that contains the same
number of elements as valid positions of the window.The rank of the input tensor and the window dimensions must
match.You may optionally specify :strides which is a tuple
of non-zero steps to take along each axis between
each window.You may also optionally specify :padding which is either
one of :valid (no padding) or :same (pad so output shape
is the same as input shape) or a general padding configuration
for each dimension in the input tensor. Your padding configuration
cannot include any negative pad values. You may only specify
padding for the high and low edges of the given dimension. The
padding value is equal to the initial value passed to acc.The initial value must be a number or a scalar shaped tensor.Examplesiex> init_value=Nx.Constants.min_finite(:s32)iex> t=Nx.tensor([[1,2,3,4],[4,5,6,7],[7,8,9,10],[11,12,13,14]])iex> Nx.window_reduce(t,init_value,{2,2},fnx,acc->Nx.max(x,acc)end)#Nx.Tensor<s32[3][3][[5,6,7],[8,9,10],[12,13,14]]>iex> init_value=Nx.Constants.min_finite(:s32)iex> t=Nx.tensor([[1,2,3],[4,5,6],[7,8,9]])iex> opts=[padding::same,strides:[1,1]]iex> Nx.window_reduce(t,init_value,{2,2},opts,fnx,acc->Nx.max(x,acc)end)#Nx.Tensor<s32[3][3][[5,6,6],[8,9,9],[8,9,9]]>iex> t=Nx.tensor([[1,2,3],[4,5,6]])iex> opts=[padding::same,strides:[1,1]]iex> Nx.window_reduce(t,0,{1,2},opts,fnx,acc->Nx.add(x,acc)end)#Nx.Tensor<s32[2][3][[3,5,3],[9,11,6]]>iex> t=Nx.tensor([[[4,2,1,3],[4,2,1,7]],[[1,2,5,7],[1,8,9,2]]])iex> opts=[padding::valid,strides:[2,1,1],window_dilations:[1,1,2]]iex> Nx.window_reduce(t,0,{1,1,2},opts,fnx,acc->Nx.add(x,acc)end)#Nx.Tensor<s32[1][2][2][[[5,5],[5,9]]]>Vectorized tensorsThe accumulator must not be vectorized. Aside from that, window_reduce will apply the reduction
over each non-vectorized entry, as follows:iex> t=Nx.tensor([[[1,2,3],[4,5,6]],[[0,-1,-2],[-3,-4,-5]]])|>Nx.vectorize(x:2)iex> opts=[padding:[{0,0},{0,1}],strides:[1,1]]iex> Nx.window_reduce(t,0,{2,2},opts,fnx,acc->Nx.add(x,acc)end)#Nx.Tensor<vectorized[x:2]s32[1][3][[[12,16,9]],[[-8,-12,-7]]]>Link to this functionwindow_scatter_max(tensor, source, init_value, window_dimensions, opts \\ [])View SourcePerforms a window_reduce to select the maximum index in each
window of the input tensor according to and scatters source tensor
to corresponding maximum indices in the output tensor.Output tensor is initialized as a full tensor with values
init_value. If indices overlap, adds overlapping source values.
The shape of the source tensor must match the valid windows in the
input tensor. This means the shape of the source tensor must match
the shape of the input tensor after a window_reduce op with padding
padding and strides strides.This function is the gradient of window_max.Examplesiex> t=Nx.tensor([...> [7,2,5,3,10,2],...> [3,8,9,3,4,2],...> [1,5,7,5,6,1],...> [0,6,2,7,2,8]...> ])iex> opts=[strides:[2,3],padding::valid]iex> Nx.window_scatter_max(t,Nx.tensor([[2,6],[3,1]]),0,{2,3},opts)#Nx.Tensor<s32[4][6][[0,0,0,0,6,0],[0,0,2,0,0,0],[0,0,3,0,0,0],[0,0,0,0,0,1]]>iex> t=Nx.tensor([...> [7,2,5,3,8],...> [3,8,9,3,4],...> [1,5,7,5,6],...> [0,6,2,10,2]...> ])iex> opts=[strides:[2,2],padding::valid]iex> Nx.window_scatter_max(t,Nx.tensor([[2,6],[3,1]]),0,{2,3},opts)#Nx.Tensor<s32[4][5][[0,0,0,0,0],[0,0,8,0,0],[0,0,3,0,0],[0,0,0,1,0]]>Vectorized tensorsThe source and target tensors can be vectorized, and will be broadcasted
through broadcast_vectors/1 for the result calculation. init_value
must not be vectorized.iex> t=Nx.tensor([...> [...> [7,2,5,3],...> [3,8,9,3]...> ],...> [...> [1,5,7,5],...> [0,6,2,8]...> ]...> ])|>Nx.vectorize(:x)iex> opts=[strides:[1,2],padding::valid]iex> source=Nx.tensor([[[2,6]],[[3,1]]])|>Nx.vectorize(:y)iex> Nx.window_scatter_max(t,source,0,{2,2},opts)#Nx.Tensor<vectorized[x:2][y:2]s32[2][4][[[[0,0,0,0],[0,2,6,0]],[[0,0,0,0],[0,3,1,0]]],[[[0,0,0,0],[0,2,0,6]],[[0,0,0,0],[0,3,0,1]]]]>Link to this functionwindow_scatter_min(tensor, source, init_value, window_dimensions, opts \\ [])View SourcePerforms a window_reduce to select the minimum index in each
window of the input tensor according to and scatters source tensor
to corresponding minimum indices in the output tensor.Output tensor is initialized as a full tensor with values
init_value. If indices overlap, adds overlapping source values.
The shape of the source tensor must match the valid windows in the
input tensor. This means the shape of the source tensor must match
the shape of the input tensor after a window_reduce op with padding
padding and strides strides.This function is the gradient of window_min.Examplesiex> t=Nx.tensor([...> [7,2,5,3,10,2],...> [3,8,9,3,4,2],...> [1,5,7,5,6,1],...> [0,6,2,7,2,8]...> ])iex> opts=[strides:[2,3],padding::valid]iex> Nx.window_scatter_min(t,Nx.tensor([[2,6],[3,1]]),0,{2,3},opts)#Nx.Tensor<s32[4][6][[0,2,0,0,0,0],[0,0,0,0,0,6],[0,0,0,0,0,1],[3,0,0,0,0,0]]>iex> t=Nx.tensor([...> [7,2,5,3,8],...> [3,8,9,3,4],...> [1,5,7,5,6],...> [0,6,2,10,2]...> ])iex> opts=[strides:[2,2],padding::valid]iex> Nx.window_scatter_min(t,Nx.tensor([[2,6],[3,1]]),0,{2,3},opts)#Nx.Tensor<s32[4][5][[0,2,0,0,0],[0,0,0,6,0],[0,0,0,0,0],[3,0,0,0,1]]>Vectorized tensorsThe source and target tensors can be vectorized, and will be broadcasted
through broadcast_vectors/1 for the result calculation. init_value
must not be vectorized.iex> t=Nx.tensor([...> [...> [7,2,5,1],...> [3,8,9,3]...> ],...> [...> [1,5,7,5],...> [0,6,2,8]...> ]...> ])|>Nx.vectorize(:x)iex> opts=[strides:[1,2],padding::valid]iex> source=Nx.tensor([[[2,6]],[[3,1]]])|>Nx.vectorize(:y)iex> Nx.window_scatter_min(t,source,0,{2,2},opts)#Nx.Tensor<vectorized[x:2][y:2]s32[2][4][[[[0,2,0,6],[0,0,0,0]],[[0,3,0,1],[0,0,0,0]]],[[[0,0,0,0],[2,0,6,0]],[[0,0,0,0],[3,0,1,0]]]]>Link to this functionwindow_sum(tensor, window_dimensions, opts \\ [])View SourceSums over each window of size window_dimensions in the
given tensor, producing a tensor that contains the same
number of elements as valid positions of the window.You may optionally specify :strides which is a tuple
of non-zero steps to take along each axis between
each window.You may also optionally specify :padding which is either
one of :valid (no padding) or :same (pad so output shape
is the same as input shape) or a general padding configuration
for each dimension in the input tensor. Your padding configuration
cannot include any negative pad values. You may only specify
padding for the high and low edges of the given dimension. Pads
with 0.Examplesiex> t=Nx.tensor([[[1,2,3],[4,5,6]],[[1,2,3],[4,5,6]]])iex> Nx.window_sum(t,{1,2,1})#Nx.Tensor<s32[2][1][3][[[5,7,9]],[[5,7,9]]]>iex> t=Nx.tensor([[[1,2,3],[4,5,6]],[[1,2,3],[4,5,6]]])iex> Nx.window_sum(t,{2,2,1},strides:[1,2,3],padding:[{0,1},{2,0},{1,1}])#Nx.Tensor<s32[2][2][2][[[0,0],[0,18]],[[0,0],[0,9]]]>iex> t=Nx.tensor([[[4.0,2.0,3.0],[2.0,5.0,6.5]],[[1.2,2.2,3.2],[4.0,5.0,6.2]]])iex> Nx.window_sum(t,{2,1,1},strides:[2,1,1],padding:[{1,1},{0,0},{1,1}])#Nx.Tensor<f32[2][2][5][[[0.0,4.0,2.0,3.0,0.0],[0.0,2.0,5.0,6.5,0.0]],[[0.0,1.2000000476837158,2.200000047683716,3.200000047683716,0.0],[0.0,4.0,5.0,6.199999809265137,0.0]]]>iex> t=Nx.tensor([[[4,2,1,3],[4,2,1,7]],[[1,2,5,7],[1,8,9,2]]])iex> opts=[strides:[2,1,1],padding::valid,window_dilations:[1,2,1]]iex> Nx.window_sum(t,{1,1,2},opts)#Nx.Tensor<s32[1][2][3][[[6,3,4],[6,3,8]]]>iex> t=Nx.tensor([[[4,2,1,3],[4,2,1,7]],[[1,2,5,7],[1,8,9,2]]])iex> opts=[strides:[2,1,1],padding::valid,window_dilations:[1,2,2]]iex> Nx.window_sum(t,{1,1,2},opts)#Nx.Tensor<s32[1][2][2][[[5,5],[5,9]]]>iex> t=Nx.tensor([[[4,2,1,3],[4,2,1,7]],[[1,2,5,7],[1,8,9,2]]])iex> opts=[strides:[2,1,1],padding:[{2,1},{3,1},{1,0}],window_dilations:[1,2,2]]iex> Nx.window_sum(t,{2,1,2},opts)#Nx.Tensor<s32[2][6][3][[[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0]],[[0,0,0],[0,0,0],[0,0,0],[4,11,14],[10,15,19],[0,0,0]]]>Vectorized tensorsFor vectorized tensors, the windows will slide throughout all vectorized axes,
and all options refer to the inner shape only.iex> t=Nx.iota({2,1,2,5})|>Nx.vectorize(:x)|>Nx.vectorize(:y)#Nx.Tensor<vectorized[x:2][y:1]s32[2][5][[[[0,1,2,3,4],[5,6,7,8,9]]],[[[10,11,12,13,14],[15,16,17,18,19]]]]>iex> Nx.window_sum(t,{2,2},strides:[1,2],window_dilations:[1,2])#Nx.Tensor<vectorized[x:2][y:1]s32[1][2][[[[14,22]]],[[[54,62]]]]>TypesLink to this typeaxes()View Source@type axes() :: Nx.Tensor.axes()Link to this typeaxis()View Source@type axis() :: Nx.Tensor.axis()Link to this typeshape()View Source@type shape() :: number() | Nx.Tensor.t() | Nx.Tensor.shape()Link to this typet()View Source@type t() :: number() | Complex.t() | Nx.Tensor.t()Represents a numerical value.Can be a plain number, a Complex number or an Nx.Tensor.See also: is_tensor/1Link to this typetemplate()View Source@type template() :: Nx.Tensor.t(%Nx.TemplateBackend{})Hex PackageHex Preview
            Search HexDocs
          
              Download ePub version
            
        Built using
        ExDoc (v0.34.2) for the

          Elixir programming languageAggregation — Nx v0.9.2
Nx
        
          v0.9.2
        
Pages
        
            Modules
          Search documentation of NxSettingsView SourceAggregationMix.install([{:nx,"~> 0.5"}])importNx,only::sigilsNxWhat is aggregation?Aggregation is the process of reducing a tensor to a single value or a smaller tensor by applying specific operations across its dimensions. You can apply aggregation functions on any tensor. The functions can be applied to the tensor as a whole or to a subsection of the tensor taken in an axis-wise fashion.As a first example, let's take a 2D tensor of shape {2, 3}. Notice that we can name the axes. The elements of the tensor t are the t[i][j] when $i$ is the row, and $j$ the column.m=Nx.tensor([[1,2,3],[4,5,6]],names:[:x,:y])#Nx.Tensor<s64[x:2][y:3][[1,2,3],[4,5,6]]>You can get the maximum number in a tensor with Nx.reduce_max(m), returning a 0D tensor. With a reduction, we lose a dimension per axis reduced, and since we applied the reduction globally, we lose all the dimensions. It should return Nx.Tensor(6) here.We can get the maximum number for each row with Nx.reduce_max(matrix, axes: [:y]), returning a 1D tensor of size 2. Why :y? Because for each row, we reduce along the :y axis.We can get the maximum number for each column with Nx.reduce_max(matrix, axes: [:x]), returning a 1D tensor of size 3. For each column, we reduce along the axis :x.max=Nx.reduce_max(m)max_x=Nx.reduce_max(m,axes:[:x])max_y=Nx.reduce_max(m,axes:[:y])%{max:max,max_x:max_x,max_y:max_y}%{max:#Nx.Tensor<s646>,max_x:#Nx.Tensor<s64[y:3][4,5,6]>,max_y:#Nx.Tensor<s64[x:2][3,6]>}Let's consider another example with Nx.weighted_mean. It supports full-tensor and per axis operations. We display how to compute the weighted mean aggregate of a matrix with the example below of a 2D tensor of shape {2,2} labeled m:m=~MAT[1234]#Nx.Tensor<s64[2][2][[1,2],[3,4]]>First, we'll compute the full-tensor aggregation. The calculations are developed below. We calculate an "array product" (aka Hadamard product, an element-wise product) of our tensor with the tensor of weights, then sum all the elements and divide by the sum of the weights.w=~MAT[10203040]nx_w_avg=Nx.weighted_mean(m,w)man_w_avg=(1*10+2*20+3*30+4*40)/(10+20+30+40)# 300/100%{nx_weighted_avg:Nx.to_number(nx_w_avg),manual_weighted_avg:man_w_avg}%{nx_weighted_avg:3.0,manual_weighted_avg:3.0}The weighted mean can be computed per axis. Let's compute it along the first axis (axes: [0]): you calculate "by column", so you aggregate/reduce along the first axis:w=~MAT[10203040]w_avg_x=Nx.weighted_mean(m,w,axes:[0])man_w_avg_x=[(1*10+3*30)/(10+30),(2*20+4*40)/(20+40)]# [100/40, 200/60]{w_avg_x,man_w_avg_x}{#Nx.Tensor<f32[2][2.5,3.3333332538604736]>,[2.5,3.3333333333333335]}We calculate weighted mean of a square matrix along the second axis (axes: [1]): you calculate per row, so you aggregate/reduce along the second axis.w=~MAT[10203040]nx_w_avg_y=Nx.weighted_mean(m,w,axes:[1])man_w_avg_y=[(1*10+2*20)/(10+20),(3*30+4*40)/(30+40)]# [ 50/30, 250/70]{nx_w_avg_y,man_w_avg_y}{#Nx.Tensor<f32[2][1.6666666269302368,3.5714285373687744]>,[1.6666666666666667,3.5714285714285716]}Example with higher rankThe example below will be used through the documentation.Take a list of numbers of length $n=36$, and turn it into a tensor with the shape {2,3,2,3}; this is a tensor of rank 4. We will name the axes, with names: [:x, :y, :z, :t].t=Nx.tensor([[[[1,2,3],[1,-2,3]],[[4,-3,2],[-4,3,2]],[[5,-1,3],[-5,1,3]]],[[[4,6,1],[4,-6,1]],[[1,2,3],[1,-2,3]],[[4,-3,2],[-4,3,2]]]],names:[:x,:y,:z,:t])#Nx.Tensor<s64[x:2][y:3][z:2][t:3][[[[1,2,3],[1,-2,3]],[[4,-3,2],[-4,3,2]],[[5,-1,3],[-5,1,3]]],[[[4,6,1],[4,-6,1]],[[1,2,3],[1,-2,3]],[[4,-3,2],[-4,3,2]]]]>With the shape {2,3,2,4}, you obtain slices of length $2$,then $3$, then $2$ and, finally, $3$.
The picture below will help to understand the aggregations.$$
\begin{bmatrix}
x=0 &
\begin{bmatrix}
y=0 &
\begin{bmatrix}
z=0 & a_{0,0,0,0} & a_{0,0,0,1} & a_{0,0,0,2} \\
z=1 & a_{0,0,1,0} & a_{0,0,1,1} & a_{0,0,1,2}
\end{bmatrix}\\
y=1 &
\begin{bmatrix}
z=0 & a_{0,1,0,0} & a_{0,1,0,1} & a_{0,1,0,2} \\
z=1 & a_{0,1,1,0} & a_{0,1,1,1} & a_{0,1,1,2} \\
\end{bmatrix}\\
y=2 & \begin{bmatrix}
z=0 & a_{0,2,0,0} & a_{0,2,0,1} & a_{0,2,0,2} \\
z=1 & a_{0,2,1,0} & a_{0,2,1,1} & a_{0,2,1,2} \\
\end{bmatrix}
\end{bmatrix} \\
x=1 &
\begin{bmatrix}
y=0 &
\begin{bmatrix}
z=0 & a_{1,0,0,0} & a_{1,0,0,1} & a_{1,0,0,2}  \\
z=1 & a_{1,0,1,0} & a_{1,0,1,1} & a_{1,0,1,2} \\
\end{bmatrix}\\
y=1 &
\begin{bmatrix}
z=0 & a_{1,1,0,0} & a_{1,1,0,1} & a_{1,1,0,2}  \\
z=1 & a_{1,1,1,0} & a_{1,1,1,1} & a_{1,1,1,2} \\
\end{bmatrix}\\
y=2 &
\begin{bmatrix}
z=0 & a_{1,2,0,0} & a_{1,2,0,1} & a_{1,2,0,2}  \\
z=1 & a_{1,2,1,0} & a_{1,2,1,1} & a_{1,2,1,2} \\
\end{bmatrix}
\end{bmatrix}
\end{bmatrix}
\equiv
\begin{bmatrix}
\begin{bmatrix}
\begin{bmatrix}
& 1, 2, 3, & \\
& 1, -2, 3, &
\end{bmatrix} \\
\begin{bmatrix}
& 4, -3, 2 & \\
& -4, 3, 2 &
\end{bmatrix}\\
\begin{bmatrix}
& 5, -1, 3 &\\
& -5, 1, 3 &
\end{bmatrix}
\end{bmatrix} \\
\begin{bmatrix}
\begin{bmatrix}
& 4, 6, 1 & \\
& 4, -6, 1 &
\end{bmatrix}\\
\begin{bmatrix}
& 1, 2, 3 & \\
& 1, -2, 3 &
\end{bmatrix}\\
\begin{bmatrix}
& 4, -3, 2 & \\
& -4, 3, 2 &
\end{bmatrix}
\end{bmatrix}
\end{bmatrix}
$$Firstly, let's check some characteristics of this tensor using full-tensor aggregation functions.%{dimensions:%{x:Nx.axis_size(t,:x),y:Nx.axis_size(t,:y),z:Nx.axis_size(t,:z),t:Nx.axis_size(t,:t)},n:Nx.size(t),shape:Nx.shape(t),rank:Nx.rank(t),analysis:%{most_frequent_nb:Nx.mode(t),smallest_element:%{value:Nx.reduce_min(t),position:Nx.argmin(t)},greatest_element:%{value:Nx.reduce_max(t),position:Nx.argmax(t)},no_zero_nb:Nx.all(t)}}%{n:36,shape:{2,3,2,3},rank:4,dimensions:%{y:3,x:2,t:3,z:2},analysis:%{most_frequent_nb:#Nx.Tensor<s643>,smallest_element:%{position:#Nx.Tensor<s6422>,value:#Nx.Tensor<s64-6>},greatest_element:%{position:#Nx.Tensor<s6419>,value:#Nx.Tensor<s646>},no_zero_nb:#Nx.Tensor<u81>}}Single row aggregation, along an axis❗ We are going to use the key :axis below. The following functions argmin, argmax, median and mode use :axis: <symbol> or number in singular mode. All of the other aggregating functions use axes: [<symbol> or number]; these have multi-axis aggregation implemented.When you aggregate along an axis, you are going to reshape the tensor and to aggregate with a function, in other words perform a reduction. It is important to consider the ordering of the axes.Aggregate along the first axis axis: :x (equivalently axis: 0).The shape of our original tensor t is [x: 2, y: 3, z: 2, t: 3]. When we aggregate along the axis :x, it collapses and the shape of the resultant tensor r is [y: 3,z: 2,t: 3]. The rule is that every axis before the selected one will remain in the structure of the tensor as well as every axis after the selected one.How? You collect from each slice of x the elements that have the same remaining indexes "on the right" $y,z,t$. In the case of our tensor, you have 2 x-slices so you build a sublist of 2 elements, and then apply the aggregating function on it. This gives you the value of the resulting tensor $r$ at the location $r[y][z][t]$:$$
\rm{agg}\big(t[0][y][z][t], t[1][y][z][t]\big) = r[y][z][t]
$$The aggregating function used below is argmin: it returns the index of the smallest element of the sublist. In case of several occurences, the :tie_break attribute takes the lowest index by default.$$
\begin{bmatrix}
\begin{bmatrix}
\begin{bmatrix}
& \boxed{1}, 2, 3, & \\
& 1, -2, 3, &
\end{bmatrix} \\
\begin{bmatrix}
& 4, -3, 2 & \\
& -4, 3, 2 &
\end{bmatrix} \\
\begin{bmatrix}
& 5, -1, 3 &\\
& -5, 1, 3 &
\end{bmatrix}
\end{bmatrix} \\
\begin{bmatrix}
\begin{bmatrix}
& \boxed{4}, 6, 1 & \\
& 4, -6, 1 &
\end{bmatrix} \\
\begin{bmatrix}
& 1, 2, 3 & \\
& 1, -2, 3 &
\end{bmatrix} \\
\begin{bmatrix}
& 4, -3, 2 & \\
& -4, 3, 2 &
\end{bmatrix}
\end{bmatrix}
\end{bmatrix}
\to
\begin{bmatrix}
\begin{bmatrix}
& \boxed{\rm{agg}(1,4)}, \rm{agg}(2,6), \rm{agg}(3, 1)& \\
& \rm{agg}(1,4), \rm{agg}(-2,-6), \rm{agg}(3,1) &
\end{bmatrix} \\
\begin{bmatrix}
& \rm{agg}(4,1), \rm{agg}(-3,2), \rm{agg}(2,3) & \\
& \rm{agg}(-4,1), \rm{agg}(3,-2), \rm{agg}(2,3) &
\end{bmatrix} \\
\begin{bmatrix}
& \rm{agg}(5,4), \rm{agg}(-1,-3), \rm{agg}(3,2) &\\
& \rm{agg}(-5,-4), \rm{agg}(1,3), \rm{agg}(3,2) &
\end{bmatrix}\\
\end{bmatrix}
\xrightarrow[]{\rm{argmin}}
\begin{bmatrix}
\begin{bmatrix}
& 0,0, 1 & \\
& 0, 1,1 &
\end{bmatrix} \\
\begin{bmatrix}
& 1, 0,0 & \\
& 0,1,0 &
\end{bmatrix} \\
\begin{bmatrix}
& 1, 1,1 &\\
& 0, 0,1 &
\end{bmatrix}
\end{bmatrix}
$$Nx.argmin(t,axis::x)#Nx.Tensor<s64[y:3][z:2][t:3][[[0,0,1],[0,1,1]],[[1,0,0],[0,1,0]],[[1,1,1],[0,0,1]]]>Aggregate along the second axis axis: :y (equivalently axis: 1) .The axis y will collapse and the shape of the resultant tensor r is [x: 2, z: 2, t: 3].How? The axis $x$ is before the selected one $y$ so it will remain. We therefor apply the procedure as above for each sub $x$-slice.More precisely, fix the $x$-slice, and let's consider the $x=1$ one. You collect in each sub y-slice the elements with the same remaining indexes "on the right", $z,t$ (and $x$ of course). You apply the aggregation and the result will be the value at location $r[1][z][t]$ of the resultant tensor $r$. So the elements of the slice $x=1$ of the resultant tensor $r$ will be:$$
r[x_{=1}][z][t] = \rm{agg}\big([t[x_{=1}][0][z][t], t[x_{=1}][1][z][t], t[x_{=1}][2][z][t]\big)
$$You repeat this operation for each $x$-slice.In the example below, we used again the argmin function.$$
\begin{bmatrix}
\begin{bmatrix}
\begin{bmatrix}
& \boxed{1}, 2, 3, & \\
& 1, -2, 3, &
\end{bmatrix}\\
\begin{bmatrix}
& \boxed{4}, -3, 2 & \\
& -4, 3, 2 &
\end{bmatrix}\\
\begin{bmatrix}
& \boxed{5}, -1, 3 &\\
& -5, 1, 3 &
\end{bmatrix}
\end{bmatrix}\\
\begin{bmatrix}
\begin{bmatrix}
& 4, 6, 1 & \\
& 4, \boxed{-6}, 1 &
\end{bmatrix}\\
\begin{bmatrix}
& 1, 2, 3 & \\
& 1, \boxed{-2}, 3 &
\end{bmatrix}\\
\begin{bmatrix}
& 4, -3, 2 & \\
& -4, \boxed{3}, 2 &
\end{bmatrix}
\end{bmatrix}
\end{bmatrix}
\to
\begin{bmatrix}
\begin{bmatrix}
&\boxed{\rm{agg}(1,4,5)}, \rm{agg}(2,-3,-1), \rm{agg}(3, 2,3)& \\
& \rm{agg}(1,-4,-5), \rm{agg}(-2,3,1), \rm{agg}(3,2,3) &
\end{bmatrix}\\
\begin{bmatrix}
& \rm{agg}(4,1,4), \rm{agg}(6,2,-3), \rm{agg}(1,3,2) & \\
& \rm{agg}(4,1,-4), \boxed{\rm{agg}(-6,-2,3)}, \rm{agg}(1,3,2) &
\end{bmatrix}
\end{bmatrix}
\xrightarrow[]{\rm{argmin}}
\begin{bmatrix}
\begin{bmatrix}
& 0, 2, 1 & \\
& 2, 0,1 &
\end{bmatrix}\\
\begin{bmatrix}
& 1, 2,0 & \\
& 2,0,0 &
\end{bmatrix}
\end{bmatrix}
$$Nx.argmin(t,axis:1)#Nx.Tensor<s64[x:2][z:2][t:3][[[0,1,1],[2,0,1]],[[1,2,0],[2,0,0]]]>Aggregate along the axis: :z axis (equivalently axis: 2).The z axis collapse and the shape of the resultant tensor ris [x: 2, y: 3, t: 3]. You now have understood that we will, for each $x$-slices and each $y$-slices on the left, collect the elements of the slice with the same remain indexes "on the right", thus $t$ here.$$
r[x][y][t] = \rm{agg}\big(t[x][y][0][t], (t[x][y][1][t] \big)
$$In the example below, we used the argmax function.$$
\begin{bmatrix}
x=0 &
\begin{bmatrix}
y=0 &
\begin{bmatrix}
\boxed{1}, 2, 3, & \\
\boxed{1}, -2, 3, &
\end{bmatrix} \\
y=1 &
\begin{bmatrix}
& 4, -3, 2 & \\
& -4, 3, 2 &
\end{bmatrix}\\
y=2 &
\begin{bmatrix}
& 5, -1, 3 &\\
& -5, 1, 3 &
\end{bmatrix}
\end{bmatrix}\\
x=1 &
\begin{bmatrix}
y=0 &
\begin{bmatrix}
& 4, \boxed{6}, 1  \\
& 4,\boxed{-6}, 1
\end{bmatrix}\\
y=1 &
\begin{bmatrix}
& 1, 2, 3 & \\
& 1, -2, 3 &
\end{bmatrix}\\
y=2 &
\begin{bmatrix}
& 4, -3, 2 & \\
& -4, 3, 2 &
\end{bmatrix}
\end{bmatrix}
\end{bmatrix}
\to
\begin{bmatrix}
x=0 &
\begin{bmatrix}
& \boxed{\rm{agg}(1,1)}, \rm{agg}(2,-2), \rm{agg}(3,3)& \\
& \rm{agg}(4,-4), \rm{agg}(-3,3), \rm{agg}(2,2) & \\
& \rm{agg}(5,-5), \rm{agg}(-1,1), \rm{agg}(3,3)
\end{bmatrix}\\
x=1 &
\begin{bmatrix}
& \rm{agg}(4,4), \boxed{\rm{agg}(6,-6)}, \rm{agg}(1,1) & \\
& \rm{agg}(1,1), \rm{agg}(2,-2), \rm{agg}(3,3) & \\
& \rm{agg}(4,-4), \rm{agg}(-3,3), \rm{agg}(2,2) &
\end{bmatrix}
\end{bmatrix}
\xrightarrow[]{\rm{argmax}}
\begin{bmatrix}
\begin{bmatrix}
0, 0, 0  \\
0, 1, 0 \\
0, 1, 0
\end{bmatrix}\\
\begin{bmatrix}
0, 0, 0  \\
0, 0, 0 \\
0, 1, 0
\end{bmatrix}
\end{bmatrix}
$$Nx.argmax(t,axis::z)#Nx.Tensor<s64[x:2][y:3][t:3][[[0,0,0],[0,1,0],[0,1,0]],[[0,0,0],[0,0,0],[0,1,0]]]>Aggregate along the last axis, axis: :t or (axis: 3 or axis: -1).This will reshape the tensor to [x: 2, y: 3, z: 2]. Since this is the last index, having in mind the matrix picture above, you aggregate along each row:$$
r[x][y][z] = \rm{agg}\big(t[x][y][z][0],  t[x][y][z][1], t[x][y][z][2]\big)
$$and repeat this for each $x$-slice, each sub $y$-slice, each sub-sub $z$-slice.Below is the result for the function argmin.$$
\begin{bmatrix}
x=0 &
\begin{bmatrix}
y=0 &
\begin{bmatrix}
& \boxed{1, 2, 3}, & \\
& 1, -2, 3, &
\end{bmatrix}\\
y=1 &
\begin{bmatrix}
& 4, -3, 2 & \\
& -4, 3, 2 &
\end{bmatrix}\\
y=2 &
\begin{bmatrix}
& 5, -1, 3 &\\
& -5, 1, 3 &
\end{bmatrix}
\end{bmatrix}\\
x=1 &
\begin{bmatrix}
y=0 &
\begin{bmatrix}
& 4, 6, 1 & \\
& 4,-6, 1 &
\end{bmatrix}\\
y=1 &
\begin{bmatrix}
& 1, 2, 3 & \\
& \boxed{1, -2, 3} &
\end{bmatrix}\\
y=2 &
\begin{bmatrix}
& 4, -3, 2 & \\
& -4, 3, 2 &
\end{bmatrix}
\end{bmatrix}
\end{bmatrix}
\xrightarrow[]{\rm{argmin}}
\begin{bmatrix}
\begin{bmatrix}
& \boxed{0}, 1 & \\
& 1, 0 & \\
& 1, 0
\end{bmatrix}\\
\begin{bmatrix}
& 2, 1 & \\
& 0, \boxed{1} & \\
& 1, 0
\end{bmatrix}
\end{bmatrix}
$$Nx.argmin(t,axis:3)#Nx.Tensor<s64[x:2][y:3][z:2][[[0,1],[1,0],[1,0]],[[2,1],[0,1],[1,0]]]>Option :tie_breakYou have the :tie_break option to decide how to operate with you have several occurences of the result. It defaults to tie_break: :low.t4=~VEC[20001]%{argmin_with_default:Nx.argmin(t4)|>Nx.to_number(),argmin_with_tie_break_high_option:Nx.argmin(t4,tie_break::high)|>Nx.to_number()}%{argmin_with_default:1,argmin_with_tie_break_high_option:3}Option :keep_axisIts default value is false. When this option set to keep_axis: true, you reduce but keep the dimension of the working axis to $1$. For example, with t of shape {2,3,2,3}, when you reduce along the third axis, :z, we saw that the shape is {2,3,3} but when you keep the axis, the shape is {2,3,1,3}:Nx.argmin(t,axis:2,keep_axis:true)==Nx.argmin(t,axis:2)|>Nx.reshape({2,3,1,3},names:[:x,:y,:z,:t])true$$
\begin{bmatrix}
\begin{bmatrix}
0, 0, 0  \\
0, 1, 0 \\
0, 1, 0
\end{bmatrix}\\
\begin{bmatrix}
0, 0, 0  \\
0, 0, 0 \\
0, 1, 0
\end{bmatrix}
\end{bmatrix}
\to
\begin{bmatrix}
\begin{bmatrix}
\begin{bmatrix}
0, 0, 0
\end{bmatrix}\\
\begin{bmatrix}
0, 1, 0
\end{bmatrix}\\
\begin{bmatrix}
0, 1, 0
\end{bmatrix}
\end{bmatrix}\\
\begin{bmatrix}
\begin{bmatrix}
0, 0, 0
\end{bmatrix}\\
\begin{bmatrix}
0, 0, 0
\end{bmatrix}\\
\begin{bmatrix}
0, 1, 0
\end{bmatrix}
\end{bmatrix}
\end{bmatrix}
$$Multi-row aggregationSuppose you want to aggregate along the axis x and z. Then you should get a tensor of shape [y: 3, t: 3]. Given an index x and z, you aggregate all the numbers with the same indexes $(y,t)$.$$
r[y][t] = \rm{agg}\big( t[0][y][0][t], t[0][y][1][t], t[1][y][0][t], t[1][y][1][t]\big)
$$$$
\begin{bmatrix}
\begin{bmatrix}
\begin{bmatrix}
\boxed{1}, 2, 3 & \\
\boxed{1}, -2, 3 &
\end{bmatrix}\\
\begin{bmatrix}
& 4, -3, 2 & \\
& -4, 3, 2 &
\end{bmatrix}\\
\begin{bmatrix}
& 5, -1, 3 &\\
& -5, 1, 3 &
\end{bmatrix}
\end{bmatrix}\\
\begin{bmatrix}
\begin{bmatrix}
\boxed{4}, 6, 1 & \\
\boxed{4},-6, 1 &
\end{bmatrix}\\
\begin{bmatrix}
& 1, 2, 3 & \\
& 1, -2, 3 &
\end{bmatrix}\\
\begin{bmatrix}
& 4, -3, 2 & \\
& -4, 3, 2 &
\end{bmatrix}
\end{bmatrix}
\end{bmatrix}
\to
\begin{bmatrix}
\rm{agg}(1,1,4,4),\rm{agg}(2,-2,6,-6),\rm{agg}(3,3,1,1) &\\
\rm{agg}(4,-4,1,1), \rm{agg}(-3,3,2,-2), \rm{agg}(2,2,3,3) & \\
\rm{agg}(5,-5,4,-4) , \rm{agg}(-1,1,-3,3,  \rm{agg}(3,3,2,2)
\end{bmatrix}
$$From this, it is easier to undertand what the aggregation returns:Nx.reduce_min(t,axes:[0,-1])#Nx.Tensor<s64[y:3][z:2][[1,-6],[-3,-4],[-3,-5]]>Nx.reduce_max(t,axes:[:x,:t])#Nx.Tensor<s64[y:3][z:2][[6,4],[4,3],[5,3]]>Nx.mean(t,axes:[:x,:y])#Nx.Tensor<f32[z:2][t:3][[3.1666667461395264,0.5,2.3333332538604736],[-1.1666666269302368,-0.5,2.3333332538604736]]>
          ← Previous Page
        
Vectorization
        Hex PackageHex Preview

              (current file)

          
            Search HexDocs
          
              Download ePub version
            
        Built using
        ExDoc (v0.34.2) for the

          Elixir programming languageAPI Reference — Nx v0.9.2
Nx
        
          v0.9.2
        
Pages
        
            Modules
          Search documentation of NxSettingsAPI Reference Nx v0.9.2ModulesNxNumerical Elixir.Nx.BackendThe behaviour for tensor backends.Nx.BatchCreates a batch of tensors (and containers).Nx.BinaryBackendAn opaque backend written in pure Elixir that stores
the data in Elixir's binaries.Nx.ConstantsCommon constants used in computations.Nx.ContainerA protocol that teaches defn how to traverse data structures.Nx.DefnNumerical functions.Nx.Defn.CompilerThe specification and helper functions for custom defn compilers.Nx.Defn.CompositeFunctions to deal with composite data types.Nx.Defn.EvaluatorThe default implementation of a Nx.Defn.Compiler
that evaluates the expression tree against the
tensor backend.Nx.Defn.ExprNx.Defn.KernelAll imported functionality available inside defn blocks.Nx.Defn.TokenA defn token used by hooks.Nx.Defn.TreeHelper functions to traverse defn expressions,
either as single nodes or in-depth.Nx.HeatmapProvides a heatmap that is printed using ANSI colors
in the terminal.Nx.LazyContainerConverts a data structure to a container lazily.Nx.LinAlgNx conveniences for linear algebra.Nx.LinAlg.CholeskyNx.LinAlg.EighNx.LinAlg.QRNx.PointerRepresents a reference to a value in memory.Nx.RandomPseudo-random number generators.Nx.ServingServing encapsulates client and server work to perform batched requests.Nx.StreamThe protocol for streaming data in and out of backends.Nx.TemplateBackendAn opaque backend written that is used as template
to declare the type, shape, and names of tensors to
be expected in the future.Nx.TensorThe tensor struct and the behaviour for backends.Nx.TypeConveniences for working with types.
          Next Page →
        
Changelog
        Hex PackageHex Preview
            Search HexDocs
          
              Download ePub version
            
        Built using
        ExDoc (v0.34.2) for the

          Elixir programming languageChangelog — Nx v0.9.2
Nx
        
          v0.9.2
        
Pages
        
            Modules
          Search documentation of NxSettingsView SourceChangelogv0.9.2 (2024-11-16)Bug fixes[Nx] Fix deprecation warnings on latest Elixir[Nx.LinAlg] Fix least_squares implementation[Nx.Random] Fix Nx.Random.shuffle repeating a single value in certain cases on GPUv0.9.1 (2024-10-08)Deprecations[Nx] Deprecate Nx.Defn.streamv0.9.0 (2024-09-26)Enhancements[Nx] Add 8-bit Floating Point numerical type[Nx] Add quantized int types (s2, s4, u2, u4)Bug fixes[Nx.LinAlg] Minor range slicing fixes on QR decomposition[Nx] Nx.Defn.Grad now supports more vectorization casesDeprecations and incompatibilities[Nx] Default integer type is now s32[Nx] Interface breaking changes for Nx.to_pointer and Nx.from_pointerv0.8.0 (2024-08-19)Enhancements[Nx] Add Nx.to_pointer/2 and Nx.from_pointer/5[Nx] Introduce ~VEC sigil for 1d tensors[Nx] Introduce ~MAT sigil for 2d tensors[Nx] Implement stack as a callback for performance[Nx] Make take an optional callback[Nx] Make take_along_axis an optional callback[Nx.LinAlg] Support :keep_axes in eighBug fixes[Nx] Fix a bug with gather when indices had more dimensions than the input tensor[Nx] Fix min/max value for 16 bit signed type[Nx] Fix argmax/argmin behaviour with NaNs[Nx.Serving] Fix a bug where streaming responses were never closingDeprecations and incompatibilities[Nx] Deprecate ~V in favor of ~VEC[Nx] Deprecate ~M in favor of ~MAT[Nx] Remove Nx.map/2v0.7.1 (2024-02-27)[Nx.LinAlg] Minor speed up to Nx.LinAlg.qr/2 default implementationv0.7.0 (2024-02-22)Enhancements[Nx] Add Nx.fft2 and Nx.ifft2[Nx] Add Nx.fill/2[Nx] Implement QR decomposition as optional callback[Nx] Support :type option in argmin/argmax[Nx] Default all sorting operations to unstable sorting (pass stable: true to change it)[Nx.BinaryBackend] Improve performance of Nx.concatenate/2[Nx.Defn] Support a mapping function in print_value/2[Nx.Defn] Add Nx.Defn.Compiler.__to_backend__/1 callback[Nx.LinAlg] Add Nx.least_squares/2Bug fixes[Nx.Constants] Fix min and max finite values for :bf16[Nx.Defn] Do not discard arguments on optional gradsIncompatible changes[Nx] Default to non-stable sorting[Nx] Remove deprecated random_uniform, random_normal, shuffle[Nx.Defn] Nx.Defn.rewrite_types/2 has been removedv0.6.4 (2023-11-13)Enhancements[Nx] Allow non-scalar tensors on accessBug fixes[Nx] Improve the :axes option in gather, indexed_add, and indexed_put[Nx] Fix grad of gather, indexed_add, and indexed_put with axes[Nx.BinaryBackend] Fix sorting of negative infinity[Nx.BinaryBackend] Always sort NaN last[Nx.Serving] Fix Nx.Batch padding with multi-device backendsv0.6.3 (2023-11-09)Enhancements[Nx] Allow non-scalars as updates on indexed_add and indexed_put[Nx] Allow non-scalars as return of gather[Nx] Support the :axes option in gather, indexed_add, and indexed_put[Nx] Add Nx.covariance[Nx] Support :type in argsort[Nx] Support :stable option in argsort for future compatibility[Nx.Serving] Add :weight option for static load balancingBug fixes[Nx] Cast input types on slicing[Nx.Defn] Support vectorized tensors in grad[Nx.Defn] Fix bugs when diffing tensor expressions[Nx.Serving] Handle serving getting stuck on timer messagesv0.6.2 (2023-09-21)Enhancements[Nx.Serving] Add Nx.Serving.batch_size/2 and perform batch splitting on run[Nx.Serving] Support input streamingv0.6.1 (2023-09-12)Enhancements[Nx] Add multivariate normal distribution[Nx.Serving] Automatically split exceeding batch sizesBug fixes[Nx] Fix Nx.pad/2 with different backends[Nx] Fix Nx.clip/3 with non-finite values[Nx.Serving] Emit batches as they arrive in Nx.Serving.streaming/2[Nx.Serving] Ensure batch key is preserved when a batch is splitv0.6.0 (2023-08-15)Enhancements[Nx] Add constant creation helpers such as u8, f32, etc[Nx] Implement Bluestein's algorithm for fft and ifft in the binary backend[Nx] Support range with steps when accessing tensors[Nx] Support vectorization via Nx.vectorize/2, Nx.devectorize/2, Nx.revectorize/2, Nx.reshape_vectors/2, and Nx.broadcast_vectors/2[Nx] Add Nx.logsumexp/2[Nx] Add Nx.split/3[Nx] Add Nx.tri/2, Nx.triu/2, Nx.tril/2[Nx] Introduce a new serialization format that is more suitable to memory mapping[Nx.Defn] Consider Inspect.Opts limit when pretty printing Nx.Defn expressions[Nx.Serving] Support multiple batch keys in Nx.Serving[Nx.Serving] Support streaming in Nx.ServingBug fixes[Nx] Fix from_numpy with 1-byte width arrays[Nx] Fix cases where pretty printing large Nx.Defn expressions would take a long time[Nx] Fix reduce_min/reduce_max for non-finite valuesDeprecations[Nx.Serving] The post-processing function must now be a two-arity function that receives the {output, metadata} as a pair or the streamBreaking changes[Nx.Serving] The nx.serving.postprocessing telemetry event no longer receives the serving output or serving metadata as event metadatav0.5.3 (2023-04-14)Bug fixes[Nx.Defn] Fix compilation error when Elixir compiler has column tracking enabled[Nx.LinAlg] Fix cases where determinant could return NaN[Nx.LinAlg] Fix SVD when working with f16 and bf16v0.5.2 (2023-03-21)Enhancements[Nx.Random] Add stop_grad to Nx.Random creation functions[Nx.Serving] Reduce references sent through servingBug fixes[Nx] Fix Nx.mode with :axis optionv0.5.1 (2023-02-18)Require Elixir v1.14.Enhancements[Nx] Support any container or lazy container in stack/concatenate[Nx] Add Nx.top_k/2[Nx] Add Nx.to_list/1[Nx] Improve shape validation in Nx.concatenate/2[Nx.Constants] Add pi, e, and euler_gamma[Nx.Random] Raise if a non-unary rank tensor is given as probabilities to Nx.Random.choice/4[Nx.Random] Make samples optional in Nx.Random.choice/3v0.5.0 (2023-02-10)Enhancements[Nx] Support serialization of containers[Nx] Rename Nx.power to Nx.pow[Nx] Add Nx.reflect and Nx.linspace[Nx.Defn] Raise at compile time for invalid defn if/cond usage[Nx.LinAlg] Support full_matrices? in SVD[Nx.LinAlg] Add Nx.LinAlg.matrix_rank[Nx.Random] Add Nx.Random.choice and Nx.Random.shuffle[Nx.Serving] Add distributed² serving by distributing over devices (GPUs/CPUs) as well as nodes[Nx.Serving] Add telemetry to Nx.Serving callbacksBackwards incompatible changes[Nx] from_numpy and from_numpy_archive have been replaced by load_numpy! and load_numpy_archive![Nx.Defn.Evaluator] Do not force GC on evaluatorv0.4.2 (2023-01-13)Enhancements[Nx] Allow tensors to be given on Nx.tensor/2[Nx] Add Nx.with_default_backend/2[Nx] Add :axes option to Nx.flatten/2[Nx] Add :axes option to Nx.weighted_mean/2[Nx.Defn] Warn if Nx.tensor/2 first-argument is not constant inside defn[Nx.LinAlg] Add Nx.LinAlg.pinv/1[Nx.LinAlg] Optimize and handle more cases in Nx.LinAlg.svd/1Bug fixes[Nx] Respect fortran order in loading from numpy[Nx.Defn] Render containers in compile error type+shape mismatch[Nx.Defn] Restore pdict state after compilationv0.4.1 (2022-12-07)Enhancements[Nx] Add Nx.Batch and Nx.Serving[Nx] Implement Nx.Container for numbers, complex, and tensors for completeness[Nx] Support batches in Nx.eye/2Bug fixes[Nx] Keep input tensor names on associative scan[Nx.BinaryBackend] Differentiate between complex and real output in as_type[Nx.BinaryBackend] Fix loss of precision in Nx.complex/2[Nx.BinaryBackend] Preserve NaNs in window and reduce operations[Nx.Random] Do not return infinity on normal/2 for f16v0.4.0 (2022-10-25)Enhancements[Nx] Add Nx.rename/2, Nx.median/2, Nx.weighted_mean/3, and Nx.mode/2[Nx] Implement cumulative operations using associative scan for improved performance[Nx.Constants] Add min and max[Nx.Defn] Allow lists and functions anywhere as arguments in defn, jit and compile[Nx.Defn] Add Nx.LazyContainer that allows a data-structure to lazily define tensors[Nx.Defn] Allow tensors and ranges as generators inside while[Nx.Defn] Add debug_expr/2 and debug_expr_apply/3[Nx.Defn.Evaluator] Calculate cache lifetime to reduce memory usage on large numerical programs[Nx.LinAlg] Handle Hermitian matrices in eigh[Nx.LinAlg] Support batched operations in adjoint, cholesky, determinant, eigh, invert, lu, matrix_power, solve, svd, and triangular_solve[Nx.Random] Support pseudo random number generators algorithmsBug fixes[Nx] Perform window_reduce/reduce operations from infinity and negative infinity[Nx.Defn] Ensure defnp emits warnings when unused[Nx.Defn] Warn on unused variables in whileDeprecations[Nx] Deprecate tensor as shape in Nx.eye/2 and Nx.iota/2[Nx] Deprecate Nx.random_uniform/2 and Nx.random_normal/2v0.3.0 (2022-08-13)Enhancements[Nx] Improve support for non-finite values in Nx.broadcast/2, Nx.all_close/2, and more[Nx] Add Nx.is_nan/1 and Nx.is_infinite/1[Nx] Support booleans in Nx.tensor/2[Nx] Add Nx.fft/2 and Nx.ifft/2[Nx] Rename Nx.logistic/1 to Nx.sigmoid/1[Nx] Add Nx.put_diagonal/3 and Nx.indexed_put/3[Nx] Add :reverse to cummulative functions[Nx] Add Nx.to_batched/3 which returns a stream[Nx] Support batched tensors in Nx.LinAlg.qr/1[Nx.Defn] Add Nx.Defn.compile/3 for precompiling expressions[Nx.Defn] Add deftransform/2 and deftransformp/2 for easier to define transforms[Nx.Defn] Add div/2[Nx.Defn] Support case/2, raise/1, and raise/2[Nx.Defn] Support booleans in if, cond, and boolean operators[Nx.Defn] Perform branch elimitation in if and cond and execute branches lazily[Nx.Defn.Evaluator] Garbage collect after evaluation (it can be disabled by setting the :garbage_collect compiler option to false)Deprecations[Nx] Nx.to_batched_list/3 is deprecated in favor of Nx.to_batched/3[Nx.Defn] transform/2 is deprecated in favor of deftransform/2 and deftransformp/2[Nx.Defn] assert_shape/2 and assert_shape_pattern/2 are deprecated in favor of case/2 + raise/2[Nx.Defn] inspect_expr/1 and inspect_value/1 are deprecated in favor of print_expr/1 and print_value/1 respectivelyv0.2.1 (2022-06-04)Enhancements[Nx] Improve support for non-finite values in Nx.tensor/1[Nx] Use iovec on serialization to avoid copying binaries[Nx.BinaryBackend] Improve for complex numbers in Nx.tensor/1[Nx.Defn] Improve for complex numbers inside defnBug fixes[Nx] Properly normalize type in Nx.from_binary/3[Nx.Defn] Raise on Nx.Defn.Expr as JIT argument[Nx.Defn.Evaluator] Handle concatenate arguments on evaluatorv0.2.0 (2022-04-28)This version requires Elixir v1.13+.Enhancements[Nx] Support atom notation as the type option throughout the API (for example, :u8, :f64, etc)[Nx] Add support for complex numbers (c64, c128)[Nx] Add Nx.cumulative_sum/2, Nx.cumulative_product/2, Nx.cumulative_min/2, Nx.cumulative_max/2[Nx] Add Nx.conjugate/1, Nx.phase/1, Nx.real/1, and Nx.imag/1[Nx] Initial support for NaN and Infinity[Nx] Add :axis option to Nx.shuffle/2[Nx] Add Nx.axis_index/2[Nx] Add Nx.variance/2 to Nx.standard_deviation/2[Nx] Rename Nx.slice_axis/3 to Nx.slice_along_axis/4[Nx.Backend] Add support for optional backends[Nx.Constants] Provide a convenient module to host constants[Nx.Defn] Improve error messages throughout the compilerv0.1.0 (2022-01-06)First release.
          ← Previous Page
        
API Reference
        
          Next Page →
        
Introduction to Nx
        Hex PackageHex Preview

              (current file)

          
            Search HexDocs
          
              Download ePub version
            
        Built using
        ExDoc (v0.34.2) for the

          Elixir programming languageExercises: 1-20 — Nx v0.9.2
Nx
        
          v0.9.2
        
Pages
        
            Modules
          Search documentation of NxSettingsView SourceExercises: 1-20Mix.install([{:nx,"~> 0.6"}])IntroductionInspired by the Python notebook 100 Numpy Exercises.1-101. Install Nx in a Livebook. (★☆☆)# Add your solution here.Example solution

  ```elixir
  Mix.install([{:nx, "~> 0.6"}])
  ```

  2. Create a 1-D tensor of 10 zeros. (★☆☆)# Add your solution here.Example solution

  ```elixir
  Nx.broadcast(0, {10})
  ```

  3. Find the number of elements in tensor. (★☆☆)tensor=Nx.tensor([[1,2,3],[4,5,6]])# Add your solution here.Example solution

  ```elixir
  Nx.size(tensor)
  ```

  4. Find the number of bytes of memory in tensor. (★☆☆)tensor=Nx.tensor([[1,2,3],[4,5,6]])# Add your solution here.Example solution

  ```elixir
  Nx.byte_size(tensor)
  ```

  5a. Use Nx.sum/2 to find the sum of all elements of tensor. (★☆☆)tensor=Nx.tensor([[1,2,3],[4,5,6],[7,8,9]])# Add your solution here.Example solution

  ```elixir
  Nx.sum(tensor)
  ```

  5b. Read the documentation for Nx.sum/2 then provide the correct option to sum across the rows. (★☆☆)# Add your solution here.Example solution

  ```elixir
  Nx.sum(tensor, axes: [1])
  ```

  Tip: You can also hover over a function inside Livebook code cells to display its documentation.
  6. Create a tensor of zeros of size 10 but where the fifth value is 1. (★☆☆)# Add your solution here.Example solution

  ```elixir
  zeros = Nx.broadcast(0, {10})
  index = Nx.tensor([4])
  Nx.indexed_put(zeros, index, 1)
  ```

  7. Create a 3x3 tensor with values ranging from 0 to 8. (★☆☆)# Add your solution here.Example solution

  ```elixir
  Nx.iota({3, 3})
  ```

  8. Create a tensor with values ranging from 10 to 49. (★☆☆)# Add your solution here.Example solution 1

  ```elixir
  Nx.iota({40})
  |> Nx.add(10)
  ```

  Example solution 2

  ```elixir
  Nx.linspace(10, 49, n: 39, type: :s64)
  ```

  9. Reverse tensor (first element becomes last). (★☆☆)tensor=Nx.tensor([2,4,6,8])# Add your solution here.Example solution

  ```elixir
  Nx.reverse(tensor)
  ```

  10a. Given an initial tensor, build a "mask" of non-zero elements. That is, build a second tensor with the same shape as the original, but that has a 1 wherever the original has a non-zero element and a 0 elsewhere. (★☆☆)tensor=Nx.tensor([1,2,0,0,4,0])# Add your solution here.Example solution

  ```elixir
  mask = Nx.not_equal(tensor, 0)
  ```

  10b. Use the mask from 10a to replace each 0 from tensor with -1. (★☆☆)# Add your solution here.Example solution

  ```elixir
  Nx.select(mask, tensor, -1)
  ```

  11-2011. Create a 3x3 identity tensor. (★☆☆)# Add your solution here.Example solution

  ```elixir
  Nx.eye(3)
  ```

  12. Create a 3x3x3 tensor with random values. (★☆☆)# Add your solution here.Example solution

  ```elixir
  key = Nx.Random.key(0)
  {random, _} = Nx.Random.normal(key, shape: {3, 3, 3})
  random
  ```

  13. Create a random 10x10 tensor then find its minimum and maximum values. (★☆☆)# Add your solution here.Example solution

  ```elixir
  key = Nx.Random.key(0)
  {tensor, _} = Nx.Random.normal(key, shape: {10, 10})

  %{
    min: Nx.reduce_min(tensor),
    max: Nx.reduce_max(tensor)
  }
  ```

  14. Create a random 1D tensor of size 30 then find its mean. (★☆☆)# Add your solution here.Example solution

  ```elixir
  key = Nx.Random.key(0)
  {tensor, _} = Nx.Random.normal(key, shape: {30})

  Nx.mean(tensor)
  ```

  15. Create a 4x4 tensor with 1 on the border and 0 inside. (★☆☆)# Add your solution here.Example solution

  ```elixir
  Nx.broadcast(1, {4, 4})
  |> Nx.put_slice([1, 1], Nx.broadcast(0, {2, 2}))
  ```

  16. Add a border of 0 around tensor (end result will be a 5x5 tensor). (★☆☆)tensor=Nx.broadcast(1,{3,3})# Add your solution here.Example solution

  ```elixir
  Nx.pad(tensor, 0, [{1, 1, 0}, {1, 1, 0}])
  ```

  17. Determine the results of the following expressions. (★☆☆)nan=Nx.Constants.nan()Nx.multiply(0,nan)Nx.equal(nan,nan)Nx.greater(nan,nan)Nx.subtract(nan,nan)# Add your solution here.Example solution

  ```
  #Nx.Tensor<
    f32
    NaN
  >
  #Nx.Tensor<
    u8
    0
  >
  #Nx.Tensor<
    u8
    0
  >
  #Nx.Tensor<
    f32
    NaN
  >
  ```

  18. Create a 5x5 tensor with values 1,2,3,4 just below the diagonal. (★☆☆)# Add your solution here.Example solution

  ```elixir
  Nx.tensor([1, 2, 3, 4])
  |> Nx.make_diagonal(offset: -1)
  ```

  19. Create a 8x8 tensor of 0 and 1 in a checkerboard pattern with 0 as the first element using Nx.tile. (★☆☆)# Add your solution here.Example solution

  ```elixir
  Nx.tensor([[0, 1], [1, 0]])
  |> Nx.tile([4, 4])
  ```

  20. Produce the same checkerboard pattern from exercise 19, but without using Nx.tile. (★☆☆)# Add your solution here.# Hint: try using `Nx.iota` in combination with `Nx.remainder`.Example solution 1

  ```elixir
  t = Nx.iota({8, 1})

  Nx.transpose(t)
  |> Nx.add(t)
  |> Nx.remainder(2)
  ```

  Example solution 2

  ```elixir
  Nx.iota({9, 9})
  |> Nx.remainder(2)
  |> Nx.slice([0, 0], [8, 8])
  ```

  
          ← Previous Page
        
Introduction to Nx
        
          Next Page →
        
Vectorization
        Hex PackageHex Preview

              (current file)

          
            Search HexDocs
          
              Download ePub version
            
        Built using
        ExDoc (v0.34.2) for the

          Elixir programming languageNx v0.9.2 — DocumentationIntroduction to Nx — Nx v0.9.2
Nx
        
          v0.9.2
        
Pages
        
            Modules
          Search documentation of NxSettingsView SourceIntroduction to NxMix.install([{:nx,"~> 0.5"}])Numerical ElixirElixir's primary numerical datatypes and structures are not optimized
for numerical programming. Nx is a library built to bridge that gap.Elixir Nx is a numerical computing library
to smoothly integrate to typed, multidimensional data implemented on other
platforms (called tensors). This support extends to the compilers and
libraries that support those tensors. Nx has three primary capabilities:In Nx, tensors hold typed data in multiple, named dimensions.Numerical definitions, known as defn, support custom code with
tensor-aware operators and functions.Automatic differentiation, also known as
autograd or autodiff, supports common computational scenarios
such as machine learning, simulations, curve fitting, and probabilistic models.Here's more about each of those capabilities. Nx tensors can hold
unsigned integers (u2, u4, u8, u16, u32, u64),
signed integers (s2, s4s8, s16, s32, s64),
floats (f32, f64), brain floats (bf16), and complex (c64, c128).
Tensors support backends implemented outside of Elixir, including Google's
Accelerated Linear Algebra (XLA) and LibTorch.Numerical definitions have compiler support to allow just-in-time compilation
that support specialized processors to speed up numeric computation including
TPUs and GPUs.To know Nx, we'll get to know tensors first. This rapid overview will touch
on the major libraries. Then, future notebooks will take a deep dive into working
with tensors in detail, autograd, and backends. Then, we'll dive into specific
problem spaces like Axon, the machine learning library.Nx and tensorsSystems of equations are a central theme in numerical computing.
These equations are often expressed and solved with multidimensional
arrays. For example, this is a two dimensional array:$$
\begin{bmatrix}
  1 & 2 \\\\
  3 & 4
\end{bmatrix}
$$Elixir programmers typically express a similar data structure using
a list of lists, like this:[[1,2],[3,4]]This data structure works fine within many functional programming
algorithms, but breaks down with deep nesting and random access.On top of that, Elixir numeric types lack optimization for many numerical
applications. They work fine when programs
need hundreds or even thousands of calculations. They tend to break
down with traditional STEM applications when a typical problem
needs millions of calculations.In Nx, we express multi-dimensional data using typed tensors. Simply put,
a tensor is a multi-dimensional array with a predetermined shape and
type. To interact with them, Nx relies on tensor-aware operators rather
than Enum.map/2 and Enum.reduce/3.In this section, we'll look at some of the various tools for
creating and interacting with tensors. The IEx helpers will assist our
exploration of the core tensor concepts.importIEx.HelpersNow, everything is set up, so we're ready to create some tensors.Creating tensorsStart out by getting a feel for Nx through its documentation.
Do so through the IEx helpers, like this:hNxImmediately, you can see that tensors are at the center of the
API. The main API for creating tensors is Nx.tensor/2:hNx.tensorWe use it to create tensors from raw Elixir lists of numbers, like this:tensor=1..4|>Enum.chunk_every(2)|>Nx.tensor(names:[:y,:x])The result shows all of the major fields that make up a tensor:The data, presented as the list of lists [[1, 2], [3, 4]].The type of the tensor, a signed integer 64 bits long, with the type s64.The shape of the tensor, going left to right, with the outside dimensions listed first.The names of each dimension.We can easily convert it to a binary:binary=Nx.to_binary(tensor)A tensor of type s64 uses eight bytes for each integer. The binary
shows the individual bytes that make up the tensor, so you can see
the integers 1..4 interspersed among the zeros that make
up the tensor. If all of our data only uses positive numbers from
0..255, we could save space with a different type:Nx.tensor([[1,2],[3,4]],type::u8)|>Nx.to_binary()If you already have a binary, you can directly convert it to a tensor
by passing the binary and the type:Nx.from_binary(<<0,1,2>>,:u8)This function comes in handy when working with published datasets
because they must often be processed. Elixir binaries make quick work
of dealing with numerical data structured for platforms other than
Elixir.We can get any cell of the tensor:tensor[0][1]Now, try getting the first row of the tensor:# ...your code here...We can also get a whole dimension:tensor[x:1]or a range:tensor[y:0..1]Now,create your own {3, 3} tensor with named dimensionsreturn a {2, 2} tensor containing the first two columns
of the first two rowsWe can get information about this most recent term with
the IEx helper i, like this:itensorThe tensor is a struct that supports the usual Inspect protocol.
The struct has keys, but we typically treat the Nx.Tensor
as an opaque data type (meaning we typically access the contents and
shape of a tensor using the tensor's API instead of the struct).Primarily, a tensor is a struct, and the
functions to access it go through a specific backend. We'll get to
the backend details in a moment. For now, use the IEx h helper
to get more documentation about tensors. We could also open a Code
cell, type Nx.tensor, and hover the cursor over the word tensor
to see the help about that function.We can get the shape of the tensor with Nx.shape/1:Nx.shape(tensor)We can also create a new tensor with a new shape using  Nx.reshape/2:Nx.reshape(tensor,{1,4},names:[:batches,:values])This operation reuses all of the tensor data and simply
changes the metadata, so it has no notable cost.The new tensor has the same type, but a new shape.Now, reshape the tensor to contain three dimensions with
one batch, one row, and four columns.# ...your code here...We can create a tensor with named dimensions, a type, a shape,
and our target data. A dimension is called an axis, and axes
can have names. We can specify the tensor type and dimension names
with options, like this:Nx.tensor([[1,2,3]],names:[:rows,:cols],type::u8)We created a tensor of the shape {1, 3}, with the type u8,
the values [1, 2, 3], and two axes named rows and cols.Now we know how to create tensors, so it's time to do something with them.Tensor aware functionsIn the last section, we created a s64[2][2] tensor. In this section,
we'll use Nx functions to work with it. Here's the value of tensor:tensorWe can use IEx.Helpers.exports/1 or code completion to find
some functions in the Nx module that operate on tensors:exportsNxYou might recognize that many of those functions have names that
suggest that they would work on primitive values, called scalars.
Indeed, a tensor can be a scalar:pi=Nx.tensor(3.1415,type::f32)Take the cosine:Nx.cos(pi)That function took the cosine of pi.  We can also call them
on a whole tensor, like this:Nx.cos(tensor)We can also call a function that aggregates the contents
of a tensor. For example, to get a sum of the numbers
in tensor, we can do this:Nx.sum(tensor)That's 1 + 2 + 3 + 4, and Nx went to multiple dimensions to get that sum.
To get the sum of values along the x axis instead, we'd do this:Nx.sum(tensor,axes:[:x])Nx sums the values across the x dimension: 1 + 2 in the first row
and 3 + 4 in the second row.Now,create a {2, 2, 2} tensorwith the values 1..8with dimension names [:z, :y, :x]calculate the sums along the y axis# ...your code here...Sometimes, we need to combine two tensors together with an
operator. Let's say we wanted to subtract one tensor from
another. Mathematically, the expression looks like this:$$
\begin{bmatrix}
  5 & 6 \\\\
  7 & 8
\end{bmatrix} -
\begin{bmatrix}
  1 & 2 \\\\
  3 & 4
\end{bmatrix} =
\begin{bmatrix}
  4 & 4 \\\\
  4 & 4
\end{bmatrix}
$$To solve this problem, subtract each right-hand integer from the
corresponding left-hand integer. Unfortunately, we cannot
use Elixir's built-in subtraction operator as it is not tensor-aware.
Luckily, we can use the Nx.subtract/2 function to solve the
problem:tensor2=Nx.tensor([[5,6],[7,8]])Nx.subtract(tensor2,tensor)We get a {2, 2} shaped tensor full of fours, exactly as we expected.
When calling Nx.subtract/2, both operands had the same shape.
Sometimes, you might want to process functions where the dimensions
don't match. To solve this problem, Nx takes advantage of
a concept called broadcasting.BroadcastsOften, the dimensions of tensors in an operator don't match.
For example, you might want to subtract a 1 from every
element of a {2, 2} tensor, like this:$$
\begin{bmatrix}
  1 & 2 \\\\
  3 & 4
\end{bmatrix} - 1 =
\begin{bmatrix}
  0 & 1 \\\\
  2 & 3
\end{bmatrix}
$$Mathematically, it's the same as this:$$
\begin{bmatrix}
  1 & 2 \\\\
  3 & 4
\end{bmatrix} -
\begin{bmatrix}
  1 & 1 \\\\
  1 & 1
\end{bmatrix} =
\begin{bmatrix}
  0 & 1 \\\\
  2 & 3
\end{bmatrix}
$$That means we need a way to convert 1 to a {2, 2} tensor.
Nx.broadcast/2 solves that problem. This function takes
a tensor or a scalar and a shape.Nx.broadcast(1,{2,2})This broadcast takes the scalar 1 and translates it
to a compatible shape by copying it.  Sometimes, it's easier
to provide a tensor as the second argument, and let broadcast/2
extract its shape:Nx.broadcast(1,tensor)The code broadcasts 1 to the shape of tensor. In many operators
and functions, the broadcast happens automatically:Nx.subtract(tensor,1)This result is possible because Nx broadcasts both tensors
in subtract/2 to compatible shapes. That means you can provide
scalar values as either argument:Nx.subtract(10,tensor)Or subtract a row or column. Mathematically, it would look like this:$$
\begin{bmatrix}
  1 & 2 \\\\
  3 & 4
\end{bmatrix} -
\begin{bmatrix}
  1 & 2
\end{bmatrix} =
\begin{bmatrix}
  0 & 0 \\\\
  2 & 2
\end{bmatrix}
$$which is the same as this:$$
\begin{bmatrix}
  1 & 2 \\\\
  3 & 4
\end{bmatrix} -
\begin{bmatrix}
  1 & 2 \\\\
  1 & 2
\end{bmatrix} =
\begin{bmatrix}
  0 & 0 \\\\
  2 & 2
\end{bmatrix}
$$This rewrite happens in Nx too, also through a broadcast. We want to
broadcast the tensor [1, 2] to match the {2, 2} shape, like this:Nx.broadcast(Nx.tensor([1,2]),{2,2})The subtract function in Nx takes care of that broadcast
implicitly, as before:Nx.subtract(tensor,Nx.tensor([1,2]))The broadcast worked as advertised, copying the [1, 2] row
enough times to fill a {2, 2} tensor.  A tensor with a
dimension of 1 will broadcast to fill the tensor:[[1],[2]]|>Nx.tensor()|>Nx.broadcast({1,2,2})[[[1,2,3]]]|>Nx.tensor()|>Nx.broadcast({4,2,3})Both of these examples copy parts of the tensor enough
times to fill out the broadcast shape. You can check out the
Nx broadcasting documentation for more details:hNx.broadcastMuch of the time, you won't have to broadcast yourself. Many of
the functions and operators Nx supports will do so automatically.We can use tensor-aware operators via various Nx functions and
many of them implicitly broadcast tensors.Throughout this section, we have been invoking Nx.subtract/2 and
our code would be more expressive if we could use its equivalent
mathematical operator. Fortunately, Nx provides a way. Next, we'll
dive into numerical definitions using defn.Numerical definitions (defn)The defn macro simplifies the expression of mathematical formulas
containing tensors. Numerical definitions have two primary benefits
over classic Elixir functions.They are tensor-aware. Nx replaces operators like Kernel.-/2
with the Defn counterparts &mdash; which in turn use Nx functions
optimized for tensors &mdash; so the formulas we express can use
tensors out of the box.defn definitions allow for building computation graph of all the
individual operations and using a just-in-time (JIT) compiler to emit
highly specialized native code for the desired computation unit.We don't have to do anything special to get access to
get tensor awareness beyond importing Nx.Defn and writing
our code within a defn block.To use Nx in a Mix project or a notebook, we need to include
the :nx dependency and import the Nx.Defn module. The
dependency is already included, so import it in a Code cell,
like this:importNx.DefnJust as the Elixir language supports def, defmacro, and defp,
Nx supports defn. There are a few restrictions. It allows only
numerical arguments in the form of primitives or tensors as arguments
or return values, and supports only a subset of the language.The subset of Elixir allowed within defn is quite broad, though. We can
use macros, pipes, and even conditionals, so we're not giving up
much when you're declaring mathematical functions.Additionally, despite these small concessions, defn provides huge benefits.
Code in a defn block uses tensor aware operators and types, so the math
beneath your functions has a better chance to shine through. Numerical
definitions can also run on accelerated numerical processors like GPUs and
TPUs. Here's an example numerical definition:defmoduleTensorMathdoimportNx.Defndefnsubtract(a,b)doa-bendendThis module has a numerical definition that will be compiled.
If we wanted to specify a compiler for this module, we could add
a module attribute before the defn clause. One of such compilers
is the EXLA compiler.
You'd add the mix dependency for EXLA and do this:@defn_compilerEXLAdefnsubtract(a,b)doa-bendNow, it's your turn. Add a defn to TensorMath
that accepts two tensors representing the lengths of sides of a
right triangle and uses the pythagorean theorem to return the
length of the hypotenuse.
Add your function directly to the previous Code cell.The last major feature we'll cover is called auto-differentiation, or autograd.Automatic differentiation (autograd)An important mathematical property for a function is the
rate of change, or the gradient. These gradients are critical
for solving systems of equations and building probabilistic
models. In advanced math, derivatives, or differential equations,
are used to take gradients. Nx can compute these derivatives
automatically through a feature called automatic differentiation,
or autograd.Here's how it works.hNx.Defn.gradWe'll build a module with a few functions,
and then create another function to create the gradients of those
functions. The function grad/1 takes a function, and returns
a function returning the gradient. We have two functions: poly/1
is a simple numerical definition, and poly_slope_at/1 returns
its gradient:$$
poly: f(x) = 3x^2 + 2x + 1 \\\\
$$$$
polySlopeAt: g(x) = 6x + 2
$$Here's the Elixir equivalent of those functions:defmoduleFunsdoimportNx.Defndefnpoly(x)do3*Nx.pow(x,2)+2*x+1enddefnpoly_slope_at(x)dograd(&poly/1).(x)endendNotice the second defn. It uses grad/1 to take its
derivative using autograd. It uses the intermediate defn AST
and mathematical composition to compute the derivative. You can
see it at work here:Funs.poly_slope_at(2)Nice. If you plug the number 2 into the function $6x + 2$
you get 14! Said another way, if you look at the graph at
exactly 2, the rate of increase is 14 units of poly(x)
for every unit of x, precisely at x.Nx also has helpers to get gradients corresponding to a number of inputs.
These come into play when solving systems of equations.Now, you try. Find a function computing the gradient of a sin wave.# your code here
          ← Previous Page
        
Changelog
        
          Next Page →
        
Exercises: 1-20
        Hex PackageHex Preview

              (current file)

          
            Search HexDocs
          
              Download ePub version
            
        Built using
        ExDoc (v0.34.2) for the

          Elixir programming languageSearch — Nx v0.9.2
Nx
        
          v0.9.2
        
Pages
        
            Modules
          Search documentation of NxSettingsHex PackageHex Preview
            Search HexDocs
          
              Download ePub version
            
        Built using
        ExDoc (v0.34.2) for the

          Elixir programming languageVectorization — Nx v0.9.2
Nx
        
          v0.9.2
        
Pages
        
            Modules
          Search documentation of NxSettingsView SourceVectorizationMix.install([{:nx,"~> 0.7"}])What is vectorization?Vectorization in Nx is the concept of imposing "for-each" semantics into leading tensor axes. This enables writing tensor operations in a simpler way, avoiding while loops, which also leading to more efficient code, as we'll see in this guide.There are a few functions related to vectorization. First we'll look into Nx.vectorize/2, Nx.devectorize/2, and Nx.revectorize/3.For these examples, we'll utilize the normalize function defined below, which only works with 1D tensors. The function subtracts the minimum value and divides the tensor by the resulting maximum.defmoduleExampledoimportNx.Defndefnnormalize(t)docaseNx.shape(t)do{_}->:ok_->raise"invalid shape"endmin=Nx.reduce_min(t)zero_min=Nx.subtract(t,min)Nx.divide(zero_min,Nx.reduce_max(zero_min))endend{:module,Example,<<70,79,82,49,0,0,11,...>>,true}We can invoke it as:Example.normalize(Nx.tensor([1,2,3]))#Nx.Tensor<f32[3][0.0,0.5,1.0]>However, if we attempt to call it for any tensor with more than one dimension, it won't work. Let's first define a 2D tensor:t=Nx.tensor([[1,2,3],[10,20,30],[4,5,6]])#Nx.Tensor<s64[3][3][[1,2,3],[10,20,30],[4,5,6]]>Now if we attempt to invoke it:Example.normalize(t)To address this, we can vectorize the tensor:vec=Nx.vectorize(t,:rows)#Nx.Tensor<vectorized[rows:3]s64[3][[1,2,3],[10,20,30],[4,5,6]]>As we can see above, the newly vectorized vec is the same tensor as t, but the first axis is now a vectorized axis called rows, with size 3. This means that for all intents and purposes, we can think of this tensor as a 1D tensor, on which our normalize function will now work as if we passed those 3 rows separately (thus the "for-each" semantics mentioned above). Let's give it a try:normalized_vec=Example.normalize(vec)#Nx.Tensor<vectorized[rows:3]f32[3][[0.0,0.5,1.0],[0.0,0.5,1.0],[0.0,0.5,1.0]]>While the tensor is vectorized, we can't treat it as a matrix (2D tensor):right=Nx.tensor([[1,2,3],[2,3,4],[3,4,5]])# The results might be unexpected, but they behave the same as Nx.add(Nx.tensor([1, 2, 3]), right)# and so on, resulting in a vectorized tensor with 3 inner matrices.Nx.add(normalized_vec,right)#Nx.Tensor<vectorized[rows:3]f32[3][3][[[1.0,2.5,4.0],[2.0,3.5,5.0],[3.0,4.5,6.0]],[[1.0,2.5,4.0],[2.0,3.5,5.0],[3.0,4.5,6.0]],[[1.0,2.5,4.0],[2.0,3.5,5.0],[3.0,4.5,6.0]]]>You can devectorize the tensor to get its original shape:# If we want to keep the vectorized axes' namesNx.devectorize(normalized_vec)#Nx.Tensor<f32[rows:3][3][[0.0,0.5,1.0],[0.0,0.5,1.0],[0.0,0.5,1.0]]># If we want to drop the vectorized axes' namesdevec=Nx.devectorize(normalized_vec,keep_names:false)#Nx.Tensor<f32[3][3][[0.0,0.5,1.0],[0.0,0.5,1.0],[0.0,0.5,1.0]]>Once devectorized, we can effectively add the two matrices together:Nx.add(devec,right)#Nx.Tensor<f32[3][3][[1.0,2.5,4.0],[2.0,3.5,5.0],[3.0,4.5,6.0]]>RevectorizationNow that we have the basics down, let's discuss Nx.revectorize with multi dimensional tensors. This is especially useful in cases where we have multiple vectorization axes which we want to collapse and then re-expand.# This version of vectorize also asserts on the size of the dimensions being vectorizedt=Nx.iota({2,1,3,2,2})|>Nx.vectorize(x:2,y:1,z:3)#Nx.Tensor<vectorized[x:2][y:1][z:3]s64[2][2][[[[[0,1],[2,3]],[[4,5],[6,7]],[[8,9],[10,11]]]],[[[[12,13],[14,15]],[[16,17],[18,19]],[[20,21],[22,23]]]]]>Let's imagine we want to operate on the vectorized iota t above, adding a specific constant for each vectorized entry. We can do this through a bit of tensor introspection in conjunction with revectorize/3collapsed_t=Nx.revectorize(t,[vectors::auto],target_shape:t.shape)[vectors:vectorized_size]=collapsed_t.vectorized_axesconstants=Nx.iota({vectorized_size})|>Nx.multiply(100)|>Nx.vectorize(vectors:vectorized_size){constants,collapsed_t}{#Nx.Tensor<vectorized[vectors:6]s64[0,100,200,300,400,500]>,#Nx.Tensor<vectorized[vectors:6]s64[2][2][[[0,1],[2,3]],[[4,5],[6,7]],[[8,9],[10,11]],[[12,13],[14,15]],[[16,17],[18,19]],[[20,21],[22,23]]]>}From the output above, we can see that we have a vectorized tensor of scalars as well a vectorized tensor with the same vectorized axis, containing matrices.Now we can add them together as we discussed above, and then re-expand the vectorized axes to the original shape.collapsed_t|>Nx.add(constants)|>Nx.revectorize(t.vectorized_axes)#Nx.Tensor<vectorized[x:2][y:1][z:3]s64[2][2][[[[[0,1],[2,3]],[[104,105],[106,107]],[[208,209],[210,211]]]],[[[[312,313],[314,315]],[[416,417],[418,419]],[[520,521],[522,523]]]]]>Before we move on to the last two vectorization functions, let's see how we can replace a while loop with vectorization. The following module defines the same function twice, once for each method.defmoduleWhileExampledoimportNx.Defndefnwhile_sum_and_multiply(x,y)doout=caseNx.shape(x)do{n,_}->Nx.broadcast(0,{n})_->raise"expected x to have rank 2"endn=Nx.axis_size(out,0)caseNx.shape(y)do{^n}->nil_->raise"expected y to have rank 1 and the same number of elements as x"end{out,_}=while{out,{x,y}},i<-0..(n-1)doupdate=Nx.sum(x[i])+y[i]updated=Nx.indexed_put(out,Nx.reshape(i,{1}),update){updated,{x,y}}endoutenddefnvectorized_sum_and_multiply(x,y)do# for the sake of equivalence, we'll keep the limitation# of accepting only rank-2 tensors for x{n,m}=caseNx.shape(x)do{n,m}->{n,m}_->raise"expected x to have rank 2"endcaseNx.shape(y)do{^n}->nil_->raise"expected y to have rank 1 and the same number of elements as x"end# this enables us to accept vectorized tensors for x and yvectorized_axes=x.vectorized_axesx=Nx.revectorize(x,[collapsed::auto,vectors:n],target_shape:{m})y=Nx.revectorize(y,[collapsed::auto,vectors:n],target_shape:{})out=Nx.sum(x)+yNx.revectorize(out,vectorized_axes,target_shape:{n})endend{:module,WhileExample,<<70,79,82,49,0,0,22,...>>,true}Let's give those definitions a try:x=Nx.tensor([[1,2,3],[1,0,0],[0,1,3]])y=Nx.tensor([4,5,6])x_vec=Nx.tile(x,[2,1])|>Nx.reshape({2,3,3})|>Nx.vectorize(:rows)# use a different set of vectorized axes for yy_vec=Nx.concatenate([y,Nx.multiply(y,2)])|>Nx.reshape({2,3})|>Nx.vectorize(:cols){WhileExample.while_sum_and_multiply(x,y),WhileExample.vectorized_sum_and_multiply(x,y),WhileExample.vectorized_sum_and_multiply(x_vec,y),WhileExample.vectorized_sum_and_multiply(x_vec,y_vec)}{#Nx.Tensor<s64[3][10,6,10]>,#Nx.Tensor<s64[3][10,6,10]>,#Nx.Tensor<vectorized[rows:2]s64[3][[10,6,10],[10,6,10]]>,#Nx.Tensor<vectorized[rows:2]s64[3][[10,6,10],[14,11,16]]>}The advantage of vectorization is that the underlying compilers and hardware may do a much better job of optimizing tensor operations than while loops, which often run linearly.The eagle-eyed reader will have noticed that we inadvertently threw away the vectorized axes that we received from y. This is because our usage of revectorize disregards the possibility that the axes could be different in each input tensor. Luckily, we can tackle this problem too.Reshape and Broadcast VectorsThe bug introduced in the previous section can be solved through Nx.reshape_vectors/1 and Nx.broadcast_vectors/1. Both functions receive lists of tensors and will operate on their vectorized axes to ensure that the shapes are compatible in one way or the other.Nx functions will natively do this for us, as we see below:base=Nx.tensor([0,1,2,3,4,5,6,7,8,9])vec_i=Nx.vectorize(Nx.reshape(base,{10,1}),i:10,j:1)vec_j=Nx.vectorize(base,:j)# easy way to build a multiplication table from 0 to 9Nx.multiply(vec_i,vec_j)#Nx.Tensor<vectorized[i:10][j:10]s64[[0,0,0,0,0,0,0,0,0,0],[0,1,2,3,4,5,6,7,8,9],[0,2,4,6,8,10,12,14,16,18],[0,3,6,9,12,15,18,21,24,27],[0,4,8,12,16,20,24,28,32,36],...]>Ok, now that we know that broadcasting will work on vectorized axes to add elements in our resulting tensor, we can look into the aforementioned functions.a=Nx.iota({2,1},vectorized_axes:[x:2,y:1])b=Nx.iota({},vectorized_axes:[z:2,x:1])Nx.reshape_vectors([a,b],align_ranks:true)[#Nx.Tensor<vectorized[x:2][y:1][z:1]s64[2][1][[[[[0],[1]]]],[[[[0],[1]]]]]>,#Nx.Tensor<vectorized[x:1][y:1][z:2]s64[1][1][[[[[0]],[[0]]]]]>]In the example above, we can see that both tensors end up containing vectorized axes :x, :y, and :z. Furthermore, we can also see that the b tensor was rearranged so that the :x axis comes first. All tensors end up with the same ordering of axes, but the axes aren't resized in any way.Finally, the align_ranks: true option is passed so that the inner shape (the non-vectorized part!) of both tensors ends up in a broadcastable shape with the same rank across all tensors. The example uses only 2 tensors, but the list can have arbitrary size.Nx.broadcast_vectors/2 works similarly, except it also broadcasts the dimensions instead of simply reshaping:x=Nx.iota({2,1},vectorized_axes:[x:2,y:1])y=Nx.iota({},vectorized_axes:[z:2,x:1])Nx.broadcast_vectors([x,y])[#Nx.Tensor<vectorized[x:2][y:1][z:2]s64[2][1][[[[[0],[1]],[[0],[1]]]],[[[[0],[1]],[[0],[1]]]]]>,#Nx.Tensor<vectorized[x:2][y:1][z:2]s64[[[0,0]],[[0,0]]]>]The key difference is that all vectorized axes will end up with the same size in all resulting tensors, effectively behaving the same as Nx.broadcast does for non-vectorized shapes. Specifically, we can see that the x tensor gets the new z: 2 axis and y gets both x: 2, y: 1, where the :x axis was already present, but has now been resized.With this knowledge we can rewrite our function without the bug, as follows:defmoduleBroadcastVectorsExampledoimportNx.Defndefnvectorized_sum_and_multiply(x,y)do# for the sake of equivalence, we'll keep the limitation# of accepting only rank-2 tensors for x[x,y]=Nx.broadcast_vectors([x,y]){n,m}=caseNx.shape(x)do{n,m}->{n,m}_->raise"expected x to have rank 2"endcaseNx.shape(y)do{^n}->nil_->raise"expected y to have rank 1 and the same number of elements as x"end# this enables us to accept vectorized tensors for x and yvectorized_axes=x.vectorized_axesx=Nx.revectorize(x,[collapsed::auto,vectors:n],target_shape:{m})y=Nx.revectorize(y,[collapsed::auto,vectors:n],target_shape:{})out=Nx.sum(x)+yNx.revectorize(out,vectorized_axes,target_shape:{n})endend{:module,BroadcastVectorsExample,<<70,79,82,49,0,0,15,...>>,true}Let's give it once again another try:x=Nx.tensor([[1,2,3],[1,0,0],[0,1,3]])y=Nx.tensor([4,5,6])x_vec=Nx.tile(x,[2,1])|>Nx.reshape({2,3,3})|>Nx.vectorize(:rows)# use a different set of vectorized axes for yy_vec=Nx.concatenate([y,Nx.multiply(y,2)])|>Nx.reshape({2,3})|>Nx.vectorize(:cols){BroadcastVectorsExample.vectorized_sum_and_multiply(x_vec,y),BroadcastVectorsExample.vectorized_sum_and_multiply(x_vec,y_vec)}{#Nx.Tensor<vectorized[rows:2]s64[3][[10,6,10],[10,6,10]]>,#Nx.Tensor<vectorized[rows:2][cols:2]s64[3][[[10,6,10],[14,11,16]],[[10,6,10],[14,11,16]]]>}
          ← Previous Page
        
Exercises: 1-20
        
          Next Page →
        
Aggregation
        Hex PackageHex Preview

              (current file)

          
            Search HexDocs
          
              Download ePub version
            
        Built using
        ExDoc (v0.34.2) for the

          Elixir programming language
